{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day8 seq2seq with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Attention\n",
    "\n",
    "- Attention enhances Seq2Seq models by allowing the decoder to selectively focus on different parts of the input sequence at each decoding step\n",
    "- Instead of relying on a fixed-length context vector from the encoder, the decoder computes a dynamic context vector by weighting the encoder's hidden states based on their relevance to the current decoding step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Fixed-length context vector\n",
    "\n",
    "- Information bottleneck\n",
    "   - Long input sequences result in the loss of critical details since all information is compressed into a single vector.\n",
    "\n",
    "- Vanishing context\n",
    "   - For longer sequences, earlier tokens in the input become less relevant in the compressed context vector.\n",
    "\n",
    "- Difficulty in aligning input and output\n",
    "   - Thereâ€™s no direct alignment between specific input tokens and the corresponding output tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/attention-problem2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Attention Mechanism\n",
    "\n",
    "The attention mechanism addresses these issues by dynamically focusing on different parts of the input sequence for each output token.\n",
    "\n",
    "- Dynamic Context\n",
    "   - Instead of relying on a single fixed-length vector, the context vector is dynamically computed by \"attending\" to specific encoder hidden states.\n",
    "\n",
    "- Better Handling of Long Sequences\n",
    "   - Attention allows the decoder to retrieve relevant information from any part of the input, regardless of sequence length.\n",
    "\n",
    "- Alignment Between Input and Output\n",
    "   - Attention naturally creates alignments, making it easier to map input tokens to corresponding output tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "```\n",
    "<SOS>MAGKL...<EOS>\n",
    "<SOS>MIAEE...<EOS>\n",
    "<SOS>MNNQK...<EOS>\n",
    "<SOS>MFHAE...<EOS>\n",
    "...\n",
    "```\n",
    "- Attention weight = Which one is the most correlated with `<SOS>`?\n",
    "- Context vector (Attention value) = Encoder hidden state + Attention weight\n",
    "- Final hidden state of decoder = Context vector + Initial hidden state of decoder (concatenation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/attention1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Attention Mechanism Steps (Dot product attention)\n",
    "\n",
    "- Alignment Scores\n",
    "   - Compute alignment scores between the decoder's current hidden state and each encoder hidden state.\n",
    "   - $h_i$ is the encoder hidden state and $s_{t-1}$ is the decoder's hidden state.\n",
    "    $$\n",
    "    e_{t,i} = h_i^\\top W s_{t-1}\n",
    "    $$\n",
    "\n",
    "- Attention Weights\n",
    "   - Normalize alignment scores with softmax to get attention weights.\n",
    "    $$\n",
    "    \\alpha_{t,i} = \\frac{\\exp(e_{t,i})}{\\sum_{j=1}^{T_x} \\exp(e_{t,j})}\n",
    "    $$\n",
    "\n",
    "\n",
    "- Context Vector\n",
    "   - Compute the context vector as the weighted sum of encoder hidden states.\n",
    "    $$\n",
    "    c_t = \\sum_{i=1}^{T_x} \\alpha_{t,i} h_i\n",
    "    $$\n",
    "\n",
    "- Combine Context and Decoder State\n",
    "   - Use the context vector $c_t$ and the decoder's current hidden state to predict the next token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/attention5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Outputs (Keys and Values): [[ 0.19573384 -0.49648183  0.1824915   0.10484739 -1.58569094]\n",
      " [ 0.64890387  0.0082949   1.41048922 -0.47920967 -0.7364587 ]\n",
      " [ 0.78422808 -0.44381825  0.67551003  1.23725628  0.2371081 ]\n",
      " [ 0.51214269  0.40668439 -0.18241258  0.28852113  0.2699827 ]]\n",
      "Decoder Hidden State (Query): [ 0.40266189 -0.79767357  0.01391088  1.08446925  0.81412344]\n",
      "Attention Scores: [-0.69986078 -0.84496306  2.21399944  0.41197298]\n",
      "Scaled Attention Scores: [-0.31298726 -0.37787897  0.99013065  0.18423992]\n",
      "Attention Weights (Softmax): [0.1377016  0.12904966 0.50684584 0.2264029 ]\n",
      "Context Vector: [ 0.62412702 -0.20016879  0.50823324  0.64501603 -0.13208979]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example encoder outputs (keys and values)\n",
    "L = 4  # Sequence length\n",
    "D_h = 5  # Hidden dimension\n",
    "\n",
    "# Simulated encoder hidden states (Keys and Values)\n",
    "encoder_outputs = np.random.randn(L, D_h)  # Shape: [L, D_h]\n",
    "\n",
    "# Example decoder hidden state (Query)\n",
    "decoder_hidden = np.random.randn(D_h)  # Shape: [D_h]\n",
    "\n",
    "# Compute Scaled Dot-Product\n",
    "# Query (1, D_h) @ Keys (L, D_h)^T -> (1, L)\n",
    "attention_scores = np.dot(decoder_hidden, encoder_outputs.T)  # Shape: [1, L]\n",
    "\n",
    "# Scale scores by sqrt of hidden dimension\n",
    "scaled_attention_scores = attention_scores / np.sqrt(D_h)  # Shape: [1, L]\n",
    "\n",
    "# Compute Attention Weights\n",
    "# Apply softmax to scaled scores\n",
    "attention_weights = np.exp(scaled_attention_scores - np.max(scaled_attention_scores))  # Stabilize softmax\n",
    "attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)  # Normalize, Shape: [1, L]\n",
    "\n",
    "# Compute Context Vector\n",
    "# Attention Weights (1, L) @ Values (L, D_h) -> (1, D_h)\n",
    "context_vector = np.dot(attention_weights, encoder_outputs)  # Shape: [1, D_h]\n",
    "\n",
    "# Output\n",
    "print(\"Encoder Outputs (Keys and Values):\", encoder_outputs)\n",
    "print(\"Decoder Hidden State (Query):\", decoder_hidden)\n",
    "print(\"Attention Scores:\", attention_scores)\n",
    "print(\"Scaled Attention Scores:\", scaled_attention_scores)\n",
    "print(\"Attention Weights (Softmax):\", attention_weights)\n",
    "print(\"Context Vector:\", context_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/att1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/att1-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation data \n",
    "- For the source sequence, insert two motif (RGD, ATL) in random amino acid sequence\n",
    "- Target sequences are generated with patterns R --> K, G --> A for RGD motif, and A-->V, T--> for ATL motif, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source batch shape: torch.Size([10, 18])\n",
      "Target batch shape: torch.Size([10, 18])\n",
      "Source lengths: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18]\n",
      "Target lengths: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18]\n",
      "Vocabulary size: 2105\n",
      "Initial seuqence: PQFSNRGDTVFRATLDASTH\n",
      "Source k-mers: ['PQF', 'QFS', 'FSN', 'SNR', 'NRG', 'RGD', 'GDT', 'DTV', 'TVF', 'VFR', 'FRA', 'RAT', 'ATL', 'TLD', 'LDA', 'DAS', 'AST', 'STH']\n",
      "Target k-mers: ['FQP', 'SFQ', 'NSF', 'RNS', 'GRN', 'KAD', 'TDG', 'VTD', 'FVT', 'RFV', 'ARF', 'TAR', 'VCL', 'DLT', 'ADL', 'SAD', 'TSA', 'HTS']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Parameters\n",
    "k = 3  # Length of k-mers\n",
    "num_sequences = 100  # Number of full amino acid sequences\n",
    "sequence_length = 20  # Length of each amino acid sequence (increased to accommodate two motifs)\n",
    "batch_size = 10  # Batch size for DataLoader\n",
    "\n",
    "# Define amino acids\n",
    "amino_acids = [\"A\", \"R\", \"N\", \"D\", \"C\", \"Q\", \"E\", \"G\", \"H\", \"I\", \"L\", \"K\", \"M\", \"F\", \"P\", \"S\", \"T\", \"W\", \"Y\", \"V\"]\n",
    "\n",
    "# Generate random amino acid sequences\n",
    "def generate_random_sequences_with_motifs(num_sequences, sequence_length):\n",
    "    sequences = []\n",
    "    for _ in range(num_sequences):\n",
    "        seq = list(np.random.choice(amino_acids, size=sequence_length))\n",
    "        # Insert biologically meaningful patterns (e.g., conserved motifs)\n",
    "        if len(seq) > 10:\n",
    "            seq[5:8] = [\"R\", \"G\", \"D\"]  # Motif 1\n",
    "            seq[12:15] = [\"A\", \"T\", \"L\"]  # Motif 2\n",
    "        sequences.append(\"\".join(seq))\n",
    "    return sequences\n",
    "\n",
    "random_sequences = generate_random_sequences_with_motifs(num_sequences, sequence_length)\n",
    "\n",
    "# Extract k-mers from sequences\n",
    "def generate_kmers(sequence, k):\n",
    "    return [sequence[i:i + k] for i in range(len(sequence) - k + 1)]\n",
    "\n",
    "source_kmers = [generate_kmers(seq, k) for seq in random_sequences]\n",
    "\n",
    "# Define a biological transformation for the target sequences\n",
    "def transform_with_evolution_two_motifs(kmers):\n",
    "    transformed_kmers = []\n",
    "    for kmer in kmers:\n",
    "        if \"RGD\" in kmer:\n",
    "            transformed_kmers.append(kmer.replace(\"R\", \"K\").replace(\"G\", \"A\"))  # Transformation for Motif 1\n",
    "        elif \"ATL\" in kmer:\n",
    "            transformed_kmers.append(kmer.replace(\"A\", \"V\").replace(\"T\", \"C\"))  # Transformation for Motif 2\n",
    "        else:\n",
    "            transformed_kmers.append(kmer[::-1])  # Reverse non-conserved k-mers\n",
    "    return transformed_kmers\n",
    "\n",
    "target_kmers_with_evolution = [transform_with_evolution_two_motifs(kmers) for kmers in source_kmers]\n",
    "\n",
    "# Build vocabulary for k-mers\n",
    "unique_kmers = set(kmer for seq in source_kmers + target_kmers_with_evolution for kmer in seq)\n",
    "vocab = {kmer: idx for idx, kmer in enumerate(unique_kmers)}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Encode k-mers into indices\n",
    "def encode_kmers(kmers, vocab):\n",
    "    return [vocab[kmer] for kmer in kmers]\n",
    "\n",
    "encoded_source_sequences = [encode_kmers(kmers, vocab) for kmers in source_kmers]\n",
    "encoded_target_with_evolution = [encode_kmers(kmers, vocab) for kmers in target_kmers_with_evolution]\n",
    "\n",
    "# Collate function for padding\n",
    "def collate_batch(batch):\n",
    "    source_batch, target_batch = zip(*batch)\n",
    "    src_lengths = [len(seq) for seq in source_batch]\n",
    "    tgt_lengths = [len(seq) for seq in target_batch]\n",
    "\n",
    "    max_src_len = max(src_lengths)\n",
    "    max_tgt_len = max(tgt_lengths)\n",
    "\n",
    "    padded_src = torch.zeros(len(source_batch), max_src_len, dtype=torch.long)\n",
    "    padded_tgt = torch.zeros(len(target_batch), max_tgt_len, dtype=torch.long)\n",
    "\n",
    "    for i, seq in enumerate(source_batch):\n",
    "        padded_src[i, :len(seq)] = torch.tensor(seq, dtype=torch.long)\n",
    "    for i, seq in enumerate(target_batch):\n",
    "        padded_tgt[i, :len(seq)] = torch.tensor(seq, dtype=torch.long)\n",
    "\n",
    "    return padded_src, src_lengths, padded_tgt, tgt_lengths\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "class Seq2SeqKmerDataset(Dataset):\n",
    "    def __init__(self, source_sequences, target_sequences):\n",
    "        self.source_sequences = source_sequences\n",
    "        self.target_sequences = target_sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.source_sequences[idx], self.target_sequences[idx]\n",
    "\n",
    "# Dataset\n",
    "dataset_with_evolution = Seq2SeqKmerDataset(encoded_source_sequences, encoded_target_with_evolution)\n",
    "\n",
    "# Dataloader\n",
    "dataloader_with_evolution = DataLoader(dataset_with_evolution, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "# Example batch\n",
    "for batch in dataloader_with_evolution:\n",
    "    src, src_lengths, tgt, tgt_lengths = batch\n",
    "    print(\"Source batch shape:\", src.shape)\n",
    "    print(\"Target batch shape:\", tgt.shape)\n",
    "    print(\"Source lengths:\", src_lengths)\n",
    "    print(\"Target lengths:\", tgt_lengths)\n",
    "    break\n",
    "\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "print(\"Initial seuqence:\", random_sequences[0])\n",
    "print(\"Source k-mers:\", source_kmers[0])\n",
    "print(\"Target k-mers:\", target_kmers_with_evolution[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Ensure hidden is [batch_size, hidden_dim * 2]\n",
    "        if hidden.dim() == 2:  # [batch_size, hidden_dim * 2]\n",
    "            hidden = hidden.unsqueeze(1)  # [batch_size, 1, hidden_dim * 2]\n",
    "\n",
    "        # Compute dot-product scores\n",
    "        scores = torch.bmm(encoder_outputs, hidden.transpose(1, 2)).squeeze(2)  # [batch_size, seq_len]\n",
    "\n",
    "        # Compute attention weights\n",
    "        attention_weights = F.softmax(scores, dim=1)  # [batch_size, seq_len]\n",
    "\n",
    "        # Compute context vector as weighted sum of encoder outputs\n",
    "        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)  # [batch_size, 1, hidden_dim * 2]\n",
    "\n",
    "        return context_vector.squeeze(1), attention_weights  # [batch_size, hidden_dim * 2], [batch_size, seq_len]\n",
    "\n",
    "\n",
    "class Seq2SeqEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Seq2SeqEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # Embed the input sequences\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_len, embedding_dim]\n",
    "\n",
    "        # Pack padded sequences for LSTM\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Bidirectional LSTM\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "\n",
    "        # Unpack LSTM outputs\n",
    "        encoder_outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        # Concatenate hidden states from both directions\n",
    "        hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)  # [batch_size, hidden_dim * 2]\n",
    "\n",
    "        return encoder_outputs, hidden\n",
    "\n",
    "class Seq2SeqDecoderWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Seq2SeqDecoderWithAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim + hidden_dim * 2, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 + hidden_dim, vocab_size)\n",
    "        self.attention = DotProductAttention()\n",
    "\n",
    "    def forward(self, input_token, hidden, encoder_outputs):\n",
    "        # Embed the input token\n",
    "        embedded = self.embedding(input_token).unsqueeze(1)  # [batch_size, 1, embedding_dim]\n",
    "\n",
    "        # Ensure hidden matches encoder_outputs dimensions\n",
    "        if hidden.dim() == 3:  # Handle multi-layer LSTM\n",
    "            hidden = hidden.transpose(0, 1).contiguous().view(hidden.size(1), -1)  # [batch_size, hidden_dim * 2]\n",
    "\n",
    "        # Compute attention context vector\n",
    "        context_vector, attention_weights = self.attention(hidden, encoder_outputs)\n",
    "\n",
    "        # Concatenate embedded input and context vector\n",
    "        lstm_input = torch.cat((embedded, context_vector.unsqueeze(1)), dim=2)  # [batch_size, 1, embedding_dim + hidden_dim * 2]\n",
    "\n",
    "        # Pass through the LSTM\n",
    "        output, (hidden, _) = self.lstm(lstm_input)\n",
    "\n",
    "        # Predict the next token\n",
    "        prediction = self.fc(torch.cat((output.squeeze(1), context_vector), dim=1))  # [batch_size, vocab_size]\n",
    "\n",
    "        return prediction, hidden, attention_weights\n",
    "\n",
    "\n",
    "\n",
    "class Seq2SeqWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Seq2SeqWithAttention, self).__init__()\n",
    "        self.encoder = Seq2SeqEncoder(vocab_size, embedding_dim, hidden_dim)\n",
    "        self.decoder = Seq2SeqDecoderWithAttention(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, src, src_lengths, tgt, teacher_forcing_ratio=0.5):\n",
    "        encoder_outputs, hidden = self.encoder(src, src_lengths)\n",
    "\n",
    "        batch_size = tgt.size(0)\n",
    "        tgt_len = tgt.size(1)\n",
    "        outputs = torch.zeros(batch_size, tgt_len, vocab_size).to(src.device)\n",
    "\n",
    "        input_token = tgt[:, 0]  # Start token\n",
    "\n",
    "        for t in range(1, tgt_len):\n",
    "            output, hidden, attention_weights = self.decoder(input_token, hidden, encoder_outputs)\n",
    "            outputs[:, t, :] = output\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            input_token = tgt[:, t] if teacher_force else output.argmax(1)\n",
    "\n",
    "        return outputs, attention_weights\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [10, 64] but got: [10, 32].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m src, tgt \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mto(device), tgt\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 23\u001b[0m outputs, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Reshape outputs and compute loss\u001b[39;00m\n\u001b[1;32m     26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size)  \u001b[38;5;66;03m# Skip <SOS>\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/kaist-prog/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/kaist-prog/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[46], line 99\u001b[0m, in \u001b[0;36mSeq2SeqWithAttention.forward\u001b[0;34m(self, src, src_lengths, tgt, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     96\u001b[0m input_token \u001b[38;5;241m=\u001b[39m tgt[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Start token\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, tgt_len):\n\u001b[0;32m---> 99\u001b[0m     output, hidden, attention_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     outputs[:, t, :] \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m    101\u001b[0m     teacher_force \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m<\u001b[39m teacher_forcing_ratio\n",
      "File \u001b[0;32m~/mambaforge/envs/kaist-prog/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/kaist-prog/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[46], line 68\u001b[0m, in \u001b[0;36mSeq2SeqDecoderWithAttention.forward\u001b[0;34m(self, input_token, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m hidden\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(hidden\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch_size, hidden_dim * 2]\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Compute attention context vector\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m context_vector, attention_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Concatenate embedded input and context vector\u001b[39;00m\n\u001b[1;32m     71\u001b[0m lstm_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((embedded, context_vector\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# [batch_size, 1, embedding_dim + hidden_dim * 2]\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/kaist-prog/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/kaist-prog/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[46], line 16\u001b[0m, in \u001b[0;36mDotProductAttention.forward\u001b[0;34m(self, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     13\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m hidden\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch_size, 1, hidden_dim * 2]\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Compute dot-product scores\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# [batch_size, seq_len]\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Compute attention weights\u001b[39;00m\n\u001b[1;32m     19\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [batch_size, seq_len]\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [10, 64] but got: [10, 32]."
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "embedding_dim = 16\n",
    "hidden_dim = 32\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Seq2SeqWithAttention(vocab_size, embedding_dim, hidden_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in dataloader_with_evolution:\n",
    "        src, src_lengths, tgt, tgt_lengths = batch\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(src, src_lengths, tgt)\n",
    "\n",
    "        # Reshape outputs and compute loss\n",
    "        outputs = outputs[:, 1:].reshape(-1, vocab_size)  # Skip <SOS>\n",
    "        tgt = tgt[:, 1:].reshape(-1)  # Skip <SOS>\n",
    "        loss = criterion(outputs, tgt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_attention(attention_weights, input_sequence, output_sequence):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(attention_weights.cpu().detach().numpy(), cmap=\"viridis\", aspect=\"auto\")\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(len(input_sequence)), input_sequence, rotation=90)\n",
    "    plt.yticks(range(len(output_sequence)), output_sequence)\n",
    "    plt.title(\"Attention Weights\")\n",
    "    plt.xlabel(\"Input Sequence\")\n",
    "    plt.ylabel(\"Output Sequence\")\n",
    "    plt.show()\n",
    "\n",
    "# Example visualization during inference\n",
    "src_example, src_len_example, tgt_example, tgt_len_example = next(iter(dataloader_with_evolution))\n",
    "src_example = src_example.to(device)\n",
    "tgt_example = tgt_example.to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs, attention_weights = model(src_example, src_len_example, tgt_example, teacher_forcing_ratio=0)\n",
    "    plot_attention(\n",
    "        attention_weights[0],  # Attention weights for the first sequence in the batch\n",
    "        [list(vocab.keys())[list(vocab.values()).index(idx)] for idx in src_example[0].cpu().numpy()],\n",
    "        [list(vocab.keys())[list(vocab.values()).index(idx)] for idx in tgt_example[0].cpu().numpy()]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/att2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/att2-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/trans1.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaist-prog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
