[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "KAIST Deep Learning",
    "section": "",
    "text": "1 KAIST EB502 programming",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "KAIST Deep Learning",
    "section": "1.1 Welcome",
    "text": "1.1 Welcome\n\n2024.11 카이스트, 공학생물학대학원 프로그래밍 강의 노트\nHaseong Kim (at KRIBB)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#환경",
    "href": "index.html#환경",
    "title": "KAIST Deep Learning",
    "section": "1.2 환경",
    "text": "1.2 환경\n\n실습 환경은 colab을 활용하며 파일 저장 등은 구글 드라이브를 활용함",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "day1_optimization.html",
    "href": "day1_optimization.html",
    "title": "2  Day1 Optimization",
    "section": "",
    "text": "Learn python for biological data analysis with chatGPT\ncolab의 default working directory에 개인의 google drive 연결\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\n2.0.1 Introduction of Google Colab\n\n2.0.1.1 Access Google Colab\n\nGo to Google Colab in your web browser.\nSign in with your Google account.\n\n\n\n2.0.1.2 Create a New Notebook\n\nClick on File -&gt; New Notebook to create a new notebook.\n\n\n\n2.0.1.3 Install Required Libraries\nGoogle Colab comes with many libraries pre-installed, but you might need to install some additional ones, such as biopython and scikit-bio. You can do this using the !pip install command directly in a cell.\n\n!pip install biopython scikit-bio matplotlib\n\n\n!pip install scikit-bio\n\n\n\n2.0.1.4 Import Libraries and Verify Installation\nIn a new code cell, import the libraries to ensure they are installed correctly.\n\n# Importing necessary libraries\nimport Bio\nimport skbio\n\nprint(\"Biopython version:\", Bio.__version__)\nprint(\"scikit-bio version:\", skbio.__version__)\n\nBiopython version: 1.84\nscikit-bio version: 0.6.2\n\n\n\n\n2.0.1.5 Upload Files to Colab\n\nCreate 2024-kaist-lecture folder\nipynb file open with colab\nDownload ganbank files from ncbi and upload the files\ncurrent directory\n\n\n!pwd\n\n/home/haseong/lecture/kaist-deeplearning-2024\n\n\n\n현재 작업 디렉토리를 위 생성한 디렉토리로 변경\n\n\nimport os\nos.chdir('drive/MyDrive/2024-kaist-lecture')\n\n\n!pwd\n\n/content/drive/MyDrive/2024-kaist-lecture\n\n\n\n분석을 위한 genbank 등의 파일을 ncbi에서 다운로드 후 위 폴더에 복사\n또는 아래 코드를 이용해서 현재 작업 디렉토리에 업로드\n\n\nfrom google.colab import files\n\nuploaded = files.upload()\n\n# Listing the uploaded files\nfor filename in uploaded.keys():\n    print(filename)\n\n\n     \n     \n      Upload widget is only available when the cell has been executed in the\n      current browser session. Please rerun this cell to enable.\n      \n       \n\n\nSaving nn.png to nn.png\nnn.png\n\n\n\n\n\nimage.png\n\n\n\n\n\n2.0.2 NumPy\nNumPy is a powerful library for numerical operations and handling arrays.\n\n2.0.2.1 Basics of NumPy\nInstallation:\n!pip install numpy\n\nimport numpy as np\n\n\n# Creating a 1D array\narr1 = np.array([1, 2, 3, 4, 5])\n\n# Creating a 2D array\narr2 = np.array([[1, 2, 3], [4, 5, 6]])\n\nprint(arr1)\nprint(arr2)\n\n\n# Element-wise operations\narr3 = arr1 * 2\nprint(arr3)\n\n# Mathematical functions\nprint(np.sqrt(arr1))\n\n[1 2 3 4 5]\n[[1 2 3]\n [4 5 6]]\n[ 2  4  6  8 10]\n[1.         1.41421356 1.73205081 2.         2.23606798]\n\n\n\n\n2.0.2.2 Numpy datatype ndarray\n\n행렬이나 다차원 배열 처리용 파이썬 라이브러리\n같은 타입의 데이터만 허용\n리스트에 비해 20배 이상 빠른 속도\n\n\nimport numpy as np\n\ndisplay(np.ones(4))\ndisplay(np.ones((3, 4)))\ndisplay(np.ones((2, 3, 4)))\n\narray([1., 1., 1., 1.])\n\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.]])\n\n\narray([[[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]],\n\n       [[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]]])\n\n\n\n\n\nalt text\n\n\n\nCreate numpy objects\n\n\nimport numpy as np\n\narr = [1, 2, 3]\nprint(arr)\nprint(type(arr))\n\na = np.array([1,2,3])\nprint(a)\nprint(a.dtype)\nprint(a.shape)\nprint(type(a))\n\n[1, 2, 3]\n&lt;class 'list'&gt;\n[1 2 3]\nint64\n(3,)\n&lt;class 'numpy.ndarray'&gt;\n\n\n\narr2 = np.array([[1,2,3], [4,5,6]])\nprint(arr2)\nprint(type(arr2))\nprint(arr2.shape)\nprint(arr2.dtype)\n\n[[1 2 3]\n [4 5 6]]\n&lt;class 'numpy.ndarray'&gt;\n(2, 3)\nint64\n\n\n\nnumpy 자료형\n\n부호가 있는 정수 int(8, 16, 32, 64)\n부호가 없는 정수 uint(8 ,16, 32, 54)\n실수 float(16, 32, 64, 128)\n복소수 complex(64, 128, 256)\n불리언 bool\n문자열 string_\n파이썬 오프젝트 object\n유니코드 unicode_\n\nnp.zeros(), np.ones(), np.arange()\n행렬 연산 지원\n\n\na = np.arange(1, 10).reshape(3,3) # [1, 10)\nprint(a)\na = np.ones((3,4), dtype=np.int16)\nb = np.ones((3,4), dtype=np.int16)\nprint(a)\nprint(b)\nprint(a+b)\nprint(a-b)\n\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\n[[1 1 1 1]\n [1 1 1 1]\n [1 1 1 1]]\n[[1 1 1 1]\n [1 1 1 1]\n [1 1 1 1]]\n[[2 2 2 2]\n [2 2 2 2]\n [2 2 2 2]]\n[[0 0 0 0]\n [0 0 0 0]\n [0 0 0 0]]\n\n\n\nnumpy 함수\n\nnp.sqrt()\nnp.log()\nnp.square()\nnp.log()\nnp.ceil()\nnp.floor()\nnp.isnan()\nnp.sum()\nnp.mean()\nnp.std()\nnp.min()\n\n\n\n\n\n2.0.3 Simple linear regression\n\nModel\n\\[\ny_i = b_0 + b_1 x_i  + \\epsilon_i \\text{ where } \\epsilon_i \\sim \\text{ iid } N(0, \\sigma^2)\n\\]\nparameters\n\n\\[\n\\theta = \\{ b_0, b_1 \\}\n\\]\n\nFind \\(\\theta\\) that minimize residuals\n\n\\[\n\\sum_{i=i}^n r_i^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2  \\\\\n\\sum_{i=1}^n (y_i - \\hat{b_1}x_i - \\hat{b_0})^2\n\\]\n\nresiduals: difference between sample observed and estimated values\n\n\nimport numpy as np\n\nnp.random.seed(123)\nX = 2 * np.random.rand(20, 1)\nY = 4 + X*0.8 + np.random.rand(20, 1)\n\nX_b = np.c_[np.ones(len(X)), X]\n# print(X)\ntheta_best = np.linalg.inv(X_b.T @ X_b) @ (X_b.T) @ Y\nY_pred_org = X_b @ theta_best\n# print(theta_best)\n\nX_new = 2 * np.random.rand(100, 1)\nX_new_b = np.c_[np.ones(len(X_new)), X_new]\nY_pred = X_new_b @ theta_best\n\nimport matplotlib.pyplot as plt\nplt.scatter(X, Y, color=\"#000000\")\nplt.plot(X_new, Y_pred, color='#cccccc', label='Predictions')\n\n# Plot residuals\nfor i in range(len(Y)):\n    plt.vlines(x=X[i], ymin=min(Y[i], Y_pred_org[i]), ymax=max(Y[i], Y_pred_org[i]), color='green', linestyle='dotted')\n\n\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.plot()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2.0.4 Ordinary least sequare (OLS)\n\nModel\n\n\\[\ny_i = b_0 + b_1 x_i  + \\epsilon_i \\text{ where } \\epsilon_i \\sim \\text{ iid } N(0, \\sigma^2), i = 1, 2, ..., n\n\\]\n\\[\nY = X \\beta + \\epsilon\n\\]\n\\[\\begin{equation}\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n... \\\\\ny_n\n\\end{bmatrix}\n\n=\n\n\\begin{pmatrix}\n1 \\ \\ x_1 \\\\\n1 \\ \\ x_2 \\\\\n... \\\\\n1 \\ \\ x_n\n\\end{pmatrix}\n\n\\begin{pmatrix}\nb_0 \\\\\nb_1\n\\end{pmatrix}\n\n+\n\n\n\\begin{pmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n... \\\\\n\\epsilon_n\n\\end{pmatrix}\n\\end{equation}\\]\n\\[\n\\mathbf{\\epsilon} = Y - X\\beta\n\\]\n\nResidual Sum of Squares (RSS)\n\n\\[\nRSS = (Y-X\\beta)^T(Y-X\\beta)\n\\]\n\nTake the gradient with respect to \\(\\beta\\) and set it to zero (Normal equation)\n\n\\[\n\\frac{\\partial RSS}{\\partial \\beta} = -2 X^T Y + 2 X^T X \\beta = 0\n\\]\n\nSolve for \\(\\beta\\) if \\((X^TX)^{-1}\\) exists\n\n\\[\n(X^T X) \\beta = X^T Y\n\\]\n\\[\n\\hat{\\beta} = (X^T X)^{-1} X^T Y\n\\]\n\n\n2.0.5 Maximum Likelihood Estimation (MLE)\n\nDetails: https://statproofbook.github.io/P/slr-mle.html\nConsider the regression as a joint probability model\nReasons to use\n\nThis framework is applicable to other complex models (non-linear, neural network)\n\nBayes rule where \\(D\\) is data, \\(\\theta\\) is parameter\n\n\\[\np(\\theta | D) = \\frac{p(D|\\theta) p(\\theta)}{p(D)}\n\\]\n\\[\n\\text{where $p(\\theta|D)$, $p(D|\\theta)$, $p(\\theta)$ are posterior, likelihood and prior, respectively}\n\\]\n\\[\np(\\theta | D) \\propto p(D|\\theta)\n\\]\n\nRegarding the likelihood, \\(p(Y|X, \\theta)\\) is interpreted as how the behaviour of the response \\(Y\\) is conditional on the values of the feature, \\(X\\), and parameters, \\(\\theta\\)\n\n\\[\n\\begin{align}\np(Y | X, \\theta)  = \\prod_{i=1}^n p( y_i | x_i, \\hat{\\theta})\n\\end{align}\n\\]\n\nThen, we can ask what is the probability of seeing the data, given a specific set of parameters? (== How the data likely to be observed given the parameters == which parameters maximize the likelihood)\n\n\\[\n\\hat{\\theta} = \\text{argmax}_\\theta \\text{ log } \\sum_{i=1}^n p( y_i | x_i, \\theta)\n\\]\n\nFor \\(p(y_i | x_i, \\theta)\\), we have assumption that all feature vectors are iid\n\n\\[\nY \\sim N(X\\beta, \\sigma)\n\\]\n\\[\n\\begin{align}\np( y_i | x_i, \\theta) &= N(y_i; X\\beta, \\sigma^2) \\\\\n&= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp{\\left( - \\frac{(y_i - b_0 - b_1 x_i)^2}{2 \\sigma^2} \\right)}\n\\end{align}\n\\]\n\nLog likelihood (LL) function\n\n\\[\n\\begin{align}\nLL(\\theta) &= \\text{ log } \\left( \\prod_{i=1}^n p( y_i | x_i, \\theta) \\right) \\\\\n&= \\text{ log } \\left( \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp{ \\left(- \\frac{(y_i - b_0 - b_1 x_i)^2}{2 \\sigma^2} \\right)} \\right) \\\\\n&= \\text{ log } \\left( \\frac{1}{\\sqrt{(2\\pi\\sigma^2)^n}} \\exp{ \\left( - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - b_0 - b_1 x_i)^2 \\right)} \\right) \\\\\n&= - \\frac{n}{2} \\log(2\\pi) - \\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - b_0 - b_1 x_i)^2\n\\end{align}\n\\]\n\nTake the gradient with respect to \\(\\beta\\) and set it to zero (OLS)\n\n\\[\n\\frac{\\partial LL(b_0, b_1, \\sigma^2)}{\\partial b_0} = \\frac{1}{\\sigma^2} \\sum_{i=1}^n (y_i - b_0 - b_1 x_i)\n\\]\n\\[\n\\begin{align}\n\\frac{\\partial LL(\\hat{b}_0, \\hat{b}_1, \\sigma^2)}{\\partial b_0}  = 0 \\\\\n\\sum_{i=1}^n (y_i - \\hat{b}_0 - \\hat{b}_1 x_i) = 0 \\\\\n\\hat{b}_0 = \\frac{1}{n}\\sum_{i=1}^n y_i - \\hat{b}_1 \\frac{1}{n} \\sum_{i=1}^n x_i \\\\\n\\end{align}\n\\]\n\\[\n\\frac{\\partial LL(\\hat{b}_0, b_1, \\sigma^2)}{\\partial b_1} = \\frac{1}{\\sigma^2} \\sum_{i=1}^n (x_i y_i - \\hat{b}_0 x_i - b_1 x_i^2)\n\\]\n\\[\n\\begin{align}\n\\frac{\\partial LL(\\hat{b}_0, \\hat{b}_1, \\sigma^2)}{\\partial b_1}  = 0 \\\\\n\\hat{b}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2 }\n\\end{align}\n\\]\n\nMaximize with respect to \\(\\sigma^2\\)\n\n\\[\n\\frac{\\partial LL(\\hat{b}_0, \\hat{b}_1, \\sigma^2)}{\\partial \\sigma^2} = - \\frac{n}{2 \\sigma^2} + \\frac{1}{2 (\\sigma^2)^2} \\sum_{i=1}^n (y_i - \\hat{b}_0 - \\hat{b}_1 x_i)^2\n\\]\n\\[\n\\frac{\\partial LL(\\hat{b}_0, \\hat{b}_1, \\hat{\\sigma}^2)}{\\partial \\sigma^2} = 0 \\\\\n\\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{b}_0 - \\hat{b}_1 x_i)^2\n\\]\n\nIn linear regression, MLE naturally leads to the OLS solution under the assumption of normally distributed residuals.\n\n\\[\n\\hat{\\beta} = (X^T X)^{-1} X^T Y\n\\]\n\\[\n\\hat{\\sigma}^2 = \\frac{1}{n} (Y-X\\beta)^T (Y-X\\beta)\n\\]\n\nHowever, MLE’s flexibility (e.g. customizable likelihood) extends beyond linear models, making it indispensable for logistic regression, mixture models, and modern deep learning frameworks.\n\n\n\n2.0.6 Gradient Decent\n\nAn iterative optimization algorithm for adjusting \\(\\beta\\) by minimizing a cost function. In linear regression, the cost function is the Mean Squared Error (MSE) under the assumption of normally distributed residuals.\nDefine a cost function \\(J(\\theta)\\)\n\n\\[\nLL(\\theta) = - \\frac{n}{2} \\log(2\\pi) - \\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - b_0 - b_1 x_i)^2\n\\]\n\\[\nJ(\\beta) =  \\sum_{i=1}^n (y_i - b_0 - b_1 x_i)^2\n\\]\n\nmatrix notation\n\n\\[\nJ(\\beta) = || Y - X\\beta ||^2 = (Y - X\\beta)^T(Y - X\\beta)\n\\]\n\nL1 norm, L2 norm (a metric for length or magnitude of a vector/matrix) \\[\n||X||_1 = \\sum_{i}^n |x_i|\n\\]\n\n\\[\n||X||_2 = \\sqrt{\\sum_i^n x_i^2}\n\\]\n\\[\nJ(\\beta) = Y^T Y - 2 Y^T X \\beta + \\beta^T X^T X \\beta\n\\]\n\nGradient of the cost function\n\n\\[\n\\nabla_\\beta J(\\beta) = \\frac{\\partial J(\\beta)}{\\partial \\beta}\n\\]\n\\[\n\\begin{align}\n\\nabla_\\beta J(\\beta) &= 0 - 2 X^T Y + 2 X^T X \\beta \\\\\n&= - 2 X^T(Y-X\\beta)\n\\end{align}\n\\]\n\nparameter update rule\n\n\\[\n\\beta^{(t+1)} = \\beta^{(t)} - \\alpha \\nabla_\\beta J(\\beta) \\text{ where } \\alpha \\text{ is learning rate}\n\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate synthetic data\nnp.random.seed(42)\nX = 2 * np.random.rand(100, 1)  # 100 samples, 1 feature from [0, 1) uniform distribution\ny = 4 + 3 * X + np.random.randn(100, 1)  # y = 4 + 3X + noise (Gaussian noise)\n\n# Add bias term (intercept)\nX_b = np.c_[np.ones((X.shape[0], 1)), X]\n\n# Initialize parameters\nbeta = np.random.randn(2, 1)  # Random initial coefficients\nlearning_rate = 0.01\nn_iterations = 100\nm = X_b.shape[0]  # Number of samples\n\nbeta_updates = [beta.copy()]\n\n# Gradient Descent\nfor iteration in range(n_iterations):\n    gradients = -2/m * X_b.T @ (y - X_b @ beta)  # Compute gradient\n    beta = beta - learning_rate * gradients  # Update parameters\n    beta_updates.append(beta.copy())\n\n# Final parameters\nprint(\"Estimated coefficients (beta):\", beta)\n\n# Predictions\ny_pred = X_b @ beta\n\n# Plot the data and regression line\nplt.figure(figsize=(8, 6))\nplt.scatter(X, y, color=\"blue\", label=\"Data points\")\nplt.plot(X, y_pred, color=\"red\", label=\"Regression line\")\nplt.xlabel(\"X\")\nplt.ylabel(\"y\")\nplt.title(\"Linear Regression using Gradient Descent\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\nEstimated coefficients (beta): [[2.96262018]\n [3.80192916]]\n\n\n\n\n\n\n\n\n\n\n# Visualize beta updates\nfor i, beta in enumerate(beta_updates):\n    print(f\"Iteration {i}: beta = {beta.flatten()}\")\n\n# Plot convergence of coefficients\nbeta_updates = np.array(beta_updates).squeeze()\n\nplt.figure(figsize=(8, 6))\nplt.plot(range(n_iterations + 1), beta_updates[:, 0], label='Intercept (beta[0])')\nplt.plot(range(n_iterations + 1), beta_updates[:, 1], label='Slope (beta[1])')\nplt.xlabel('Iteration')\nplt.ylabel('Value of beta')\nplt.title('Convergence of Coefficients')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nIteration 0: beta = [0.01300189 1.45353408]\nIteration 1: beta = [0.12180499 1.56507648]\nIteration 2: beta = [0.22633422 1.67181808]\nIteration 3: beta = [0.32676535 1.77395782]\nIteration 4: beta = [0.42326689 1.8716864 ]\nIteration 5: beta = [0.5160004  1.96518667]\nIteration 6: beta = [0.60512075 2.05463391]\nIteration 7: beta = [0.69077645 2.14019616]\nIteration 8: beta = [0.77310984 2.22203453]\nIteration 9: beta = [0.85225741 2.30030345]\nIteration 10: beta = [0.92835001 2.37515099]\nIteration 11: beta = [1.00151308 2.44671909]\nIteration 12: beta = [1.0718669  2.51514384]\nIteration 13: beta = [1.13952675 2.58055569]\nIteration 14: beta = [1.20460319 2.64307972]\nIteration 15: beta = [1.26720221 2.70283582]\nIteration 16: beta = [1.32742539 2.75993895]\nIteration 17: beta = [1.38537016 2.81449929]\nIteration 18: beta = [1.4411299 2.8666225]\nIteration 19: beta = [1.49479416 2.91640985]\nIteration 20: beta = [1.54644877 2.96395843]\nIteration 21: beta = [1.59617603 3.00936133]\nIteration 22: beta = [1.64405484 3.05270779]\nIteration 23: beta = [1.69016085 3.09408334]\nIteration 24: beta = [1.73456657 3.13357001]\nIteration 25: beta = [1.77734155 3.17124641]\nIteration 26: beta = [1.81855245 3.20718793]\nIteration 27: beta = [1.85826316 3.24146681]\nIteration 28: beta = [1.89653497 3.27415234]\nIteration 29: beta = [1.93342661 3.30531092]\nIteration 30: beta = [1.96899442 3.33500621]\nIteration 31: beta = [2.00329239 3.36329925]\nIteration 32: beta = [2.03637228 3.39024855]\nIteration 33: beta = [2.06828373 3.4159102 ]\nIteration 34: beta = [2.09907433 3.44033798]\nIteration 35: beta = [2.1287897  3.46358343]\nIteration 36: beta = [2.15747358 3.48569598]\nIteration 37: beta = [2.1851679 3.506723 ]\nIteration 38: beta = [2.21191288 3.52670991]\nIteration 39: beta = [2.23774706 3.54570024]\nIteration 40: beta = [2.26270741 3.56373574]\nIteration 41: beta = [2.28682934 3.58085643]\nIteration 42: beta = [2.31014685 3.59710066]\nIteration 43: beta = [2.3326925 3.6125052]\nIteration 44: beta = [2.35449751 3.62710531]\nIteration 45: beta = [2.37559184 3.64093478]\nIteration 46: beta = [2.39600419 3.65402601]\nIteration 47: beta = [2.41576208 3.66641005]\nIteration 48: beta = [2.43489191 3.67811668]\nIteration 49: beta = [2.45341896 3.68917444]\nIteration 50: beta = [2.47136752 3.69961069]\nIteration 51: beta = [2.48876082 3.70945165]\nIteration 52: beta = [2.50562118 3.71872248]\nIteration 53: beta = [2.52196997 3.72744726]\nIteration 54: beta = [2.53782769 3.73564912]\nIteration 55: beta = [2.55321401 3.74335019]\nIteration 56: beta = [2.56814776 3.75057171]\nIteration 57: beta = [2.58264703 3.75733403]\nIteration 58: beta = [2.59672912 3.76365667]\nIteration 59: beta = [2.61041067 3.76955833]\nIteration 60: beta = [2.62370759 3.77505693]\nIteration 61: beta = [2.63663515 3.78016967]\nIteration 62: beta = [2.64920801 3.78491302]\nIteration 63: beta = [2.66144021 3.78930278]\nIteration 64: beta = [2.6733452  3.79335407]\nIteration 65: beta = [2.68493589 3.79708142]\nIteration 66: beta = [2.69622467 3.80049874]\nIteration 67: beta = [2.70722341 3.80361935]\nIteration 68: beta = [2.71794348 3.80645605]\nIteration 69: beta = [2.7283958  3.80902108]\nIteration 70: beta = [2.73859083 3.81132618]\nIteration 71: beta = [2.74853861 3.81338263]\nIteration 72: beta = [2.75824876 3.8152012 ]\nIteration 73: beta = [2.7677305  3.81679224]\nIteration 74: beta = [2.77699268 3.81816566]\nIteration 75: beta = [2.78604379 3.81933097]\nIteration 76: beta = [2.79489196 3.82029728]\nIteration 77: beta = [2.803545   3.82107331]\nIteration 78: beta = [2.81201037 3.82166745]\nIteration 79: beta = [2.82029527 3.82208769]\nIteration 80: beta = [2.82840657 3.82234175]\nIteration 81: beta = [2.83635086 3.82243698]\nIteration 82: beta = [2.84413447 3.82238045]\nIteration 83: beta = [2.85176348 3.82217893]\nIteration 84: beta = [2.85924369 3.8218389 ]\nIteration 85: beta = [2.8665807  3.82136659]\nIteration 86: beta = [2.87377985 3.82076795]\nIteration 87: beta = [2.88084627 3.8200487 ]\nIteration 88: beta = [2.8877849  3.81921431]\nIteration 89: beta = [2.89460044 3.81827003]\nIteration 90: beta = [2.90129743 3.81722089]\nIteration 91: beta = [2.90788021 3.8160717 ]\nIteration 92: beta = [2.91435295 3.81482709]\nIteration 93: beta = [2.92071965 3.81349148]\nIteration 94: beta = [2.92698413 3.81206911]\nIteration 95: beta = [2.93315007 3.81056405]\nIteration 96: beta = [2.93922099 3.8089802 ]\nIteration 97: beta = [2.94520029 3.80732128]\nIteration 98: beta = [2.9510912  3.80559087]\nIteration 99: beta = [2.95689684 3.8037924 ]\nIteration 100: beta = [2.96262018 3.80192916]\n\n\n\n\n\n\n\n\n\n\n2.0.6.1 Reasons to use GD instead of MLE, OLS\n\nGradient Descent is favored over MLE or OLS in scenarios involving large-scale data, high-dimensional features, non-linear models, or custom loss functions due to its flexibility, efficiency, and scalability. However, for simple, small-scale problems, OLS or MLE may still be preferred for their directness and precision.\n\n\n\n\n2.0.7 Model fitting\n\nFinding parameters with LSE, MLE, and GD\nGD provides more flexible (non-linear) and scalable (high-dimentional data) way for modeling\nGD procedure\n\nSet random initial parameters\nCompute gradient that reduces cost\nParameter update until conversing",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Day1 Optimization</span>"
    ]
  },
  {
    "objectID": "day2_neural_networks.html",
    "href": "day2_neural_networks.html",
    "title": "3  Day2 Neural Networks",
    "section": "",
    "text": "3.0.1 Modeling procedure (Testing vs. Prediction)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Day2 Neural Networks</span>"
    ]
  },
  {
    "objectID": "day2_neural_networks.html#pytorch",
    "href": "day2_neural_networks.html#pytorch",
    "title": "3  Day2 Neural Networks",
    "section": "3.1 PyTorch",
    "text": "3.1 PyTorch\nPyTorch is an open-source deep learning framework developed by Facebook’s AI Research lab (FAIR). It is widely used in research and industry due to its dynamic computation graph and ease of use.\nPyTorch Ecosystem Overview:\ntorch: The core library for tensor operations and automatic differentiation.\ntorch.nn: A sub-library used to build and train neural network models.\ntorch.optim: Tools for optimization algorithms (e.g., SGD, Adam).\ntorchvision: Provides datasets, pre-trained models, and image transformations.\nTensors\nTensors are the primary data structures in PyTorch, analogous to NumPy arrays but with added capabilities such as the ability to run on GPUs for faster computation.\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n\n\n\n# Input data and true labels\nX = torch.tensor([[0.5, 0.2], [0.1, 0.4], [0.6, 0.9]], dtype=torch.float32)  # Shape (3 samples, 2 features)\ny = torch.tensor([[1], [0], [1]], dtype=torch.float32)                      # True labels (Shape: 3x1)\n\n# Define the neural network\nclass SimpleNN(nn.Module):\n    def __init__(self):\n        super(SimpleNN, self).__init__()\n        # Define layers\n        self.hidden = nn.Linear(2, 3)  # Input to hidden layer (2 inputs, 3 hidden nodes)\n        self.output = nn.Linear(3, 1)  # Hidden to output layer (3 hidden nodes, 1 output)\n\n    def forward(self, x):\n        # Forward pass: Input -&gt; Hidden Layer -&gt; Output Layer\n        x = torch.relu(self.hidden(x))       # ReLU activation for hidden layer\n        x = torch.sigmoid(self.output(x))    # Sigmoid activation for output layer\n        return x\n\n# Initialize the network\nmodel = SimpleNN()\n\n# Loss function and optimizer\ncriterion = nn.MSELoss()  # Mean Squared Error Loss\noptimizer = optim.SGD(model.parameters(), lr=0.1)  # Stochastic Gradient Descent optimizer\n\n# Training parameters\nepochs = 1000\n\n# Containers to store loss and accuracy for each epoch\nloss_history = []\naccuracy_history = []\n\n\n# Training loop\nfor epoch in range(epochs):\n    # Forward pass\n    outputs = model(X)  # Predicted outputs\n    loss = criterion(outputs, y)  # Compute loss\n\n    # Compute accuracy\n    predicted = (outputs &gt;= 0.5).float()  # Threshold at 0.5 for binary classification\n    accuracy = (predicted == y).sum().item() / y.size(0)\n\n\n    # Backpropagation\n    optimizer.zero_grad()  # Clear previous gradients\n    loss.backward()        # Compute gradients\n    optimizer.step()       # Update weights\n\n    # Record loss and accuracy\n    loss_history.append(loss.item())\n    accuracy_history.append(accuracy)\n\n\n    # Print loss every 100 epochs\n    if epoch % 100 == 0:\n        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n\n# Final predictions\nwith torch.no_grad():  # No need to compute gradients during inference\n    final_outputs = model(X)\n    print(\"\\nFinal Predictions:\")\n    print(final_outputs)\n\n\n# Plot the loss and accuracy over epochs\nplt.figure(figsize=(12, 5))\n\n# Loss plot\nplt.subplot(1, 2, 1)\nplt.plot(range(epochs), loss_history, label=\"Loss\", color=\"red\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss over Epochs\")\nplt.grid(True)\nplt.legend()\n\n# Accuracy plot\nplt.subplot(1, 2, 2)\nplt.plot(range(epochs), accuracy_history, label=\"Accuracy\", color=\"blue\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy over Epochs\")\nplt.grid(True)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\nEpoch 0, Loss: 0.2275\nEpoch 100, Loss: 0.2192\nEpoch 200, Loss: 0.2141\nEpoch 300, Loss: 0.2061\nEpoch 400, Loss: 0.1925\nEpoch 500, Loss: 0.1692\nEpoch 600, Loss: 0.1317\nEpoch 700, Loss: 0.0874\nEpoch 800, Loss: 0.0543\nEpoch 900, Loss: 0.0376\n\nFinal Predictions:\ntensor([[0.8810],\n        [0.2476],\n        [0.9134]])",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Day2 Neural Networks</span>"
    ]
  },
  {
    "objectID": "day3_cnn_dna.html",
    "href": "day3_cnn_dna.html",
    "title": "4  Day3 Convolution Neural Networks with DNA sequence",
    "section": "",
    "text": "4.0.1 Objectives\n\nDeveloping a CNN Model for classifying sequences\nExample: Developing a model to identify specific DNA motifs bound by an arbitrary transcription factor. The model predicts whether a given transcription factor binds to an input DNA sequence (output: 1) or not (output: 0).”\n\n\n\n4.0.2 Data\n\nTo train a deep learning model, labeled data is required (though not always necessary in modern self-supervised learning approaches).\nIn sequence analysis, DNA sequences paired with their corresponding phenotypes can serve as labeled data (genotype-phenotype paired data).\n\nFor example, to train a deep learning model to predict DNA sequences bound by a specific transcription factor, you would need a dataset containing transcription factor sequence data along with labels indicating whether or not the transcription factor binds to a given DNA sequence (True or False).\n\nGenerally, data for statistical analysis is represented as a 2D array, with rows corresponding to samples and columns to variables. In deep learning, data is represented in the same way. The number of samples required depends on the complexity of the model, but typically, at least thousands of samples are needed. Using tens of thousands or more samples is recommended for optimal results.\nA dataset collected for deep learning is divided into training and test datasets. Sometimes, the training dataset is further split into training and validation datasets for model development and performance evaluation.\n\n\n\n4.0.3 One-hot encoding\n\nFor deep learning, data must be represented numerically in a format that machines can process.\nOne-hot encoding is one of the most widely used methods in deep learning.\nFor DNA sequences with four types of nucleotides, encoding can be done as follows:\n\n“A” → [1, 0, 0, 0]\n\n“T” → [0, 0, 0, 1]\n\n“G” → [0, 0, 1, 0]\n\n“C” → [0, 1, 0, 0]\n\n\n\nimport numpy as np\n\nmy_string=\"ATACAA\"\nmy_array=np.array(list(my_string))\nprint(my_array)\n\n['A' 'T' 'A' 'C' 'A' 'A']\n\n\n\nlist(my_string)\n\n['A', 'T', 'A', 'C', 'A', 'A']\n\n\n\nnp.zeros((7,5))\n\narray([[0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.]])\n\n\n\nbox = np.zeros((3, 7, 5))\ntype(box)\nbox\n\narray([[[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]]])\n\n\n\nonehot_encode = np.zeros((len(my_array),4), dtype=int)\nbase_dict = {\"A\":0, \"C\":1, \"G\":2, \"T\":3}\nfor i in range(len(my_array)):\n    onehot_encode[i, base_dict[my_array[i]]] = 1\n\nprint(onehot_encode)\nprint(onehot_encode.shape)\n\n[[1 0 0 0]\n [0 0 0 1]\n [1 0 0 0]\n [0 1 0 0]\n [1 0 0 0]\n [1 0 0 0]]\n(6, 4)\n\n\n\n\n4.0.4 Set sequence motif for simulation\n\nUnderstanding the concepts of PFM (Position Frequency Matrix) and PWM (Position Weight Matrix) is essential.\nAssuming an alignment of several sequences, the PFM represents the frequency of each nucleotide (A, T, G, C) at specific positions in the sequences, while the PWM represents the proportion of each nucleotide at those positions.\n\n\nfrom Bio import motifs\nfrom Bio.Seq import Seq\n\ninstances = [Seq(\"TACAA\"), Seq(\"TACGA\"), Seq(\"TACAA\")]\nm = motifs.create(instances)\npfm = m.counts\nprint(pfm)\npwm = m.counts.normalize(pseudocounts=0.5)\nprint (pwm)\n\n        0      1      2      3      4\nA:   0.00   3.00   0.00   2.00   3.00\nC:   0.00   0.00   3.00   0.00   0.00\nG:   0.00   0.00   0.00   1.00   0.00\nT:   3.00   0.00   0.00   0.00   0.00\n\n        0      1      2      3      4\nA:   0.10   0.70   0.10   0.50   0.70\nC:   0.10   0.10   0.70   0.10   0.10\nG:   0.10   0.10   0.10   0.30   0.10\nT:   0.70   0.10   0.10   0.10   0.10\n\n\n\n\nPseudocounts are used in calculations to avoid division by NULL or zero.\nThe PWM (Position Weight Matrix) of a specific sequence motif can be used to search for the motif’s location in a new sequence provided in one-hot encoding format.\nUsing a sliding window approach, the motif can be scanned from the beginning to the end of the sequence to identify its presence.\nThe following demonstrates how to detect the presence of a given PWM motif (assuming a binary 0 and 1 representation) in a sequence.\n\n\n\n\nalt text\n\n\n\nIf there is a ‘5’ in the length-3 array as shown above, it indicates that the target sequence contains a sequence matching the motif.\nTo search for the presence of the PWM motif in the sequence “ATACAA,” a sliding window of length 5 can be used to divide the sequence into two sub-sequences: “ATACA” and “TACAA.”\n\nBy converting these two sub-sequences into one-hot encoding and multiplying their elements with the corresponding elements of the PWM, only the PWM values at the non-zero positions of the one-hot encoding remain.\nTo quantify how similar a given sequence is to the motif: 1. Multiply all non-zero values from the PWM. 2. Take the logarithm of the result.\nThe resulting scalar value indicates the similarity between the sequence and the motif. Theoretically, a value of 0 implies an identical sequence match to the motif.\n\n\n\npwm_arr = np.array(list(pwm.values())).transpose()\nprint(pwm_arr.shape)\n\nprint(onehot_encode.shape)\nprint(onehot_encode[0:5,].shape)\nprint(onehot_encode[1:6,].shape)\n\ns1 = np.multiply(onehot_encode[0:5,], pwm_arr)\ns2 = np.multiply(onehot_encode[1:6,], pwm_arr)\nprint(s1)\nprint(s2)\n\nprint(np.sum(s1, axis=1))\nprint(np.prod(np.sum(s1, axis=1)))\n\nprint(np.log(np.prod(np.sum(s1, axis=1)))) #s1 score\nprint(np.log(np.prod(np.sum(s2, axis=1)))) #s2 score\n\n(5, 4)\n(6, 4)\n(5, 4)\n(5, 4)\n[[0.1 0.  0.  0. ]\n [0.  0.  0.  0.1]\n [0.1 0.  0.  0. ]\n [0.  0.1 0.  0. ]\n [0.7 0.  0.  0. ]]\n[[0.  0.  0.  0.7]\n [0.7 0.  0.  0. ]\n [0.  0.7 0.  0. ]\n [0.5 0.  0.  0. ]\n [0.7 0.  0.  0. ]]\n[0.1 0.1 0.1 0.1 0.7]\n7.000000000000002e-05\n-9.567015315914915\n-2.119846956314875\n\n\n\nDeep learning styled array\n\n\n\n\nalt text\n\n\n\n\n4.0.5 Simulation data generation\nGenerate 1,000 simulated positive sequences by embedding a motif in the middle of the sequences, and 1,000 negative sequences with random DNA sequences.\n\nimport numpy as np\nseq_length = 20\nnum_sample = 1000\n#motif CCGGAA\nmotif_pwm = np.array([[10.41, 22.86, 1.92, 1.55, 98.60, 86.66],\n            [68.20, 65.25, 0.50, 0.35, 0.25, 2.57],\n            [17.27, 8.30, 94.77, 97.32, 0.87, 0.00],\n            [4.13, 3.59, 2.81, 0.78, 0.28, 10.77]])\npwm = np.hstack([np.ones((4, 7)), motif_pwm, np.ones((4, 7))])\npos = np.array([np.random.choice( ['A', 'C', 'G', 'T'], num_sample,\n                                  p=pwm[:,i]/sum(pwm[:,i])) for i in range(seq_length)]).transpose()\nneg = np.array([np.random.choice( ['A', 'C', 'G', 'T'], num_sample,\n                                  p=np.array([1,1,1,1])/4) for i in range(seq_length)]).transpose()\n\nprint(pos.shape)\ndisplay([''.join(x) for x in pos[1:5,]])\nprint()\ndisplay([''.join(x) for x in neg[1:5,]])\n\n(1000, 20)\n\n\n\n['CTAAAGACCGGAACAAAATG',\n 'TGGTGCGCCGGAAATGCGTG',\n 'CAACGTGACTGAATATCTCG',\n 'TCCGTCCGCGGAAACAGAGC']\n\n\n['TGTAAGCAATGCGTCTTGCG',\n 'CTCGGCTTATCAGGACGATG',\n 'GGCTTGTATCAGCTAGTCTA',\n 'GGTGTGAGGGTGTCACCGTG']\n\n\n\n\n4.0.6 DNA data preprocessing\n\nbase_dict = {'A':0, 'C':1, 'G':2, 'T':3}\n\n# response variable for pos\nonehot_encode_pos = np.zeros((num_sample, seq_length, 4))\nonehot_encode_pos_label = np.zeros((num_sample, 2), dtype=int)\nonehot_encode_pos_label[:,0] = 1\n# print(onehot_encode_pos_label)\n\n# response variable for pos\nonehot_encode_neg = np.zeros((num_sample, seq_length, 4))\nonehot_encode_neg_label = np.zeros((num_sample, 2), dtype=int)\nonehot_encode_neg_label[:,1] = 1\n# print(onehot_encode_neg_label)\n\n# convert sequence to onehot\nfor i in range(num_sample):\n    for j in range(seq_length):\n        onehot_encode_pos[i,j,base_dict[pos[i,j]]] = 1\n        onehot_encode_neg[i,j,base_dict[neg[i,j]]] = 1\n\n# concatenation\nX = np.vstack((onehot_encode_pos, onehot_encode_neg))\ny = np.vstack((onehot_encode_pos_label, onehot_encode_neg_label))\n\nprint(X.shape, y.shape)\n# (2000, 20, 4) (2000, 2)\n\n(2000, 20, 4) (2000, 2)\n\n\n\nPyTorch Conv1d requires [batch_size, channels, length] so transpose(1,2) excuted\n\n\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\n# 데이터를 훈련 세트와 테스트 세트로 나눔\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=125)\nprint(X_train.shape, y_train.shape)\n\n# NumPy 배열을 PyTorch 텐서로 변환\nX_train = torch.tensor(X_train, dtype=torch.float32).transpose(1,2)\nX_test = torch.tensor(X_test, dtype=torch.float32).transpose(1,2)\ny_train = torch.tensor(y_train, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.float32)\nprint(y_test.dtype)\n\n# DataLoader 설정\ntrain_dataset = TensorDataset(X_train, y_train)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nprint(train_loader.dataset.tensors[0].shape)\nprint(train_loader.dataset.tensors[1].shape)\ntest_dataset = TensorDataset(X_test, y_test)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n(1600, 20, 4) (1600, 2)\ntorch.float32\ntorch.Size([1600, 4, 20])\ntorch.Size([1600, 2])\n\n\n\nimport torch\n\nX_torch = torch.tensor(X_train, dtype=torch.float32)\nprint(X_torch.shape)\n\ntorch.Size([1600, 4, 20])\n\n\n/tmp/ipykernel_2688/3124571761.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_torch = torch.tensor(X_train, dtype=torch.float32)\n\n\n\n\n4.0.7 CNN model class\n\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass DNA_CNN(nn.Module):\n    def __init__(self):\n        super(DNA_CNN, self).__init__()\n        self.conv1 = nn.Conv1d(in_channels=4, out_channels=16, kernel_size=3, padding=1)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool1d(kernel_size=2)\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(160, 64)  # Adjust the input features according to your pooling and conv1d output\n        self.fc2 = nn.Linear(64, 2)  # Adjust according to your problem's needs (e.g., number of classes)\n        #self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.flatten(x)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        #x = self.softmax(x)\n        return x\n\nmodel = DNA_CNN()\nif torch.cuda.is_available():\n    model.cuda()\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nfrom torchsummary import summary\nsummary(model, input_size=(4, 20))  # (Channels, Length)\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv1d-1               [-1, 16, 20]             208\n              ReLU-2               [-1, 16, 20]               0\n         MaxPool1d-3               [-1, 16, 10]               0\n           Flatten-4                  [-1, 160]               0\n            Linear-5                   [-1, 64]          10,304\n            Linear-6                    [-1, 2]             130\n================================================================\nTotal params: 10,642\nTrainable params: 10,642\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.01\nParams size (MB): 0.04\nEstimated Total Size (MB): 0.05\n----------------------------------------------------------------\n\n\n\n\n4.0.8 Training\n\n# 훈련 루프\nnum_epochs = 20\nfor epoch in range(num_epochs):\n    for inputs, labels in train_loader:\n        if torch.cuda.is_available():\n            inputs, labels = inputs.cuda(), labels.cuda()\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n\nEpoch [1/20], Loss: 0.5050\nEpoch [2/20], Loss: 0.2472\nEpoch [3/20], Loss: 0.1437\nEpoch [4/20], Loss: 0.0821\nEpoch [5/20], Loss: 0.0966\nEpoch [6/20], Loss: 0.0719\nEpoch [7/20], Loss: 0.0729\nEpoch [8/20], Loss: 0.0387\nEpoch [9/20], Loss: 0.1018\nEpoch [10/20], Loss: 0.1520\nEpoch [11/20], Loss: 0.0712\nEpoch [12/20], Loss: 0.0277\nEpoch [13/20], Loss: 0.0272\nEpoch [14/20], Loss: 0.0318\nEpoch [15/20], Loss: 0.0215\nEpoch [16/20], Loss: 0.0464\nEpoch [17/20], Loss: 0.0975\nEpoch [18/20], Loss: 0.0566\nEpoch [19/20], Loss: 0.0191\nEpoch [20/20], Loss: 0.0516\n\n\n\n\n4.0.9 Model evaluation\n\n# 모델 평가\nmodel.eval()\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for inputs, labels in test_loader:\n        if torch.cuda.is_available():\n            inputs, labels = inputs.cuda(), labels.cuda()\n        outputs = model(inputs)\n        #print(outputs.data)\n        _, predicted = torch.max(outputs.data, 1)\n        #print(predicted)\n        total += labels.size(0)\n        labels_max = torch.max(labels, 1)[1]\n        #print(labels_max)\n        correct += (predicted == labels_max).sum().item()\n\n    print(f'Accuracy of the model on the test images: {100 * correct / total} %')\n\nAccuracy of the model on the test images: 98.5 %\n\n\n\n\n4.0.10 Training and evaluation\n\nimport matplotlib.pyplot as plt\n\n# 데이터 저장을 위한 리스트 초기화\ntrain_losses = []\nval_accuracies = []\n\nnum_epochs = 200\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        if torch.cuda.is_available():\n            inputs, labels = inputs.cuda(), labels.cuda()\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * inputs.size(0)\n\n    epoch_loss = running_loss / len(train_loader.dataset)\n    train_losses.append(epoch_loss)\n\n    # 모델 평가\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            if torch.cuda.is_available():\n                inputs, labels = inputs.cuda(), labels.cuda()\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            labels_max = torch.max(labels, 1)[1]\n            correct += (predicted == labels_max).sum().item()\n\n    epoch_accuracy = 100 * correct / total\n    val_accuracies.append(epoch_accuracy)\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n\n# 그래프 그리기\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label='Training Loss')\nplt.title('Training Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(val_accuracies, label='Validation Accuracy')\nplt.title('Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy (%)')\nplt.legend()\n\nplt.show()\n\nEpoch [1/200], Loss: 0.5790, Accuracy: 95.00%\nEpoch [2/200], Loss: 0.3117, Accuracy: 96.25%\nEpoch [3/200], Loss: 0.1510, Accuracy: 98.00%\nEpoch [4/200], Loss: 0.0968, Accuracy: 97.75%\nEpoch [5/200], Loss: 0.0771, Accuracy: 98.00%\nEpoch [6/200], Loss: 0.0696, Accuracy: 97.25%\nEpoch [7/200], Loss: 0.0605, Accuracy: 98.00%\nEpoch [8/200], Loss: 0.0557, Accuracy: 98.00%\nEpoch [9/200], Loss: 0.0577, Accuracy: 98.25%\nEpoch [10/200], Loss: 0.0551, Accuracy: 97.75%\nEpoch [11/200], Loss: 0.0476, Accuracy: 98.50%\nEpoch [12/200], Loss: 0.0437, Accuracy: 97.50%\nEpoch [13/200], Loss: 0.0409, Accuracy: 98.00%\nEpoch [14/200], Loss: 0.0382, Accuracy: 98.00%\nEpoch [15/200], Loss: 0.0367, Accuracy: 97.50%\nEpoch [16/200], Loss: 0.0407, Accuracy: 98.00%\nEpoch [17/200], Loss: 0.0345, Accuracy: 98.00%\nEpoch [18/200], Loss: 0.0343, Accuracy: 98.00%\nEpoch [19/200], Loss: 0.0342, Accuracy: 97.50%\nEpoch [20/200], Loss: 0.0363, Accuracy: 97.50%\nEpoch [21/200], Loss: 0.0305, Accuracy: 98.00%\nEpoch [22/200], Loss: 0.0271, Accuracy: 98.00%\nEpoch [23/200], Loss: 0.0275, Accuracy: 98.00%\nEpoch [24/200], Loss: 0.0250, Accuracy: 98.00%\nEpoch [25/200], Loss: 0.0278, Accuracy: 97.75%\nEpoch [26/200], Loss: 0.0234, Accuracy: 98.00%\nEpoch [27/200], Loss: 0.0214, Accuracy: 97.75%\nEpoch [28/200], Loss: 0.0225, Accuracy: 97.50%\nEpoch [29/200], Loss: 0.0213, Accuracy: 97.25%\nEpoch [30/200], Loss: 0.0199, Accuracy: 97.50%\nEpoch [31/200], Loss: 0.0189, Accuracy: 97.50%\nEpoch [32/200], Loss: 0.0169, Accuracy: 97.00%\nEpoch [33/200], Loss: 0.0186, Accuracy: 97.75%\nEpoch [34/200], Loss: 0.0185, Accuracy: 97.00%\nEpoch [35/200], Loss: 0.0149, Accuracy: 98.00%\nEpoch [36/200], Loss: 0.0147, Accuracy: 97.50%\nEpoch [37/200], Loss: 0.0142, Accuracy: 97.00%\nEpoch [38/200], Loss: 0.0131, Accuracy: 97.50%\nEpoch [39/200], Loss: 0.0139, Accuracy: 97.50%\nEpoch [40/200], Loss: 0.0126, Accuracy: 97.75%\nEpoch [41/200], Loss: 0.0120, Accuracy: 97.75%\nEpoch [42/200], Loss: 0.0099, Accuracy: 97.75%\nEpoch [43/200], Loss: 0.0114, Accuracy: 97.50%\nEpoch [44/200], Loss: 0.0092, Accuracy: 97.75%\nEpoch [45/200], Loss: 0.0086, Accuracy: 97.50%\nEpoch [46/200], Loss: 0.0077, Accuracy: 97.50%\nEpoch [47/200], Loss: 0.0072, Accuracy: 97.25%\nEpoch [48/200], Loss: 0.0088, Accuracy: 97.50%\nEpoch [49/200], Loss: 0.0095, Accuracy: 97.25%\nEpoch [50/200], Loss: 0.0083, Accuracy: 97.00%\nEpoch [51/200], Loss: 0.0065, Accuracy: 97.25%\nEpoch [52/200], Loss: 0.0061, Accuracy: 97.50%\nEpoch [53/200], Loss: 0.0049, Accuracy: 97.50%\nEpoch [54/200], Loss: 0.0058, Accuracy: 97.50%\nEpoch [55/200], Loss: 0.0043, Accuracy: 97.50%\nEpoch [56/200], Loss: 0.0043, Accuracy: 97.75%\nEpoch [57/200], Loss: 0.0043, Accuracy: 97.75%\nEpoch [58/200], Loss: 0.0040, Accuracy: 97.50%\nEpoch [59/200], Loss: 0.0034, Accuracy: 97.50%\nEpoch [60/200], Loss: 0.0040, Accuracy: 97.50%\nEpoch [61/200], Loss: 0.0038, Accuracy: 97.50%\nEpoch [62/200], Loss: 0.0034, Accuracy: 97.50%\nEpoch [63/200], Loss: 0.0028, Accuracy: 97.50%\nEpoch [64/200], Loss: 0.0027, Accuracy: 97.50%\nEpoch [65/200], Loss: 0.0025, Accuracy: 97.75%\nEpoch [66/200], Loss: 0.0024, Accuracy: 97.50%\nEpoch [67/200], Loss: 0.0023, Accuracy: 97.75%\nEpoch [68/200], Loss: 0.0024, Accuracy: 97.75%\nEpoch [69/200], Loss: 0.0025, Accuracy: 97.50%\nEpoch [70/200], Loss: 0.0020, Accuracy: 97.50%\nEpoch [71/200], Loss: 0.0018, Accuracy: 97.50%\nEpoch [72/200], Loss: 0.0019, Accuracy: 97.50%\nEpoch [73/200], Loss: 0.0018, Accuracy: 97.50%\nEpoch [74/200], Loss: 0.0017, Accuracy: 97.50%\nEpoch [75/200], Loss: 0.0015, Accuracy: 97.50%\nEpoch [76/200], Loss: 0.0016, Accuracy: 97.50%\nEpoch [77/200], Loss: 0.0016, Accuracy: 97.50%\nEpoch [78/200], Loss: 0.0014, Accuracy: 97.50%\nEpoch [79/200], Loss: 0.0013, Accuracy: 97.50%\nEpoch [80/200], Loss: 0.0014, Accuracy: 97.50%\nEpoch [81/200], Loss: 0.0013, Accuracy: 97.50%\nEpoch [82/200], Loss: 0.0012, Accuracy: 97.50%\nEpoch [83/200], Loss: 0.0012, Accuracy: 97.50%\nEpoch [84/200], Loss: 0.0012, Accuracy: 97.75%\nEpoch [85/200], Loss: 0.0011, Accuracy: 97.50%\nEpoch [86/200], Loss: 0.0010, Accuracy: 97.50%\nEpoch [87/200], Loss: 0.0010, Accuracy: 97.50%\nEpoch [88/200], Loss: 0.0010, Accuracy: 97.50%\nEpoch [89/200], Loss: 0.0009, Accuracy: 97.50%\nEpoch [90/200], Loss: 0.0008, Accuracy: 97.50%\nEpoch [91/200], Loss: 0.0008, Accuracy: 97.50%\nEpoch [92/200], Loss: 0.0008, Accuracy: 97.50%\nEpoch [93/200], Loss: 0.0008, Accuracy: 97.50%\nEpoch [94/200], Loss: 0.0008, Accuracy: 97.50%\nEpoch [95/200], Loss: 0.0008, Accuracy: 97.50%\nEpoch [96/200], Loss: 0.0007, Accuracy: 97.50%\nEpoch [97/200], Loss: 0.0007, Accuracy: 97.50%\nEpoch [98/200], Loss: 0.0007, Accuracy: 97.50%\nEpoch [99/200], Loss: 0.0006, Accuracy: 97.50%\nEpoch [100/200], Loss: 0.0006, Accuracy: 97.50%\nEpoch [101/200], Loss: 0.0006, Accuracy: 97.50%\nEpoch [102/200], Loss: 0.0006, Accuracy: 97.50%\nEpoch [103/200], Loss: 0.0006, Accuracy: 97.50%\nEpoch [104/200], Loss: 0.0005, Accuracy: 97.50%\nEpoch [105/200], Loss: 0.0005, Accuracy: 97.50%\nEpoch [106/200], Loss: 0.0005, Accuracy: 97.50%\nEpoch [107/200], Loss: 0.0005, Accuracy: 97.50%\nEpoch [108/200], Loss: 0.0005, Accuracy: 97.50%\nEpoch [109/200], Loss: 0.0005, Accuracy: 97.50%\nEpoch [110/200], Loss: 0.0004, Accuracy: 97.50%\nEpoch [111/200], Loss: 0.0005, Accuracy: 97.50%\nEpoch [112/200], Loss: 0.0004, Accuracy: 97.50%\nEpoch [113/200], Loss: 0.0004, Accuracy: 97.50%\nEpoch [114/200], Loss: 0.0004, Accuracy: 97.50%\nEpoch [115/200], Loss: 0.0004, Accuracy: 97.50%\nEpoch [116/200], Loss: 0.0004, Accuracy: 97.50%\nEpoch [117/200], Loss: 0.0004, Accuracy: 97.50%\nEpoch [118/200], Loss: 0.0004, Accuracy: 97.50%\nEpoch [119/200], Loss: 0.0004, Accuracy: 97.50%\nEpoch [120/200], Loss: 0.0003, Accuracy: 97.50%\nEpoch [121/200], Loss: 0.0003, Accuracy: 97.50%\nEpoch [122/200], Loss: 0.0004, Accuracy: 97.50%\nEpoch [123/200], Loss: 0.0003, Accuracy: 97.50%\nEpoch [124/200], Loss: 0.0003, Accuracy: 97.50%\nEpoch [125/200], Loss: 0.0003, Accuracy: 97.50%\nEpoch [126/200], Loss: 0.0003, Accuracy: 97.50%\nEpoch [127/200], Loss: 0.0003, Accuracy: 97.50%\nEpoch [128/200], Loss: 0.0003, Accuracy: 97.50%\nEpoch [129/200], Loss: 0.0003, Accuracy: 97.50%\nEpoch [130/200], Loss: 0.0003, Accuracy: 97.50%\nEpoch [131/200], Loss: 0.0003, Accuracy: 97.50%\nEpoch [132/200], Loss: 0.0003, Accuracy: 97.50%\nEpoch [133/200], Loss: 0.0002, Accuracy: 97.50%\nEpoch [134/200], Loss: 0.0002, Accuracy: 97.50%\nEpoch [135/200], Loss: 0.0002, Accuracy: 97.50%\nEpoch [136/200], Loss: 0.0002, Accuracy: 97.50%\nEpoch [137/200], Loss: 0.0002, Accuracy: 97.50%\nEpoch [138/200], Loss: 0.0002, Accuracy: 97.50%\nEpoch [139/200], Loss: 0.0002, Accuracy: 97.50%\nEpoch [140/200], Loss: 0.0002, Accuracy: 97.50%\nEpoch [141/200], Loss: 0.0002, Accuracy: 97.50%\nEpoch [142/200], Loss: 0.0002, Accuracy: 97.50%\nEpoch [143/200], Loss: 0.0002, Accuracy: 97.50%\nEpoch [144/200], Loss: 0.0002, Accuracy: 97.50%\nEpoch [145/200], Loss: 0.0002, Accuracy: 97.50%\nEpoch [146/200], Loss: 0.0002, Accuracy: 97.50%\nEpoch [147/200], Loss: 0.0002, Accuracy: 97.50%\nEpoch [148/200], Loss: 0.0002, Accuracy: 97.50%\nEpoch [149/200], Loss: 0.0002, Accuracy: 97.50%\nEpoch [150/200], Loss: 0.0002, Accuracy: 97.50%\nEpoch [151/200], Loss: 0.0002, Accuracy: 97.50%\nEpoch [152/200], Loss: 0.0002, Accuracy: 97.50%\nEpoch [153/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [154/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [155/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [156/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [157/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [158/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [159/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [160/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [161/200], Loss: 0.0001, Accuracy: 97.25%\nEpoch [162/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [163/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [164/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [165/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [166/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [167/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [168/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [169/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [170/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [171/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [172/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [173/200], Loss: 0.0001, Accuracy: 97.25%\nEpoch [174/200], Loss: 0.0001, Accuracy: 97.25%\nEpoch [175/200], Loss: 0.0001, Accuracy: 97.25%\nEpoch [176/200], Loss: 0.0001, Accuracy: 97.25%\nEpoch [177/200], Loss: 0.0001, Accuracy: 97.00%\nEpoch [178/200], Loss: 0.0001, Accuracy: 97.25%\nEpoch [179/200], Loss: 0.0001, Accuracy: 97.00%\nEpoch [180/200], Loss: 0.0001, Accuracy: 97.25%\nEpoch [181/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [182/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [183/200], Loss: 0.0001, Accuracy: 97.25%\nEpoch [184/200], Loss: 0.0001, Accuracy: 97.25%\nEpoch [185/200], Loss: 0.0001, Accuracy: 97.25%\nEpoch [186/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [187/200], Loss: 0.0001, Accuracy: 97.00%\nEpoch [188/200], Loss: 0.0001, Accuracy: 97.25%\nEpoch [189/200], Loss: 0.0001, Accuracy: 97.25%\nEpoch [190/200], Loss: 0.0001, Accuracy: 97.00%\nEpoch [191/200], Loss: 0.0001, Accuracy: 97.00%\nEpoch [192/200], Loss: 0.0001, Accuracy: 97.25%\nEpoch [193/200], Loss: 0.0001, Accuracy: 97.00%\nEpoch [194/200], Loss: 0.0001, Accuracy: 97.25%\nEpoch [195/200], Loss: 0.0001, Accuracy: 97.00%\nEpoch [196/200], Loss: 0.0001, Accuracy: 97.25%\nEpoch [197/200], Loss: 0.0001, Accuracy: 97.00%\nEpoch [198/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [199/200], Loss: 0.0001, Accuracy: 97.25%\nEpoch [200/200], Loss: 0.0001, Accuracy: 97.00%",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Day3 Convolution Neural Networks with DNA sequence</span>"
    ]
  },
  {
    "objectID": "day4_cnn.html",
    "href": "day4_cnn.html",
    "title": "5  Day4 CNN with details",
    "section": "",
    "text": "5.1 Overview\nConvolutional Neural Networks (CNNs) are a type of deep learning model specifically designed for recognizing patterns and spatial hierarchies in data. While traditionally used in image processing, CNNs are also powerful for tasks involving sequential data, such as DNA sequences, because of their ability to detect local patterns.\nhttps://nafizshahriar.medium.com/what-is-convolutional-neural-network-cnn-deep-learning-b3921bdd82d5",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Day4 CNN with details</span>"
    ]
  },
  {
    "objectID": "day4_cnn.html#dimension-3d-4d",
    "href": "day4_cnn.html#dimension-3d-4d",
    "title": "5  Day4 CNN with details",
    "section": "5.2 Dimension (3D, 4D)",
    "text": "5.2 Dimension (3D, 4D)\n\nndarray (numpy)\ntensor (torch)\n\n\n\n\nalt text\n\n\n\n5.2.0.1 Image data\n\nndarray (numpy): Data is stored as (height, width, channels) (e.g., grayscale or RGB)\ntensor (PyTorch): Data is stored as (batch, channels, height, width)\n\n\nimport numpy as np\nimport torch\n\ndisplay(np.ones((2, 3, 4)))\ndisplay(torch.ones((2, 3, 4)))\n\narray([[[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]],\n\n       [[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]]])\n\n\ntensor([[[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]],\n\n        [[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]])\n\n\n\n\n\n5.2.0.2 Tensor\n\nimport torch\n\n# Scalar (0D tensor)\nscalar = torch.tensor(5)\nprint(\"Scalar:\", scalar)\n\n# Vector (1D tensor)\nvector = torch.tensor([1, 2, 3])\nprint(\"Vector:\", vector)\n\n# Matrix (2D tensor)\nmatrix = torch.tensor([[1, 2], [3, 4]])\nprint(\"Matrix:\\n\", matrix)\n\n# 3D Tensor (e.g., RGB image)\ntensor_3d = torch.rand(3, 4, 3)  # Random 3D tensor (height=3, width=4, channels=3)\nprint(\"3D Tensor (RGB image shape):\", tensor_3d.shape)\nprint(tensor_3d)\n\n# 4D Tensor (Batch of images)\ntensor_4d = torch.rand(2, 3, 4, 3)  # Batch size=2\nprint(\"4D Tensor (Batch of RGB images shape):\", tensor_4d.shape)\n\nScalar: tensor(5)\nVector: tensor([1, 2, 3])\nMatrix:\n tensor([[1, 2],\n        [3, 4]])\n3D Tensor (RGB image shape): torch.Size([3, 4, 3])\ntensor([[[0.9165, 0.6617, 0.1589],\n         [0.4045, 0.3932, 0.8812],\n         [0.8592, 0.8760, 0.5807],\n         [0.6333, 0.9934, 0.5808]],\n\n        [[0.7836, 0.8921, 0.0630],\n         [0.9065, 0.6955, 0.4772],\n         [0.8679, 0.4292, 0.9987],\n         [0.7440, 0.8195, 0.5946]],\n\n        [[0.7467, 0.4266, 0.1780],\n         [0.2938, 0.6778, 0.0691],\n         [0.3945, 0.7627, 0.3576],\n         [0.0728, 0.8352, 0.5470]]])\n4D Tensor (Batch of RGB images shape): torch.Size([2, 3, 4, 3])\n\n\n\n\n5.2.0.3 Example\n\nfrom PIL import Image  # For reading and processing images\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\n# Load the image\nimage_path = 'images/3cbe_model-1.jpeg'  # Replace with your image path\nimage = Image.open(image_path)\n\n# Show basic properties\nprint(\"Image Size:\", image.size)  # (width, height)\nprint(\"Image Mode:\", image.mode)  # e.g., \"RGB\"\n\n# Display the image\nplt.imshow(image)\nplt.title(\"3CBE\")\nplt.axis(\"off\")\nplt.show()\n\n\nImage Size: (500, 500)\nImage Mode: RGB\n\n\n\n\n\n\n\n\n\n\n\n5.2.0.4 Conversion\n\n# Convert image to NumPy array\nimage_np = np.array(image)\nprint(\"Image Shape (NumPy):\", image_np.shape)  # e.g., (height, width, channels)\n\nImage Shape (NumPy): (500, 500, 3)\n\n\n\n# Convert NumPy array to PyTorch tensor\nimage_tensor = torch.from_numpy(image_np).permute(2, 0, 1)  # Change to (channels, height, width)\nimage_tensor = image_tensor.float() / 255.0  # Normalize pixel values to [0, 1]\nprint(\"Image Shape (PyTorch Tensor):\", image_tensor.shape)  # e.g., (3, height, width)\n\n# Add batch dimension for model input\nimage_tensor = image_tensor.unsqueeze(0)  # Shape: (1, channels, height, width)\nprint(\"Image Shape with Batch Dimension:\", image_tensor.shape)\n\nImage Shape (PyTorch Tensor): torch.Size([3, 500, 500])\nImage Shape with Batch Dimension: torch.Size([1, 3, 500, 500])\n\n\n\n# Flip the image horizontally\nflipped_tensor = image_tensor.flip(2)  # Flip along the last dimension (width)\n\n# Convert back to NumPy for visualization\nflipped_image = flipped_tensor.squeeze(0).permute(1, 2, 0).numpy()\n\n# Display the flipped image\nplt.imshow(flipped_image)\nplt.title(\"Flipped Image\")\nplt.axis(\"off\")\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Day4 CNN with details</span>"
    ]
  },
  {
    "objectID": "day4_cnn.html#dot-products",
    "href": "day4_cnn.html#dot-products",
    "title": "5  Day4 CNN with details",
    "section": "5.3 Dot products",
    "text": "5.3 Dot products\n\nA fundamental operation in linear algebra that combines two vectors to produce a single scalar value.\nIt measures how aligned two vectors are and has applications in geometry, physics, and machine learning.\nFor two vectors $ = [a_1, a_2, , a_n] $ and $ = [b_1, b_2, , b_n] $ in \\(n\\)-dimensional space, the dot product is defined as:\n\n\\[\n\\mathbf{a} \\cdot \\mathbf{b} = a_1b_1 + a_2b_2 + \\dots + a_nb_n = \\sum_{i=1}^n a_i b_i\n\\]\n\nGeometric Interpretation:\n\nThe dot product measures the projection of one vector onto another.\nIt is related to the angle $ $ between the vectors: \\[\n\\mathbf{a} \\cdot \\mathbf{b} = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\cos\\theta\n\\]\n$ || $ and $ || $ are the magnitudes (lengths) of $ $ and $ $.\n\nOrthogonality:\n\nIf $ = 0 $, the vectors are perpendicular (orthogonal).\n\nSignificance of Value:\n\nPositive dot product: Vectors point in roughly the same direction.\nZero dot product: Vectors are orthogonal (90° apart).\nNegative dot product: Vectors point in opposite directions.\n\n\n\n\n5.3.0.1 Example\n\nSimple Numerical Example\n\nLet $ = [1, 2, 3] $ and $ = [4, 5, 6] \\(. The dot product is:\\)$ = (1)(4) + (2)(5) + (3)(6) = 4 + 10 + 18 = 32 $$\n\nGeometric Example If $ || = 5 $, $ || = 3 $, and the angle between them is $ = 60^\\(, the dot product is:\\)$ = || || = 5 (60^) = 15 = 7.5 $$\nApplications in Machine Learning\n\nSimilarity Measurement: Dot product measures similarity between vectors, such as in cosine similarity.\nConvolutions: Extract features by computing dot products between filters and input regions.\nAttention Mechanisms: Uses dot products to calculate importance weights between query and key vectors.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Day4 CNN with details</span>"
    ]
  },
  {
    "objectID": "day4_cnn.html#dataset-and-dataloader",
    "href": "day4_cnn.html#dataset-and-dataloader",
    "title": "5  Day4 CNN with details",
    "section": "5.4 Dataset and Dataloader",
    "text": "5.4 Dataset and Dataloader\n\n5.4.0.1 Dataset\n\nA class that represents your data, providing a way to access samples and their corresponding labels.\nWe can define a custom dataset by subclassing torch.utils.data.Dataset and overriding:\n\n__len__: Returns the total number of samples.\n__getitem__: Retrieves a single sample (data and label) by index.\n\n\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\nclass SimpleDataset(Dataset):\n    def __init__(self, size):\n        # Generate random x values\n        self.x = np.random.rand(size, 1) * 10  # Shape: (size, 1)\n        self.y = 2 * self.x + 3  # Generate labels (y = 2x + 3)\n\n    def __len__(self):\n        # Total number of samples\n        return len(self.x)\n\n    def __getitem__(self, idx):\n        # Retrieve the sample at index `idx`\n        sample = torch.tensor(self.x[idx], dtype=torch.float32)\n        label = torch.tensor(self.y[idx], dtype=torch.float32)\n        return sample, label\n\n# Create an instance of the dataset\ndataset = SimpleDataset(size=100)\n\n# Access the first sample\nsample, label = dataset[0]\nprint(\"Sample:\", sample, \"Label:\", label)\n\n# Check the length of the dataset\nprint(\"Number of samples in dataset:\", len(dataset))\n\n\nSample: tensor([0.6128]) Label: tensor([4.2255])\nNumber of samples in dataset: 100\n\n\n\n\n5.4.0.2 Dataloader\n\nIt provides “Efficient batching of data”, “Shuffling of data to avoid bias”, and “Parallel data loading using multiple workers.”\nBatch Processing:\n\nInstead of processing one sample at a time, DataLoader automatically groups samples into batches.\nThis improves computational efficiency, especially with GPUs.\n\nShuffling:\n\nShuffles the data during training to reduce bias.\n\nParallel Loading:\n\nLoads data in parallel using multiple workers (num_workers parameter).\n\n\n\n# Create a DataLoader to handle batching\ndataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n\n# Iterate through the DataLoader\nfor batch_idx, (batch_samples, batch_labels) in enumerate(dataloader):\n    print(f\"Batch {batch_idx + 1}\")\n    print(\"Samples:\\n\", batch_samples)\n    print(\"Labels:\\n\", batch_labels)\n    break  # Show only the first batch\n\nBatch 1\nSamples:\n tensor([[9.4416],\n        [7.3267],\n        [0.8215],\n        [3.6725],\n        [3.7980],\n        [3.2378],\n        [9.5629],\n        [9.5563],\n        [6.4680],\n        [4.2153]])\nLabels:\n tensor([[21.8831],\n        [17.6535],\n        [ 4.6430],\n        [10.3449],\n        [10.5959],\n        [ 9.4756],\n        [22.1257],\n        [22.1126],\n        [15.9360],\n        [11.4305]])\n\n\n\n\n5.4.0.3 Sequence example\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n# Data generation\nseq_length = 20\nnum_sample = 1000\n\n# Motif CCGGAA PWM\nmotif_pwm = np.array([[10.41, 22.86, 1.92, 1.55, 98.60, 86.66],\n                      [68.20, 65.25, 0.50, 0.35, 0.25, 2.57],\n                      [17.27, 8.30, 94.77, 97.32, 0.87, 0.00],\n                      [4.13, 3.59, 2.81, 0.78, 0.28, 10.77]])\npwm = np.hstack([np.ones((4, 7)), motif_pwm, np.ones((4, 7))])\n\n# Generate positive samples\npos = np.array([np.random.choice(['A', 'C', 'G', 'T'], num_sample,\n                                  p=pwm[:, i] / sum(pwm[:, i])) for i in range(seq_length)]).transpose()\n\n# Generate negative samples\nneg = np.array([np.random.choice(['A', 'C', 'G', 'T'], num_sample,\n                                  p=np.array([1, 1, 1, 1]) / 4) for i in range(seq_length)]).transpose()\n\n# Combine data and create labels\ndata = np.vstack([pos, neg])\nlabels = np.array([1] * num_sample + [0] * num_sample)  # Positive: 1, Negative: 0\n\nprint(data.shape, labels.shape)\n\n(2000, 20) (2000,)\n\n\n\nclass SequenceDataset(Dataset):\n    def __init__(self, sequences, labels):\n        \"\"\"\n        Initialize the dataset.\n        Args:\n        - sequences: A NumPy array of shape (num_samples, seq_length) containing DNA sequences.\n        - labels: A NumPy array of shape (num_samples,) containing labels (0 or 1).\n        \"\"\"\n        self.sequences = sequences\n        self.labels = labels\n        self.nucleotide_to_idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        # Convert sequence to one-hot encoding\n        sequence = self.sequences[idx]\n        one_hot = np.zeros((4, len(sequence)), dtype=np.float32)\n        for i, nucleotide in enumerate(sequence):\n            one_hot[self.nucleotide_to_idx[nucleotide], i] = 1.0\n\n        # Convert to PyTorch tensor\n        one_hot_tensor = torch.tensor(one_hot, dtype=torch.float32)\n        label_tensor = torch.tensor(self.labels[idx], dtype=torch.float32)\n\n        return one_hot_tensor, label_tensor\n\n# Create the dataset\ndataset = SequenceDataset(data, labels)\n\n# Create the DataLoader\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Check a batch of data\nfor batch_idx, (sequences, labels) in enumerate(dataloader):\n    print(f\"Batch {batch_idx + 1}\")\n    print(\"Sequences Shape:\", sequences.shape)  # Shape: (batch_size, 4, seq_length)\n    print(\"Labels Shape:\", labels.shape)        # Shape: (batch_size,)\n    print(\"First Sequence (One-Hot):\\n\", sequences[0])\n    print(\"First Label:\", labels[0])\n    break  # Show only the first batch\n\nBatch 1\nSequences Shape: torch.Size([32, 4, 20])\nLabels Shape: torch.Size([32])\nFirst Sequence (One-Hot):\n tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n         0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n         1., 0.],\n        [0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n         0., 0.],\n        [1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 1.]])\nFirst Label: tensor(1.)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Day4 CNN with details</span>"
    ]
  },
  {
    "objectID": "day4_cnn.html#convolutional-layers",
    "href": "day4_cnn.html#convolutional-layers",
    "title": "5  Day4 CNN with details",
    "section": "5.5 Convolutional Layers",
    "text": "5.5 Convolutional Layers\n\nExtract local patterns (features) from the input data (e.g., motifs in DNA sequences, image patterns).\nA small sliding window (filter or kernel) moves across the data.\nThe filter computes a dot product between its weights and the input it covers, producing a feature map.\n\n\n\n5.5.0.1 Kernel (Filter)\n\nA kernel (also called a filter) is a small, learnable matrix used in the convolution operation.\nKernels slide over the input data to detect patterns, such as edges in images or motifs in DNA sequences.\nThe values inside the kernel are the parameters that the model learns during training.\nFor a 2D convolution: \\[\n\\text{Kernel} =\n\\begin{bmatrix}\n1 & 0 & -1 \\\\\n1 & 0 & -1 \\\\\n1 & 0 & -1\n\\end{bmatrix}\n\\]\nFor a 1D convolution: \\[\n\\text{Kernel} =\n\\begin{bmatrix}\n1 & 0 & -1\n\\end{bmatrix}\n\\]\nIn the case of DNA, a kernel for 1D convolution that detects “ATG”: \\[\n\\text{Kernel} =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 1 & 0\n\\end{bmatrix}\n\\]\nConsider the sequence: “ATGCGTTG”.\nOne-hot encoding of the sequence 4 by 8 matrix: \\[\n\\text{Input} =\n\\begin{bmatrix}\n1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\  % A\n0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 \\\\  % C\n0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 \\\\  % G\n0 & 0 & 1 & 0 & 0 & 1 & 1 & 0    % T\n\\end{bmatrix}\n\\]\n\n\n\n5.5.0.2 Stride and padding\n\nStride = 1: The kernel moves one position at a time. This results in a highly overlapping convolution operation\nStride &gt; 1: The kernel skips positions while sliding, reducing the spatial dimensions of the feature map. This makes the computation faster but may lose some spatial detail.\n\nFor a 1D convolution, the output size is calculated as: \\[\n\\text{Output Length} = \\left\\lfloor \\frac{\\text{Input Length} - \\text{Kernel Size} + 2 \\times \\text{Padding}}{\\text{Stride}} \\right\\rfloor + 1\n\\]\nFor a 2D convolution, the output size for height and width is: \\[\n\\text{Output Height} = \\left\\lfloor \\frac{\\text{Input Height} - \\text{Kernel Height} + 2 \\times \\text{Padding}}{\\text{Stride}} \\right\\rfloor + 1\n\\]\n\\[\n\\text{Output Width} = \\left\\lfloor \\frac{\\text{Input Width} - \\text{Kernel Width} + 2 \\times \\text{Padding}}{\\text{Stride}} \\right\\rfloor + 1\n\\]\n\nimport torch\nimport torch.nn as nn\n\n# Define a 1D convolutional layer\nconv1d = nn.Conv1d(in_channels=4, out_channels=8, kernel_size=3, stride=1, padding=1)\n\n# Input tensor (batch_size=1, channels=1, seq_length=10)\ninput_tensor = torch.randn(1, 4, 10)\n\n# Apply convolution\noutput = conv1d(input_tensor)\nprint(\"Input Shape:\", input_tensor.shape)  # (1, 4, 10)\nprint(\"Output Shape:\", output.shape)       # (1, 8, 10) \n\nInput Shape: torch.Size([1, 4, 10])\nOutput Shape: torch.Size([1, 8, 10])\n\n\n\nimport torch\nimport torch.nn as nn\n\n# Define a 1D convolutional layer\nconv1d = nn.Conv1d(in_channels=4, out_channels=8, kernel_size=3, stride=3, padding=1)\n\n# Input tensor (batch_size=1, channels=1, seq_length=10)\ninput_tensor = torch.randn(1, 4, 10)\n\n# Apply convolution\noutput = conv1d(input_tensor)\nprint(\"Input Shape:\", input_tensor.shape)  # (1, 1, 10)\nprint(\"Output Shape:\", output.shape)       # (1, 1, 4) \n\nInput Shape: torch.Size([1, 4, 10])\nOutput Shape: torch.Size([1, 8, 4])\n\n\n\nWhat if padding = 0?\nWhat is the meaning of out_channels?\n\n\n\n5.5.0.3 numpy code for the motif example\n\nimport numpy as np\n\nnp.random.seed(0)\ndef conv1d_numpy(input_data, kernel):\n    \"\"\"\n    Perform 1D convolution for one-hot encoded DNA sequence.\n    Args:\n    - input_data: NumPy array of shape (4, seq_length) representing one-hot encoded DNA.\n    - kernel: NumPy array of shape (4, kernel_size) representing the convolution filter.\n    Returns:\n    - feature_map: NumPy array of the convolved output.\n    \"\"\"\n    num_channels, seq_length = input_data.shape\n    _, kernel_size = kernel.shape\n    output_length = seq_length - kernel_size + 1\n\n    # Initialize the feature map\n    feature_map = np.zeros(output_length)\n\n    # Perform convolution (dot product for each sliding window)\n    for i in range(output_length):\n        window = input_data[:, i:i+kernel_size]  # Extract sliding window\n        # out = np.multiply(window, kernel)\n        # feature_map[i] = np.sum(out)\n        feature_map[i] = np.sum(window * kernel) # dot product\n\n    return feature_map\n\n\nsequence, label = dataset[0]\nsequence_np = sequence.numpy()\n\nconv_kernel = np.random.rand(4, 5)\nconv_kernel\n\nfeature_map_np = conv1d_numpy(sequence_np, conv_kernel)\nprint(\"Feature Map (NumPy):\", feature_map_np)\nprint(\"Feature Map Shape (NumPy):\", feature_map_np.shape)\n\nFeature Map (NumPy): [2.20269505 1.83470684 3.41871594 3.17915171 3.3165857  3.15578407\n 2.97188702 3.00077732 2.71137158 3.33827867 2.95887059 3.40566087\n 2.48418074 2.8549015  3.95476016 3.13685259]\nFeature Map Shape (NumPy): (16,)\n\n\n\n\npytorch code\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DNA_CNN(nn.Module):\n    def __init__(self):\n        super(DNA_CNN, self).__init__()\n        # Convolution Layer: 4 input channels (A, C, G, T), \n        self.conv1 = nn.Conv1d(in_channels=4, out_channels=1, kernel_size=5, stride=1, padding=0)\n\n    def forward(self, x):\n        x = self.conv1(x)  # Apply convolution\n        return x\n\n# Instantiate the model\nmodel = DNA_CNN()\nprint(model)\n\nsequence, label = dataset[0]\nout = model(sequence)\nprint(\"Output Shape:\", out.shape)\nprint(out)\nnoact_out = out\n\nDNA_CNN(\n  (conv1): Conv1d(4, 1, kernel_size=(5,), stride=(1,))\n)\nOutput Shape: torch.Size([1, 16])\ntensor([[ 0.1340, -0.1981,  0.1160,  0.1986,  0.4438,  0.5583,  0.5739,  0.3242,\n         -0.0027, -0.2718,  0.2756,  0.4053,  0.2138,  0.2551,  0.4002,  0.4910]],\n       grad_fn=&lt;SqueezeBackward1&gt;)\n\n\n\n\n5.5.0.4 Activation Functions\n\nIntroduce non-linearity into the model.\nCommon Function: ReLU (Rectified Linear Unit) is often used because it accelerates training and reduces the chance of vanishing gradients.\nTypes of activation functions\n\nReLU : Default choice for hidden layers in CNNs.\n\nSigmoid : Final output layer for binary classification.\n\nTanh : Hidden layers when symmetric output is beneficial (e.g., RNNs).\n\nLeaky ReLU : When addressing “dead neurons” in ReLU.\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DNA_CNN(nn.Module):\n    def __init__(self):\n        super(DNA_CNN, self).__init__()\n        # Convolution Layer: 4 input channels (A, C, G, T), \n        self.conv1 = nn.Conv1d(in_channels=4, out_channels=1, kernel_size=5, stride=1, padding=0)\n\n    def forward(self, x):\n        x = self.conv1(x)  # Apply convolution\n        x = F.relu(x)      # Apply ReLU activation\n        return x\n\nnp.random.seed(10)\n# Instantiate the model\nmodel = DNA_CNN()\nprint(model)\n\nsequence, label = dataset[0]\nout = model(sequence)\nprint(\"Output Shape:\", out.shape)\nprint(out)\n\n## plot and compare values in noact_out vs out with bar plot side by side\nimport matplotlib.pyplot as plt\n\n# Convert to NumPy arrays\nnoact_out_np = noact_out.squeeze(0).detach().numpy()\nout_np = out.squeeze(0).detach().numpy()\n\n# Plot the feature maps\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.bar(range(len(noact_out_np)), noact_out_np, color='b')\nplt.title(\"Before Activation (ReLU)\")\nplt.xlabel(\"Position\")\nplt.ylabel(\"Activation Value\")\nplt.subplot(1, 2, 2)\nplt.bar(range(len(out_np)), out_np, color='r')\nplt.title(\"After Activation (ReLU)\")\nplt.xlabel(\"Position\")\nplt.ylabel(\"Activation Value\")\nplt.show()\n\nDNA_CNN(\n  (conv1): Conv1d(4, 1, kernel_size=(5,), stride=(1,))\n)\nOutput Shape: torch.Size([1, 16])\ntensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.2317, 0.2173, 0.3338, 0.0836, 0.0000,\n         0.0000, 0.0000, 0.2567, 0.1971, 0.0000, 0.1429, 0.0000]],\n       grad_fn=&lt;ReluBackward0&gt;)\n\n\n\n\n\n\n\n\n\n\n\n5.5.0.5 Pooling layers\n\nTo reduce the spatial dimensions of feature maps. It helps reducing the computational complexity of the network, aggregating features, making the model more robust to small translations or distortions in the input\nTypes:\n\nMax Pooling: Keeps the maximum value in a window.\nAverage Pooling: Averages the values in a window.\n\n\n\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DNA_CNN(nn.Module):\n    def __init__(self):\n        super(DNA_CNN, self).__init__()\n        # Convolution Layer: 4 input channels, 1 filter, kernel size=5\n        self.conv1 = nn.Conv1d(in_channels=4, out_channels=1, kernel_size=5, stride=1, padding=0)\n        # Pooling Layer: Max Pooling with kernel size=2, stride=2\n        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n\n    def forward(self, x):\n        x = self.conv1(x)  # Convolution\n        x = F.relu(x)      # ReLU activation\n        x = self.pool(x)   # Max Pooling\n        return x\n    \n\n# Example DNA sequence\nsequence, label = dataset[0]  # First sample\nsequence = sequence.unsqueeze(0)  # Add batch dimension (1, 4, 20)\n\n# Instantiate the model and pass data through it\nmodel = DNA_CNN()\noutput = model(sequence)\n\nprint(\"Input Shape:\", sequence.shape)  # (1, 4, 20)\nprint(\"Output Shape After Convolution:\", model.conv1(sequence).shape)  # (1, 1, 16)\nprint(\"Output Shape After Pooling:\", output.shape)  # (1, 1, 8)\n\n\nInput Shape: torch.Size([1, 4, 20])\nOutput Shape After Convolution: torch.Size([1, 1, 16])\nOutput Shape After Pooling: torch.Size([1, 1, 8])\n\n\n\n\n5.5.0.6 Flattening\n\nConverts the multidimensional output of a convolutional or pooling layer into a 1D vector.\nThis is necessary because the subsequent layers (like fully connected or dense layers) expect inputs to be in a flattened format.\n\n\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DNA_CNN(nn.Module):\n    def __init__(self):\n        super(DNA_CNN, self).__init__()\n        self.conv1 = nn.Conv1d(in_channels=4, out_channels=2, kernel_size=5, stride=1, padding=0)\n        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.fc1 = nn.Linear(1 * 16, 1)  # Fully connected layer (adjust input size)\n\n    def forward(self, x):\n        x = self.conv1(x)  # Convolution\n        x = F.relu(x)      # ReLU activation\n        x = self.pool(x)   # Max Pooling\n        x = torch.flatten(x, start_dim=1)  # Flatten for fully connected layer\n        x = self.fc1(x)    # Fully connected layer\n        return x\n\n# Example DNA sequence\nsequence, label = dataset[0]  # First sample\nsequence = sequence.unsqueeze(0)  # Add batch dimension (1, 4, 20)\n\n# Instantiate the model and pass data through it\nmodel = DNA_CNN()\noutput = model(sequence)\n\nprint(\"Input Shape:\", sequence.shape)  # (1, 4, 20)\nprint(\"Shape After Convolution:\", model.conv1(sequence).shape)  # (1, 1, 16)\nprint(\"Shape After Pooling:\", model.pool(model.conv1(sequence)).shape)  # (1, 1, 8)\nprint(\"Shape After Flattening:\", torch.flatten(model.pool(model.conv1(sequence)), start_dim=1).shape)  # (1, 8)\nprint(\"Output Shape (Final):\", output.shape)  # (1, 1)\n\nInput Shape: torch.Size([1, 4, 20])\nShape After Convolution: torch.Size([1, 2, 16])\nShape After Pooling: torch.Size([1, 2, 8])\nShape After Flattening: torch.Size([1, 16])\nOutput Shape (Final): torch.Size([1, 1])\n\n\n\n\n5.5.0.7 Fully Connected Layers\n\nPerform classification or regression based on the extracted features.\nCombines all the features detected by earlier layers to predict an output.\n\n\n\n5.5.0.8 Output Layer\n\nGenerate the final prediction.\nActivation Functions:\n\nSigmoid: For binary classification.\nSoftmax: For multi-class classification.\n\n\n\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass DNA_CNN(nn.Module):\n    def __init__(self):\n        super(DNA_CNN, self).__init__()\n        self.conv1 = nn.Conv1d(in_channels=4, out_channels=16, kernel_size=3, stride=1, padding=1) # output length: 20 - 3 + 2*1 + 1 = 20\n        self.relu = nn.ReLU() # \n        self.maxpool = nn.MaxPool1d(kernel_size=2)\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(in_features=160, out_features=64)  \n        self.fc2 = nn.Linear(in_features=64, out_features=2)  \n        #self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.flatten(x)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        #x = self.softmax(x)\n        return x\n\nmodel = DNA_CNN()\nif torch.cuda.is_available():\n    model.cuda()\n\nfrom torchsummary import summary\nsummary(model, input_size=(4, 20))  # (Channels, Length)\n\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv1d-1               [-1, 16, 20]             208\n              ReLU-2               [-1, 16, 20]               0\n         MaxPool1d-3               [-1, 16, 10]               0\n           Flatten-4                  [-1, 160]               0\n            Linear-5                   [-1, 64]          10,304\n            Linear-6                    [-1, 2]             130\n================================================================\nTotal params: 10,642\nTrainable params: 10,642\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.01\nParams size (MB): 0.04\nEstimated Total Size (MB): 0.05\n----------------------------------------------------------------\n\n\n\n\n5.5.1 Complete code\n\n5.5.1.1 Data\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n# Data generation\nseq_length = 20\nnum_sample = 1000\n\n# Motif CCGGAA PWM\nmotif_pwm = np.array([[10.41, 22.86, 1.92, 1.55, 98.60, 86.66],\n                      [68.20, 65.25, 0.50, 0.35, 0.25, 2.57],\n                      [17.27, 8.30, 94.77, 97.32, 0.87, 0.00],\n                      [4.13, 3.59, 2.81, 0.78, 0.28, 10.77]])\npwm = np.hstack([np.ones((4, 7)), motif_pwm, np.ones((4, 7))])\n\n# Generate positive samples\npos = np.array([np.random.choice(['A', 'C', 'G', 'T'], num_sample,\n                                  p=pwm[:, i] / sum(pwm[:, i])) for i in range(seq_length)]).transpose()\n\n# Generate negative samples\nneg = np.array([np.random.choice(['A', 'C', 'G', 'T'], num_sample,\n                                  p=np.array([1, 1, 1, 1]) / 4) for i in range(seq_length)]).transpose()\n\n# Combine data and create labels\ndata = np.vstack([pos, neg])\nlabels = np.array([1] * num_sample + [0] * num_sample)  # Positive: 1, Negative: 0\n\n\nfrom sklearn.model_selection import train_test_split\n\n# Split data into training and test sets (80% training, 20% test)\ntrain_data, test_data, train_labels, test_labels = train_test_split(\n    data, labels, test_size=0.2, random_state=42, stratify=labels\n)\n\nprint(\"Training Data Shape:\", train_data.shape)\nprint(\"Test Data Shape:\", test_data.shape)\nprint(\"Training Labels Shape:\", train_labels.shape)\nprint(\"Test Labels Shape:\", test_labels.shape)\n\n\nclass SequenceDataset(Dataset):\n    def __init__(self, sequences, labels):\n        \"\"\"\n        Initialize the dataset.\n        Args:\n        - sequences: A NumPy array of shape (num_samples, seq_length) containing DNA sequences.\n        - labels: A NumPy array of shape (num_samples,) containing labels (0 or 1).\n        \"\"\"\n        self.sequences = sequences\n        self.labels = labels\n        self.nucleotide_to_idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        # Convert sequence to one-hot encoding\n        sequence = self.sequences[idx]\n        one_hot = np.zeros((4, len(sequence)), dtype=np.float32)\n        for i, nucleotide in enumerate(sequence):\n            one_hot[self.nucleotide_to_idx[nucleotide], i] = 1.0\n\n        # Convert to PyTorch tensor\n        one_hot_tensor = torch.tensor(one_hot, dtype=torch.float32)\n        label_tensor = torch.tensor(self.labels[idx], dtype=torch.float32)\n\n        return one_hot_tensor, label_tensor\n\n# Create the dataset\ntrain_dataset = SequenceDataset(train_data, train_labels)\n\n# Create the DataLoader\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\nTraining Data Shape: (1600, 20)\nTest Data Shape: (400, 20)\nTraining Labels Shape: (1600,)\nTest Labels Shape: (400,)\n\n\n\n\n5.5.1.2 Model and training\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass DNA_CNN(nn.Module):\n    def __init__(self):\n        super(DNA_CNN, self).__init__()\n        self.conv1 = nn.Conv1d(in_channels=4, out_channels=16, kernel_size=3, stride=1, padding=1) # output length: 20 - 3 + 2*1 + 1 = 20\n        self.relu = nn.ReLU() # \n        self.maxpool = nn.MaxPool1d(kernel_size=2)\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(in_features=160, out_features=64)  \n        self.fc2 = nn.Linear(in_features=64, out_features=2)  \n        #self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.flatten(x)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        #x = self.softmax(x)\n        return x\n\n\n\nmodel = DNA_CNN()\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nepochs = 10  # Number of epochs\n\nfor epoch in range(epochs):\n    model.train()  # Set model to training mode\n    total_loss = 0\n\n    for batch_idx, (sequences, labels) in enumerate(train_dataloader):\n        # Prepare data\n        sequences = sequences  # (batch_size, 4, seq_length)\n        labels = labels.long()  # Convert labels to long for CrossEntropyLoss\n\n        # Forward pass\n        outputs = model(sequences)\n\n        # Compute loss\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Accumulate loss\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_dataloader):.4f}\")\n\n\n\nEpoch 1/10, Loss: 0.4741\nEpoch 2/10, Loss: 0.1364\nEpoch 3/10, Loss: 0.0889\nEpoch 4/10, Loss: 0.0768\nEpoch 5/10, Loss: 0.0738\nEpoch 6/10, Loss: 0.0616\nEpoch 7/10, Loss: 0.0578\nEpoch 8/10, Loss: 0.0547\nEpoch 9/10, Loss: 0.0528\nEpoch 10/10, Loss: 0.0488\n\n\n\n\n5.5.1.3 Testing\n\n# Create test dataset and dataloader\ntest_dataset = SequenceDataset(test_data, test_labels)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# Evaluate the model\nmodel.eval()  # Set model to evaluation mode\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():  # Disable gradient calculation for testing\n    for sequences, labels in test_dataloader:\n        sequences = sequences  # (batch_size, 4, seq_length)\n        labels = labels.long()  # Convert labels to long for CrossEntropyLoss\n\n        # Forward pass\n        outputs = model(sequences)\n\n        # Get predictions\n        _, predictions = torch.max(outputs, 1)\n\n        # Count correct predictions\n        correct += (predictions == labels).sum().item()\n        total += labels.size(0)\n\nprint(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n\nTest Accuracy: 96.75%\n\n\n\n\n\n5.5.2 Device for computation\n\nimport torch\n\n# Check if CUDA is available\nif torch.cuda.is_available():\n    print(\"CUDA is available. Training will be performed on GPU.\")\nelse:\n    print(\"CUDA is not available. Training will be performed on CPU.\")\n\nprint(next(model.parameters()).device)\nprint(sequences.device)\nprint(labels.device)\n\nprint(f\"Allocated GPU memory: {torch.cuda.memory_allocated() / 1024 ** 2:.2f} MB\")\nprint(f\"Cached GPU memory: {torch.cuda.memory_reserved() / 1024 ** 2:.2f} MB\")\n\nCUDA is available. Training will be performed on GPU.\ncpu\ncpu\ncpu\nAllocated GPU memory: 8.44 MB\nCached GPU memory: 22.00 MB\n\n\n\n!nvidia-smi\n\nMon Dec  2 12:58:41 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.183.04             Driver Version: 538.78       CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA RTX A5500 Laptop GPU    On  | 00000000:01:00.0 Off |                  Off |\n| N/A   55C    P8              12W /  82W |   1419MiB / 16384MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|    0   N/A  N/A      2669      C   /python3.11                               N/A      |\n+---------------------------------------------------------------------------------------+\n\n\n\n\n5.5.3 Run on GPU\n\n\n\n# Check for GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Move the model to GPU\nmodel = DNA_CNN().to(device)\nimport torch.optim as optim\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()  # CrossEntropyLoss for classification\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nepochs = 5  # Number of epochs\nfor epoch in range(epochs):\n    model.train()  # Set the model to training mode\n    total_loss = 0\n\n    for batch_idx, (sequences, labels) in enumerate(train_dataloader):\n        # Move data to GPU\n        sequences = sequences.to(device)  # Move input to GPU\n        labels = labels.to(device).long()  # Move labels to GPU and ensure correct type\n\n        # Forward pass\n        outputs = model(sequences)\n\n        # Compute loss\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Accumulate loss\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_dataloader):.4f}\")\n\nUsing device: cuda\nEpoch 1/5, Loss: 0.4898\nEpoch 2/5, Loss: 0.1334\nEpoch 3/5, Loss: 0.0785\nEpoch 4/5, Loss: 0.0703\nEpoch 5/5, Loss: 0.0630\n\n\n\n# Test the model\nmodel.eval()  # Set the model to evaluation mode\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():  # Disable gradient calculation\n    for sequences, labels in test_dataloader:\n        # Move data to GPU\n        sequences = sequences.to(device)\n        labels = labels.to(device).long()\n\n        # Forward pass\n        outputs = model(sequences)\n\n        # Get predictions\n        _, predictions = torch.max(outputs, 1)\n\n        # Count correct predictions\n        correct += (predictions == labels).sum().item()\n        total += labels.size(0)\n\nprint(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n\nTest Accuracy: 96.50%\n\n\n\nimport torch\n\n# Check if CUDA is available\nif torch.cuda.is_available():\n    print(\"CUDA is available. Training will be performed on GPU.\")\nelse:\n    print(\"CUDA is not available. Training will be performed on CPU.\")\n\nprint(next(model.parameters()).device)\nprint(sequences.device)\nprint(labels.device)\n\nprint(f\"Allocated GPU memory: {torch.cuda.memory_allocated() / 1024 ** 2:.2f} MB\")\nprint(f\"Cached GPU memory: {torch.cuda.memory_reserved() / 1024 ** 2:.2f} MB\")\n\nCUDA is available. Training will be performed on GPU.\ncuda:0\ncuda:0\ncuda:0\nAllocated GPU memory: 16.74 MB\nCached GPU memory: 22.00 MB",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Day4 CNN with details</span>"
    ]
  },
  {
    "objectID": "day5_model_evaluation.html",
    "href": "day5_model_evaluation.html",
    "title": "6  Day5 Model Evaluation Metrics",
    "section": "",
    "text": "6.1 Basic Concepts",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Day5 Model Evaluation Metrics</span>"
    ]
  },
  {
    "objectID": "day5_model_evaluation.html#basic-concepts",
    "href": "day5_model_evaluation.html#basic-concepts",
    "title": "6  Day5 Model Evaluation Metrics",
    "section": "",
    "text": "When evaluating a classification model, predictions are compared with the actual (ground truth) labels.\n\n\n6.1.0.1 True Positive (TP)\n\nThe model correctly predicts the positive class.\n\nActual: 1 (positive).\nPredicted: 1 (positive).\n\n\n\n\n6.1.0.2 False Positive (FP) (Type I Error)\n\nThe model predicts the positive class, but the actual class is negative.\n\nActual: 0 (negative).\nPredicted: 1 (positive).\nIn disease detection, marking a normal person as patient.\n\n\n\n\n6.1.0.3 True Negative (TN)\n\nThe model correctly predicts the negative class.\n\nActual: 0 (negative).\nPredicted: 0 (negative).\n\nIndicates how well the model identifies the negative class.\n\n\n\n6.1.0.4 False Negative (FN) (Type II Error)\n\nThe model predicts the negative class, but the actual class is positive.\n\nActual: 1 (positive).\nPredicted: 0 (negative).\nIn disease detection, failing to identify a patient with the disease.\n\n\n\n\n6.1.0.5 Confusion matrix from wiki",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Day5 Model Evaluation Metrics</span>"
    ]
  },
  {
    "objectID": "day5_model_evaluation.html#accuracy",
    "href": "day5_model_evaluation.html#accuracy",
    "title": "6  Day5 Model Evaluation Metrics",
    "section": "6.2 Accuracy",
    "text": "6.2 Accuracy\n\nAccuracy is one of the most straightforward and commonly used metrics to evaluate the performance of a classification model.\nIt measures the proportion of correctly classified samples out of the total number of samples.\n\n\\[\n\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n\\]\nIn terms of the confusion matrix, accuracy can also be expressed as:\n\\[\n\\text{Accuracy} = \\frac{\\text{True Positives (TP)} + \\text{True Negatives (TN)}}{\\text{Total Number of Samples (TP + TN + FP + FN)}}\n\\]\n\nGood\n\nWhen the dataset is balanced, meaning the number of positive and negative samples is roughly equal.\nWhen all misclassification errors (false positives and false negatives) are equally costly.\nEasy to understand and implement.\nWorks well for balanced datasets.\n\nNo good\n\nwhen the dataset is imbalanced:\nFor example, if 95% of the samples belong to 1, a model that always predicts the majority class will achieve 95% accuracy, but this is misleading.\nAccuracy treats all errors equally, which is not suitable if false positives and false negatives have different costs.\n\n\n\nfrom sklearn.metrics import accuracy_score\n\n# Ground truth labels and model predictions\ny_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\ny_pred = [1, 0, 1, 0, 0, 1, 0, 0, 1, 1]\n\ntotal = len(y_pred)\ncorrect = sum(1 for yt, yp in zip(y_true, y_pred) if yt == yp)\naccuracy = correct / total\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")\n\naccuracy = accuracy_score(y_true, y_pred)\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")\n\nAccuracy: 80.00%\nAccuracy: 80.00%",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Day5 Model Evaluation Metrics</span>"
    ]
  },
  {
    "objectID": "day5_model_evaluation.html#precision",
    "href": "day5_model_evaluation.html#precision",
    "title": "6  Day5 Model Evaluation Metrics",
    "section": "6.3 Precision",
    "text": "6.3 Precision\n\nA metric used to evaluate the performance of a classification model, particularly for the positive class.\nIt measures the proportion of true positive predictions out of all the samples that were predicted as positive.\n\n\\[\n\\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Positives (FP)}}\n\\]\n\nPrecision is critical in scenarios where incorrectly predicting positives (FP) has significant consequences.\nHigh precision ensures the model is not making too many false positive predictions, thus reducing “false alarms.”\nGood\n\nEnsures positive predictions are reliable.\nHelps reduce “false alarms” in critical systems.\nPrecision is especially useful for imbalanced datasets where the positive class is rare.\n\nNo Good\n\nPrecision does not account for cases where the model misses actual positives (FN).\nIn situations where identifying all positives is crucial (e.g., disease detection), precision alone is insufficient.\n\n\n\n\nfrom sklearn.metrics import precision_score\n\n# True labels\ny_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n\n# Predicted labels\ny_pred = [1, 0, 1, 0, 0, 1, 0, 0, 1, 1]\n\nprecision = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 1) / sum(1 for yp in y_pred if yp == 1)\nprint(f\"Precision: {precision * 100:.2f}%\")\n\n# Compute precision\nprecision = precision_score(y_true, y_pred)\nprint(f\"Precision: {precision * 100:.2f}%\")\n\nPrecision: 80.00%\nPrecision: 80.00%",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Day5 Model Evaluation Metrics</span>"
    ]
  },
  {
    "objectID": "day5_model_evaluation.html#recall",
    "href": "day5_model_evaluation.html#recall",
    "title": "6  Day5 Model Evaluation Metrics",
    "section": "6.4 Recall",
    "text": "6.4 Recall\n\nA metric that measures the ability of a classification model to correctly identify all positive samples in the dataset.\nKnown as sensitivity or true positive rate (TPR)\n\n\\[\n\\text{Recall} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Negatives (FN)}}\n\\]\n\nMeaning: Of all the actual positive samples, how many did the model correctly identify?\nIt focuses on the model’s ability to avoid missing true positives (false negatives).\nRecall is critical in scenarios where missing positive samples (FN) can have severe consequences.\n\nMedical Diagnosis: Failing to identify a disease could lead to fatal consequences.\n\nRecall and precision often have a trade-off:\n\nHigh recall increases false positives, lowering precision.\nHigh precision may miss true positives, lowering recall.\n\n\n\n\n6.4.1 5. Example Calculation\n\n6.4.1.1 Confusion Matrix:\n[\n\\[\\begin{bmatrix}\n\\text{True Negatives (TN)} & \\text{False Positives (FP)} \\\\\n\\text{False Negatives (FN)} & \\text{True Positives (TP)}\n\\end{bmatrix}\\]\n=\n\\[\\begin{bmatrix}\n50 & 10 \\\\\n5 & 35\n\\end{bmatrix}\\]\n]\n\n\n6.4.1.2 Calculate Recall:\n[ = = = = 0.875 , ( 87.5%) ]\n\n\n\n\n6.4.2 6. Advantages of Recall\n\nCaptures All Positives:\n\nRecall ensures that the model does not miss positive samples.\n\nImportant in Costly Misses:\n\nIn applications like medical diagnosis, recall is a priority since false negatives are costly.\n\n\n\n\n\n6.4.3 7. Disadvantages of Recall\n\nIgnores False Positives:\n\nRecall does not account for how many false positives the model predicts.\nThis can lead to overly lenient models that classify many samples as positive to maximize recall.\n\nPoor Choice for Applications Where Precision Matters:\n\nIn applications like spam detection, focusing solely on recall might result in excessive false positives (e.g., legitimate emails marked as spam).\n\n\n\n\n\n6.4.4 8. Use Cases for Recall\n\n6.4.4.1 Medical Diagnosis:\n\nWhy:\n\nFalse negatives mean undiagnosed patients, which can have serious consequences.\n\nGoal:\n\nHigh recall to ensure all patients with the disease are correctly identified.\n\n\n\n\n6.4.4.2 Fraud Detection:\n\nWhy:\n\nMissing fraudulent transactions could lead to significant financial losses.\n\nGoal:\n\nHigh recall to flag as many fraudulent transactions as possible.\n\n\n\n\n6.4.4.3 Search Engines:\n\nWhy:\n\nEnsure all relevant results are retrieved for a query.\n\nGoal:\n\nHigh recall to return all relevant documents, even at the cost of some irrelevant ones.\n\n\n\n\n\n\n6.4.5 9. Recall in Imbalanced Datasets\nRecall is especially useful for imbalanced datasets where the positive class is rare. For example: - In disease detection, the number of positive cases (sick patients) is often much smaller than negative cases (healthy patients). Recall ensures that the model captures as many positive cases as possible.\n\n\n\n6.4.6 10. Python Code Example\n\n6.4.6.1 Using sklearn\nfrom sklearn.metrics import recall_score\n\n# True labels\ny_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n\n# Predicted labels\ny_pred = [1, 0, 1, 0, 0, 1, 0, 0, 1, 1]\n\n# Compute recall\nrecall = recall_score(y_true, y_pred)\nprint(f\"Recall: {recall:.2f}\")\n\n\n6.4.6.2 Manual Calculation\nimport numpy as np\n\n# Confusion matrix components\nTP = 35\nFN = 5\n\n# Calculate recall\nrecall = TP / (TP + FN)\nprint(f\"Recall: {recall:.2f}\")\n\n\n\n\n6.4.7 11. Recall vs Precision\n\n\n\n\n\n\n\n\nMetric\nWhat It Measures\nUse Case\n\n\n\n\nRecall\nHow many actual positives were correctly identified.\nUseful when false negatives are costly.\n\n\nPrecision\nHow many positive predictions are correct.\nUseful when false positives are costly.\n\n\n\n\n\n\n6.4.8 12. Recall vs Accuracy\n\n6.4.8.1 Why Not Use Accuracy Instead?\nAccuracy measures overall correctness but fails to account for the balance between true positives and false negatives, which is critical in recall-focused applications.\n\n\n6.4.8.2 Example:\nConsider a dataset with 99% negatives and 1% positives: - A model predicting all negatives achieves 99% accuracy. - However, recall would be 0% because it fails to identify any positives.\n\n\n\n\n6.4.9 13. Summary\n\n\n\n\n\n\n\nAspect\nRecall\n\n\n\n\nDefinition\nThe proportion of actual positives that are correctly predicted.\n\n\nFormula\n( = )\n\n\nUse Cases\nMedical diagnosis, fraud detection, search engines.\n\n\nStrengths\nEnsures no positive sample is missed; critical when false negatives are costly.\n\n\nWeaknesses\nIgnores false positives; can result in a high false alarm rate.\n\n\n\nRecall is crucial in applications where missing positive instances is significantly more harmful than identifying false positives. However, it is often used alongside precision or F1-score for a balanced evaluation.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Day5 Model Evaluation Metrics</span>"
    ]
  }
]