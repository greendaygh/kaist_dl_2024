[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "KAIST Deep Learning",
    "section": "",
    "text": "1 KAIST EB502 programming",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "KAIST Deep Learning",
    "section": "1.1 Welcome",
    "text": "1.1 Welcome\n\n2024.11 카이스트, 공학생물학대학원 프로그래밍 강의 노트\nHaseong Kim (at KRIBB)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#환경",
    "href": "index.html#환경",
    "title": "KAIST Deep Learning",
    "section": "1.2 환경",
    "text": "1.2 환경\n\n실습 환경은 colab을 활용하며 파일 저장 등은 구글 드라이브를 활용함",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "day1_optimization.html",
    "href": "day1_optimization.html",
    "title": "2  Day1 Optimization",
    "section": "",
    "text": "Learn python for biological data analysis with chatGPT\ncolab의 default working directory에 개인의 google drive 연결\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\n2.0.1 Introduction of Google Colab\n\n2.0.1.1 Access Google Colab\n\nGo to Google Colab in your web browser.\nSign in with your Google account.\n\n\n\n2.0.1.2 Create a New Notebook\n\nClick on File -&gt; New Notebook to create a new notebook.\n\n\n\n2.0.1.3 Install Required Libraries\nGoogle Colab comes with many libraries pre-installed, but you might need to install some additional ones, such as biopython and scikit-bio. You can do this using the !pip install command directly in a cell.\n\n!pip install biopython scikit-bio matplotlib\n\n\n!pip install scikit-bio\n\n\n\n2.0.1.4 Import Libraries and Verify Installation\nIn a new code cell, import the libraries to ensure they are installed correctly.\n\n# Importing necessary libraries\nimport Bio\nimport skbio\n\nprint(\"Biopython version:\", Bio.__version__)\nprint(\"scikit-bio version:\", skbio.__version__)\n\nBiopython version: 1.84\nscikit-bio version: 0.6.2\n\n\n\n\n2.0.1.5 Upload Files to Colab\n\nCreate 2024-kaist-lecture folder\nipynb file open with colab\nDownload ganbank files from ncbi and upload the files\ncurrent directory\n\n\n!pwd\n\n/home/haseong/lecture/kaist-deeplearning-2024\n\n\n\n현재 작업 디렉토리를 위 생성한 디렉토리로 변경\n\n\nimport os\nos.chdir('drive/MyDrive/2024-kaist-lecture')\n\n\n!pwd\n\n/content/drive/MyDrive/2024-kaist-lecture\n\n\n\n분석을 위한 genbank 등의 파일을 ncbi에서 다운로드 후 위 폴더에 복사\n또는 아래 코드를 이용해서 현재 작업 디렉토리에 업로드\n\n\nfrom google.colab import files\n\nuploaded = files.upload()\n\n# Listing the uploaded files\nfor filename in uploaded.keys():\n    print(filename)\n\n\n     \n     \n      Upload widget is only available when the cell has been executed in the\n      current browser session. Please rerun this cell to enable.\n      \n       \n\n\nSaving nn.png to nn.png\nnn.png\n\n\n\n\n\nimage.png\n\n\n\n\n\n2.0.2 NumPy\nNumPy is a powerful library for numerical operations and handling arrays.\n\n2.0.2.1 Basics of NumPy\nInstallation:\n!pip install numpy\n\nimport numpy as np\n\n\n# Creating a 1D array\narr1 = np.array([1, 2, 3, 4, 5])\n\n# Creating a 2D array\narr2 = np.array([[1, 2, 3], [4, 5, 6]])\n\nprint(arr1)\nprint(arr2)\n\n\n# Element-wise operations\narr3 = arr1 * 2\nprint(arr3)\n\n# Mathematical functions\nprint(np.sqrt(arr1))\n\n[1 2 3 4 5]\n[[1 2 3]\n [4 5 6]]\n[ 2  4  6  8 10]\n[1.         1.41421356 1.73205081 2.         2.23606798]\n\n\n\n\n2.0.2.2 Numpy datatype ndarray\n\n행렬이나 다차원 배열 처리용 파이썬 라이브러리\n같은 타입의 데이터만 허용\n리스트에 비해 20배 이상 빠른 속도\n\n\nimport numpy as np\n\ndisplay(np.ones(4))\ndisplay(np.ones((3, 4)))\ndisplay(np.ones((2, 3, 4)))\n\narray([1., 1., 1., 1.])\n\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.]])\n\n\narray([[[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]],\n\n       [[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]]])\n\n\n\n\n\nalt text\n\n\n\nCreate numpy objects\n\n\nimport numpy as np\n\narr = [1, 2, 3]\nprint(arr)\nprint(type(arr))\n\na = np.array([1,2,3])\nprint(a)\nprint(a.dtype)\nprint(a.shape)\nprint(type(a))\n\n[1, 2, 3]\n&lt;class 'list'&gt;\n[1 2 3]\nint64\n(3,)\n&lt;class 'numpy.ndarray'&gt;\n\n\n\narr2 = np.array([[1,2,3], [4,5,6]])\nprint(arr2)\nprint(type(arr2))\nprint(arr2.shape)\nprint(arr2.dtype)\n\n[[1 2 3]\n [4 5 6]]\n&lt;class 'numpy.ndarray'&gt;\n(2, 3)\nint64\n\n\n\nnumpy 자료형\n\n부호가 있는 정수 int(8, 16, 32, 64)\n부호가 없는 정수 uint(8 ,16, 32, 54)\n실수 float(16, 32, 64, 128)\n복소수 complex(64, 128, 256)\n불리언 bool\n문자열 string_\n파이썬 오프젝트 object\n유니코드 unicode_\n\nnp.zeros(), np.ones(), np.arange()\n행렬 연산 지원\n\n\na = np.arange(1, 10).reshape(3,3) # [1, 10)\nprint(a)\na = np.ones((3,4), dtype=np.int16)\nb = np.ones((3,4), dtype=np.int16)\nprint(a)\nprint(b)\nprint(a+b)\nprint(a-b)\n\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\n[[1 1 1 1]\n [1 1 1 1]\n [1 1 1 1]]\n[[1 1 1 1]\n [1 1 1 1]\n [1 1 1 1]]\n[[2 2 2 2]\n [2 2 2 2]\n [2 2 2 2]]\n[[0 0 0 0]\n [0 0 0 0]\n [0 0 0 0]]\n\n\n\nnumpy 함수\n\nnp.sqrt()\nnp.log()\nnp.square()\nnp.log()\nnp.ceil()\nnp.floor()\nnp.isnan()\nnp.sum()\nnp.mean()\nnp.std()\nnp.min()\n\n\n\n\n\n2.0.3 Simple linear regression\n\nModel\n\\[\ny_i = b_0 + b_1 x_i  + \\epsilon_i \\text{ where } \\epsilon_i \\sim \\text{ iid } N(0, \\sigma^2)\n\\]\nparameters\n\n\\[\n\\theta = \\{ b_0, b_1 \\}\n\\]\n\nFind \\(\\theta\\) that minimize residuals\n\n\\[\n\\sum_{i=i}^n r_i^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2  \\\\\n\\sum_{i=1}^n (y_i - \\hat{b_1}x_i - \\hat{b_0})^2\n\\]\n\nresiduals: difference between sample observed and estimated values\n\n\nimport numpy as np\n\nnp.random.seed(123)\nX = 2 * np.random.rand(20, 1)\nY = 4 + X*0.8 + np.random.rand(20, 1)\n\nX_b = np.c_[np.ones(len(X)), X]\n# print(X)\ntheta_best = np.linalg.inv(X_b.T @ X_b) @ (X_b.T) @ Y\nY_pred_org = X_b @ theta_best\n# print(theta_best)\n\nX_new = 2 * np.random.rand(100, 1)\nX_new_b = np.c_[np.ones(len(X_new)), X_new]\nY_pred = X_new_b @ theta_best\n\nimport matplotlib.pyplot as plt\nplt.scatter(X, Y, color=\"#000000\")\nplt.plot(X_new, Y_pred, color='#cccccc', label='Predictions')\n\n# Plot residuals\nfor i in range(len(Y)):\n    plt.vlines(x=X[i], ymin=min(Y[i], Y_pred_org[i]), ymax=max(Y[i], Y_pred_org[i]), color='green', linestyle='dotted')\n\n\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.plot()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2.0.4 Ordinary least sequare (OLS)\n\nModel\n\n\\[\ny_i = b_0 + b_1 x_i  + \\epsilon_i \\text{ where } \\epsilon_i \\sim \\text{ iid } N(0, \\sigma^2), i = 1, 2, ..., n\n\\]\n\\[\nY = X \\beta + \\epsilon\n\\]\n\\[\\begin{equation}\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n... \\\\\ny_n\n\\end{bmatrix}\n\n=\n\n\\begin{pmatrix}\n1 \\ \\ x_1 \\\\\n1 \\ \\ x_2 \\\\\n... \\\\\n1 \\ \\ x_n\n\\end{pmatrix}\n\n\\begin{pmatrix}\nb_0 \\\\\nb_1\n\\end{pmatrix}\n\n+\n\n\n\\begin{pmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n... \\\\\n\\epsilon_n\n\\end{pmatrix}\n\\end{equation}\\]\n\\[\n\\mathbf{\\epsilon} = Y - X\\beta\n\\]\n\nResidual Sum of Squares (RSS)\n\n\\[\nRSS = (Y-X\\beta)^T(Y-X\\beta)\n\\]\n\nTake the gradient with respect to \\(\\beta\\) and set it to zero (Normal equation)\n\n\\[\n\\frac{\\partial RSS}{\\partial \\beta} = -2 X^T Y + 2 X^T X \\beta = 0\n\\]\n\nSolve for \\(\\beta\\) if \\((X^TX)^{-1}\\) exists\n\n\\[\n(X^T X) \\beta = X^T Y\n\\]\n\\[\n\\hat{\\beta} = (X^T X)^{-1} X^T Y\n\\]\n\n\n2.0.5 Maximum Likelihood Estimation (MLE)\n\nDetails: https://statproofbook.github.io/P/slr-mle.html\nConsider the regression as a joint probability model\nReasons to use\n\nThis framework is applicable to other complex models (non-linear, neural network)\n\nBayes rule where \\(D\\) is data, \\(\\theta\\) is parameter\n\n\\[\np(\\theta | D) = \\frac{p(D|\\theta) p(\\theta)}{p(D)}\n\\]\n\\[\n\\text{where $p(\\theta|D)$, $p(D|\\theta)$, $p(\\theta)$ are posterior, likelihood and prior, respectively}\n\\]\n\\[\np(\\theta | D) \\propto p(D|\\theta)\n\\]\n\nRegarding the likelihood, \\(p(Y|X, \\theta)\\) is interpreted as how the behaviour of the response \\(Y\\) is conditional on the values of the feature, \\(X\\), and parameters, \\(\\theta\\)\n\n\\[\n\\begin{align}\np(Y | X, \\theta)  = \\prod_{i=1}^n p( y_i | x_i, \\hat{\\theta})\n\\end{align}\n\\]\n\nThen, we can ask what is the probability of seeing the data, given a specific set of parameters? (== How the data likely to be observed given the parameters == which parameters maximize the likelihood)\n\n\\[\n\\hat{\\theta} = \\text{argmax}_\\theta \\text{ log } \\sum_{i=1}^n p( y_i | x_i, \\theta)\n\\]\n\nFor \\(p(y_i | x_i, \\theta)\\), we have assumption that all feature vectors are iid\n\n\\[\nY \\sim N(X\\beta, \\sigma)\n\\]\n\\[\n\\begin{align}\np( y_i | x_i, \\theta) &= N(y_i; X\\beta, \\sigma^2) \\\\\n&= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp{\\left( - \\frac{(y_i - b_0 - b_1 x_i)^2}{2 \\sigma^2} \\right)}\n\\end{align}\n\\]\n\nLog likelihood (LL) function\n\n\\[\n\\begin{align}\nLL(\\theta) &= \\text{ log } \\left( \\prod_{i=1}^n p( y_i | x_i, \\theta) \\right) \\\\\n&= \\text{ log } \\left( \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp{ \\left(- \\frac{(y_i - b_0 - b_1 x_i)^2}{2 \\sigma^2} \\right)} \\right) \\\\\n&= \\text{ log } \\left( \\frac{1}{\\sqrt{(2\\pi\\sigma^2)^n}} \\exp{ \\left( - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - b_0 - b_1 x_i)^2 \\right)} \\right) \\\\\n&= - \\frac{n}{2} \\log(2\\pi) - \\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - b_0 - b_1 x_i)^2\n\\end{align}\n\\]\n\nTake the gradient with respect to \\(\\beta\\) and set it to zero (OLS)\n\n\\[\n\\frac{\\partial LL(b_0, b_1, \\sigma^2)}{\\partial b_0} = \\frac{1}{\\sigma^2} \\sum_{i=1}^n (y_i - b_0 - b_1 x_i)\n\\]\n\\[\n\\begin{align}\n\\frac{\\partial LL(\\hat{b}_0, \\hat{b}_1, \\sigma^2)}{\\partial b_0}  = 0 \\\\\n\\sum_{i=1}^n (y_i - \\hat{b}_0 - \\hat{b}_1 x_i) = 0 \\\\\n\\hat{b}_0 = \\frac{1}{n}\\sum_{i=1}^n y_i - \\hat{b}_1 \\frac{1}{n} \\sum_{i=1}^n x_i \\\\\n\\end{align}\n\\]\n\\[\n\\frac{\\partial LL(\\hat{b}_0, b_1, \\sigma^2)}{\\partial b_1} = \\frac{1}{\\sigma^2} \\sum_{i=1}^n (x_i y_i - \\hat{b}_0 x_i - b_1 x_i^2)\n\\]\n\\[\n\\begin{align}\n\\frac{\\partial LL(\\hat{b}_0, \\hat{b}_1, \\sigma^2)}{\\partial b_1}  = 0 \\\\\n\\hat{b}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2 }\n\\end{align}\n\\]\n\nMaximize with respect to \\(\\sigma^2\\)\n\n\\[\n\\frac{\\partial LL(\\hat{b}_0, \\hat{b}_1, \\sigma^2)}{\\partial \\sigma^2} = - \\frac{n}{2 \\sigma^2} + \\frac{1}{2 (\\sigma^2)^2} \\sum_{i=1}^n (y_i - \\hat{b}_0 - \\hat{b}_1 x_i)^2\n\\]\n\\[\n\\frac{\\partial LL(\\hat{b}_0, \\hat{b}_1, \\hat{\\sigma}^2)}{\\partial \\sigma^2} = 0 \\\\\n\\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{b}_0 - \\hat{b}_1 x_i)^2\n\\]\n\nIn linear regression, MLE naturally leads to the OLS solution under the assumption of normally distributed residuals.\n\n\\[\n\\hat{\\beta} = (X^T X)^{-1} X^T Y\n\\]\n\\[\n\\hat{\\sigma}^2 = \\frac{1}{n} (Y-X\\beta)^T (Y-X\\beta)\n\\]\n\nHowever, MLE’s flexibility (e.g. customizable likelihood) extends beyond linear models, making it indispensable for logistic regression, mixture models, and modern deep learning frameworks.\n\n\n\n2.0.6 Gradient Decent\n\nAn iterative optimization algorithm for adjusting \\(\\beta\\) by minimizing a cost function. In linear regression, the cost function is the Mean Squared Error (MSE) under the assumption of normally distributed residuals.\nDefine a cost function \\(J(\\theta)\\)\n\n\\[\nLL(\\theta) = - \\frac{n}{2} \\log(2\\pi) - \\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - b_0 - b_1 x_i)^2\n\\]\n\\[\nJ(\\beta) =  \\sum_{i=1}^n (y_i - b_0 - b_1 x_i)^2\n\\]\n\nmatrix notation\n\n\\[\nJ(\\beta) = || Y - X\\beta ||^2 = (Y - X\\beta)^T(Y - X\\beta)\n\\]\n\nL1 norm, L2 norm (a metric for length or magnitude of a vector/matrix) \\[\n||X||_1 = \\sum_{i}^n |x_i|\n\\]\n\n\\[\n||X||_2 = \\sqrt{\\sum_i^n x_i^2}\n\\]\n\\[\nJ(\\beta) = Y^T Y - 2 Y^T X \\beta + \\beta^T X^T X \\beta\n\\]\n\nGradient of the cost function\n\n\\[\n\\nabla_\\beta J(\\beta) = \\frac{\\partial J(\\beta)}{\\partial \\beta}\n\\]\n\\[\n\\begin{align}\n\\nabla_\\beta J(\\beta) &= 0 - 2 X^T Y + 2 X^T X \\beta \\\\\n&= - 2 X^T(Y-X\\beta)\n\\end{align}\n\\]\n\nparameter update rule\n\n\\[\n\\beta^{(t+1)} = \\beta^{(t)} - \\alpha \\nabla_\\beta J(\\beta) \\text{ where } \\alpha \\text{ is learning rate}\n\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate synthetic data\nnp.random.seed(42)\nX = 2 * np.random.rand(100, 1)  # 100 samples, 1 feature from [0, 1) uniform distribution\ny = 4 + 3 * X + np.random.randn(100, 1)  # y = 4 + 3X + noise (Gaussian noise)\n\n# Add bias term (intercept)\nX_b = np.c_[np.ones((X.shape[0], 1)), X]\n\n# Initialize parameters\nbeta = np.random.randn(2, 1)  # Random initial coefficients\nlearning_rate = 0.01\nn_iterations = 100\nm = X_b.shape[0]  # Number of samples\n\nbeta_updates = [beta.copy()]\n\n# Gradient Descent\nfor iteration in range(n_iterations):\n    gradients = -2/m * X_b.T @ (y - X_b @ beta)  # Compute gradient\n    beta = beta - learning_rate * gradients  # Update parameters\n    beta_updates.append(beta.copy())\n\n# Final parameters\nprint(\"Estimated coefficients (beta):\", beta)\n\n# Predictions\ny_pred = X_b @ beta\n\n# Plot the data and regression line\nplt.figure(figsize=(8, 6))\nplt.scatter(X, y, color=\"blue\", label=\"Data points\")\nplt.plot(X, y_pred, color=\"red\", label=\"Regression line\")\nplt.xlabel(\"X\")\nplt.ylabel(\"y\")\nplt.title(\"Linear Regression using Gradient Descent\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\nEstimated coefficients (beta): [[2.96262018]\n [3.80192916]]\n\n\n\n\n\n\n\n\n\n\n# Visualize beta updates\nfor i, beta in enumerate(beta_updates):\n    print(f\"Iteration {i}: beta = {beta.flatten()}\")\n\n# Plot convergence of coefficients\nbeta_updates = np.array(beta_updates).squeeze()\n\nplt.figure(figsize=(8, 6))\nplt.plot(range(n_iterations + 1), beta_updates[:, 0], label='Intercept (beta[0])')\nplt.plot(range(n_iterations + 1), beta_updates[:, 1], label='Slope (beta[1])')\nplt.xlabel('Iteration')\nplt.ylabel('Value of beta')\nplt.title('Convergence of Coefficients')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nIteration 0: beta = [0.01300189 1.45353408]\nIteration 1: beta = [0.12180499 1.56507648]\nIteration 2: beta = [0.22633422 1.67181808]\nIteration 3: beta = [0.32676535 1.77395782]\nIteration 4: beta = [0.42326689 1.8716864 ]\nIteration 5: beta = [0.5160004  1.96518667]\nIteration 6: beta = [0.60512075 2.05463391]\nIteration 7: beta = [0.69077645 2.14019616]\nIteration 8: beta = [0.77310984 2.22203453]\nIteration 9: beta = [0.85225741 2.30030345]\nIteration 10: beta = [0.92835001 2.37515099]\nIteration 11: beta = [1.00151308 2.44671909]\nIteration 12: beta = [1.0718669  2.51514384]\nIteration 13: beta = [1.13952675 2.58055569]\nIteration 14: beta = [1.20460319 2.64307972]\nIteration 15: beta = [1.26720221 2.70283582]\nIteration 16: beta = [1.32742539 2.75993895]\nIteration 17: beta = [1.38537016 2.81449929]\nIteration 18: beta = [1.4411299 2.8666225]\nIteration 19: beta = [1.49479416 2.91640985]\nIteration 20: beta = [1.54644877 2.96395843]\nIteration 21: beta = [1.59617603 3.00936133]\nIteration 22: beta = [1.64405484 3.05270779]\nIteration 23: beta = [1.69016085 3.09408334]\nIteration 24: beta = [1.73456657 3.13357001]\nIteration 25: beta = [1.77734155 3.17124641]\nIteration 26: beta = [1.81855245 3.20718793]\nIteration 27: beta = [1.85826316 3.24146681]\nIteration 28: beta = [1.89653497 3.27415234]\nIteration 29: beta = [1.93342661 3.30531092]\nIteration 30: beta = [1.96899442 3.33500621]\nIteration 31: beta = [2.00329239 3.36329925]\nIteration 32: beta = [2.03637228 3.39024855]\nIteration 33: beta = [2.06828373 3.4159102 ]\nIteration 34: beta = [2.09907433 3.44033798]\nIteration 35: beta = [2.1287897  3.46358343]\nIteration 36: beta = [2.15747358 3.48569598]\nIteration 37: beta = [2.1851679 3.506723 ]\nIteration 38: beta = [2.21191288 3.52670991]\nIteration 39: beta = [2.23774706 3.54570024]\nIteration 40: beta = [2.26270741 3.56373574]\nIteration 41: beta = [2.28682934 3.58085643]\nIteration 42: beta = [2.31014685 3.59710066]\nIteration 43: beta = [2.3326925 3.6125052]\nIteration 44: beta = [2.35449751 3.62710531]\nIteration 45: beta = [2.37559184 3.64093478]\nIteration 46: beta = [2.39600419 3.65402601]\nIteration 47: beta = [2.41576208 3.66641005]\nIteration 48: beta = [2.43489191 3.67811668]\nIteration 49: beta = [2.45341896 3.68917444]\nIteration 50: beta = [2.47136752 3.69961069]\nIteration 51: beta = [2.48876082 3.70945165]\nIteration 52: beta = [2.50562118 3.71872248]\nIteration 53: beta = [2.52196997 3.72744726]\nIteration 54: beta = [2.53782769 3.73564912]\nIteration 55: beta = [2.55321401 3.74335019]\nIteration 56: beta = [2.56814776 3.75057171]\nIteration 57: beta = [2.58264703 3.75733403]\nIteration 58: beta = [2.59672912 3.76365667]\nIteration 59: beta = [2.61041067 3.76955833]\nIteration 60: beta = [2.62370759 3.77505693]\nIteration 61: beta = [2.63663515 3.78016967]\nIteration 62: beta = [2.64920801 3.78491302]\nIteration 63: beta = [2.66144021 3.78930278]\nIteration 64: beta = [2.6733452  3.79335407]\nIteration 65: beta = [2.68493589 3.79708142]\nIteration 66: beta = [2.69622467 3.80049874]\nIteration 67: beta = [2.70722341 3.80361935]\nIteration 68: beta = [2.71794348 3.80645605]\nIteration 69: beta = [2.7283958  3.80902108]\nIteration 70: beta = [2.73859083 3.81132618]\nIteration 71: beta = [2.74853861 3.81338263]\nIteration 72: beta = [2.75824876 3.8152012 ]\nIteration 73: beta = [2.7677305  3.81679224]\nIteration 74: beta = [2.77699268 3.81816566]\nIteration 75: beta = [2.78604379 3.81933097]\nIteration 76: beta = [2.79489196 3.82029728]\nIteration 77: beta = [2.803545   3.82107331]\nIteration 78: beta = [2.81201037 3.82166745]\nIteration 79: beta = [2.82029527 3.82208769]\nIteration 80: beta = [2.82840657 3.82234175]\nIteration 81: beta = [2.83635086 3.82243698]\nIteration 82: beta = [2.84413447 3.82238045]\nIteration 83: beta = [2.85176348 3.82217893]\nIteration 84: beta = [2.85924369 3.8218389 ]\nIteration 85: beta = [2.8665807  3.82136659]\nIteration 86: beta = [2.87377985 3.82076795]\nIteration 87: beta = [2.88084627 3.8200487 ]\nIteration 88: beta = [2.8877849  3.81921431]\nIteration 89: beta = [2.89460044 3.81827003]\nIteration 90: beta = [2.90129743 3.81722089]\nIteration 91: beta = [2.90788021 3.8160717 ]\nIteration 92: beta = [2.91435295 3.81482709]\nIteration 93: beta = [2.92071965 3.81349148]\nIteration 94: beta = [2.92698413 3.81206911]\nIteration 95: beta = [2.93315007 3.81056405]\nIteration 96: beta = [2.93922099 3.8089802 ]\nIteration 97: beta = [2.94520029 3.80732128]\nIteration 98: beta = [2.9510912  3.80559087]\nIteration 99: beta = [2.95689684 3.8037924 ]\nIteration 100: beta = [2.96262018 3.80192916]\n\n\n\n\n\n\n\n\n\n\n2.0.6.1 Reasons to use GD instead of MLE, OLS\n\nGradient Descent is favored over MLE or OLS in scenarios involving large-scale data, high-dimensional features, non-linear models, or custom loss functions due to its flexibility, efficiency, and scalability. However, for simple, small-scale problems, OLS or MLE may still be preferred for their directness and precision.\n\n\n\n\n2.0.7 Model fitting\n\nFinding parameters with LSE, MLE, and GD\nGD provides more flexible (non-linear) and scalable (high-dimentional data) way for modeling\nGD procedure\n\nSet random initial parameters\nCompute gradient that reduces cost\nParameter update until conversing",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Day1 Optimization</span>"
    ]
  },
  {
    "objectID": "day2_neural_networks.html",
    "href": "day2_neural_networks.html",
    "title": "3  Day2 Neural Networks",
    "section": "",
    "text": "3.0.1 Modeling procedure (Testing vs. Prediction)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Day2 Neural Networks</span>"
    ]
  },
  {
    "objectID": "day2_neural_networks.html#pytorch",
    "href": "day2_neural_networks.html#pytorch",
    "title": "3  Day2 Neural Networks",
    "section": "3.1 PyTorch",
    "text": "3.1 PyTorch\nPyTorch is an open-source deep learning framework developed by Facebook’s AI Research lab (FAIR). It is widely used in research and industry due to its dynamic computation graph and ease of use.\nPyTorch Ecosystem Overview:\ntorch: The core library for tensor operations and automatic differentiation.\ntorch.nn: A sub-library used to build and train neural network models.\ntorch.optim: Tools for optimization algorithms (e.g., SGD, Adam).\ntorchvision: Provides datasets, pre-trained models, and image transformations.\nTensors\nTensors are the primary data structures in PyTorch, analogous to NumPy arrays but with added capabilities such as the ability to run on GPUs for faster computation.\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n\n\n\n# Input data and true labels\nX = torch.tensor([[0.5, 0.2], [0.1, 0.4], [0.6, 0.9]], dtype=torch.float32)  # Shape (3 samples, 2 features)\ny = torch.tensor([[1], [0], [1]], dtype=torch.float32)                      # True labels (Shape: 3x1)\n\n# Define the neural network\nclass SimpleNN(nn.Module):\n    def __init__(self):\n        super(SimpleNN, self).__init__()\n        # Define layers\n        self.hidden = nn.Linear(2, 3)  # Input to hidden layer (2 inputs, 3 hidden nodes)\n        self.output = nn.Linear(3, 1)  # Hidden to output layer (3 hidden nodes, 1 output)\n\n    def forward(self, x):\n        # Forward pass: Input -&gt; Hidden Layer -&gt; Output Layer\n        x = torch.relu(self.hidden(x))       # ReLU activation for hidden layer\n        x = torch.sigmoid(self.output(x))    # Sigmoid activation for output layer\n        return x\n\n# Initialize the network\nmodel = SimpleNN()\n\n# Loss function and optimizer\ncriterion = nn.MSELoss()  # Mean Squared Error Loss\noptimizer = optim.SGD(model.parameters(), lr=0.1)  # Stochastic Gradient Descent optimizer\n\n# Training parameters\nepochs = 1000\n\n# Containers to store loss and accuracy for each epoch\nloss_history = []\naccuracy_history = []\n\n\n# Training loop\nfor epoch in range(epochs):\n    # Forward pass\n    outputs = model(X)  # Predicted outputs\n    loss = criterion(outputs, y)  # Compute loss\n\n    # Compute accuracy\n    predicted = (outputs &gt;= 0.5).float()  # Threshold at 0.5 for binary classification\n    accuracy = (predicted == y).sum().item() / y.size(0)\n\n\n    # Backpropagation\n    optimizer.zero_grad()  # Clear previous gradients\n    loss.backward()        # Compute gradients\n    optimizer.step()       # Update weights\n\n    # Record loss and accuracy\n    loss_history.append(loss.item())\n    accuracy_history.append(accuracy)\n\n\n    # Print loss every 100 epochs\n    if epoch % 100 == 0:\n        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n\n# Final predictions\nwith torch.no_grad():  # No need to compute gradients during inference\n    final_outputs = model(X)\n    print(\"\\nFinal Predictions:\")\n    print(final_outputs)\n\n\n# Plot the loss and accuracy over epochs\nplt.figure(figsize=(12, 5))\n\n# Loss plot\nplt.subplot(1, 2, 1)\nplt.plot(range(epochs), loss_history, label=\"Loss\", color=\"red\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss over Epochs\")\nplt.grid(True)\nplt.legend()\n\n# Accuracy plot\nplt.subplot(1, 2, 2)\nplt.plot(range(epochs), accuracy_history, label=\"Accuracy\", color=\"blue\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy over Epochs\")\nplt.grid(True)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\nEpoch 0, Loss: 0.2275\nEpoch 100, Loss: 0.2192\nEpoch 200, Loss: 0.2141\nEpoch 300, Loss: 0.2061\nEpoch 400, Loss: 0.1925\nEpoch 500, Loss: 0.1692\nEpoch 600, Loss: 0.1317\nEpoch 700, Loss: 0.0874\nEpoch 800, Loss: 0.0543\nEpoch 900, Loss: 0.0376\n\nFinal Predictions:\ntensor([[0.8810],\n        [0.2476],\n        [0.9134]])",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Day2 Neural Networks</span>"
    ]
  },
  {
    "objectID": "day3_cnn_dna.html",
    "href": "day3_cnn_dna.html",
    "title": "4  Day3 Convolution Neural Networks with DNA sequence",
    "section": "",
    "text": "4.0.1 Objectives\n\nDeveloping a CNN Model for classifying sequences\nExample: Developing a model to identify specific DNA motifs bound by an arbitrary transcription factor. The model predicts whether a given transcription factor binds to an input DNA sequence (output: 1) or not (output: 0).”\n\n\n\n4.0.2 Data\n\nTo train a deep learning model, labeled data is required (though not always necessary in modern self-supervised learning approaches).\nIn sequence analysis, DNA sequences paired with their corresponding phenotypes can serve as labeled data (genotype-phenotype paired data).\n\nFor example, to train a deep learning model to predict DNA sequences bound by a specific transcription factor, you would need a dataset containing transcription factor sequence data along with labels indicating whether or not the transcription factor binds to a given DNA sequence (True or False).\n\nGenerally, data for statistical analysis is represented as a 2D array, with rows corresponding to samples and columns to variables. In deep learning, data is represented in the same way. The number of samples required depends on the complexity of the model, but typically, at least thousands of samples are needed. Using tens of thousands or more samples is recommended for optimal results.\nA dataset collected for deep learning is divided into training and test datasets. Sometimes, the training dataset is further split into training and validation datasets for model development and performance evaluation.\n\n\n\n4.0.3 One-hot encoding\n\nFor deep learning, data must be represented numerically in a format that machines can process.\nOne-hot encoding is one of the most widely used methods in deep learning.\nFor DNA sequences with four types of nucleotides, encoding can be done as follows:\n\n“A” → [1, 0, 0, 0]\n\n“T” → [0, 0, 0, 1]\n\n“G” → [0, 0, 1, 0]\n\n“C” → [0, 1, 0, 0]\n\n\n\nimport numpy as np\n\nmy_string=\"ATACAA\"\nmy_array=np.array(list(my_string))\nprint(my_array)\n\n['A' 'T' 'A' 'C' 'A' 'A']\n\n\n\nlist(my_string)\n\n['A', 'T', 'A', 'C', 'A', 'A']\n\n\n\nnp.zeros((7,5))\n\narray([[0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.]])\n\n\n\nbox = np.zeros((3, 7, 5))\ntype(box)\nbox\n\narray([[[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]]])\n\n\n\nonehot_encode = np.zeros((len(my_array),4), dtype=int)\nbase_dict = {\"A\":0, \"C\":1, \"G\":2, \"T\":3}\nfor i in range(len(my_array)):\n    onehot_encode[i, base_dict[my_array[i]]] = 1\n\nprint(onehot_encode)\nprint(onehot_encode.shape)\n\n[[1 0 0 0]\n [0 0 0 1]\n [1 0 0 0]\n [0 1 0 0]\n [1 0 0 0]\n [1 0 0 0]]\n(6, 4)\n\n\n\n\n4.0.4 Set sequence motif for simulation\n\nUnderstanding the concepts of PFM (Position Frequency Matrix) and PWM (Position Weight Matrix) is essential.\nAssuming an alignment of several sequences, the PFM represents the frequency of each nucleotide (A, T, G, C) at specific positions in the sequences, while the PWM represents the proportion of each nucleotide at those positions.\n\n\nfrom Bio import motifs\nfrom Bio.Seq import Seq\n\ninstances = [Seq(\"TACAA\"), Seq(\"TACGA\"), Seq(\"TACAA\")]\nm = motifs.create(instances)\npfm = m.counts\nprint(pfm)\npwm = m.counts.normalize(pseudocounts=0.5)\nprint (pwm)\n\n        0      1      2      3      4\nA:   0.00   3.00   0.00   2.00   3.00\nC:   0.00   0.00   3.00   0.00   0.00\nG:   0.00   0.00   0.00   1.00   0.00\nT:   3.00   0.00   0.00   0.00   0.00\n\n        0      1      2      3      4\nA:   0.10   0.70   0.10   0.50   0.70\nC:   0.10   0.10   0.70   0.10   0.10\nG:   0.10   0.10   0.10   0.30   0.10\nT:   0.70   0.10   0.10   0.10   0.10\n\n\n\n\nPseudocounts are used in calculations to avoid division by NULL or zero.\nThe PWM (Position Weight Matrix) of a specific sequence motif can be used to search for the motif’s location in a new sequence provided in one-hot encoding format.\nUsing a sliding window approach, the motif can be scanned from the beginning to the end of the sequence to identify its presence.\nThe following demonstrates how to detect the presence of a given PWM motif (assuming a binary 0 and 1 representation) in a sequence.\n\n\n\n\nalt text\n\n\n\nIf there is a ‘5’ in the length-3 array as shown above, it indicates that the target sequence contains a sequence matching the motif.\nTo search for the presence of the PWM motif in the sequence “ATACAA,” a sliding window of length 5 can be used to divide the sequence into two sub-sequences: “ATACA” and “TACAA.”\n\nBy converting these two sub-sequences into one-hot encoding and multiplying their elements with the corresponding elements of the PWM, only the PWM values at the non-zero positions of the one-hot encoding remain.\nTo quantify how similar a given sequence is to the motif: 1. Multiply all non-zero values from the PWM. 2. Take the logarithm of the result.\nThe resulting scalar value indicates the similarity between the sequence and the motif. Theoretically, a value of 0 implies an identical sequence match to the motif.\n\n\n\npwm_arr = np.array(list(pwm.values())).transpose()\nprint(pwm_arr.shape)\n\nprint(onehot_encode.shape)\nprint(onehot_encode[0:5,].shape)\nprint(onehot_encode[1:6,].shape)\n\ns1 = np.multiply(onehot_encode[0:5,], pwm_arr)\ns2 = np.multiply(onehot_encode[1:6,], pwm_arr)\nprint(s1)\nprint(s2)\n\nprint(np.sum(s1, axis=1))\nprint(np.prod(np.sum(s1, axis=1)))\n\nprint(np.log(np.prod(np.sum(s1, axis=1)))) #s1 score\nprint(np.log(np.prod(np.sum(s2, axis=1)))) #s2 score\n\n(5, 4)\n(6, 4)\n(5, 4)\n(5, 4)\n[[0.1 0.  0.  0. ]\n [0.  0.  0.  0.1]\n [0.1 0.  0.  0. ]\n [0.  0.1 0.  0. ]\n [0.7 0.  0.  0. ]]\n[[0.  0.  0.  0.7]\n [0.7 0.  0.  0. ]\n [0.  0.7 0.  0. ]\n [0.5 0.  0.  0. ]\n [0.7 0.  0.  0. ]]\n[0.1 0.1 0.1 0.1 0.7]\n7.000000000000002e-05\n-9.567015315914915\n-2.119846956314875\n\n\n\nDeep learning styled array\n\n\n\n\nalt text\n\n\n\n\n4.0.5 Simulation data generation\nGenerate 1,000 simulated positive sequences by embedding a motif in the middle of the sequences, and 1,000 negative sequences with random DNA sequences.\n\nimport numpy as np\nseq_length = 20\nnum_sample = 1000\n#motif CCGGAA\nmotif_pwm = np.array([[10.41, 22.86, 1.92, 1.55, 98.60, 86.66],\n            [68.20, 65.25, 0.50, 0.35, 0.25, 2.57],\n            [17.27, 8.30, 94.77, 97.32, 0.87, 0.00],\n            [4.13, 3.59, 2.81, 0.78, 0.28, 10.77]])\npwm = np.hstack([np.ones((4, 7)), motif_pwm, np.ones((4, 7))])\npos = np.array([np.random.choice( ['A', 'C', 'G', 'T'], num_sample,\n                                  p=pwm[:,i]/sum(pwm[:,i])) for i in range(seq_length)]).transpose()\nneg = np.array([np.random.choice( ['A', 'C', 'G', 'T'], num_sample,\n                                  p=np.array([1,1,1,1])/4) for i in range(seq_length)]).transpose()\n\nprint(pos.shape)\ndisplay([''.join(x) for x in pos[1:5,]])\nprint()\ndisplay([''.join(x) for x in neg[1:5,]])\n\n(1000, 20)\n\n\n\n['CAGGTGGCCGGATTCTAGCA',\n 'TCGACTTGCGGAACAGGTTC',\n 'AGACGGGCCGGAACATAGTC',\n 'CTTAGCGCCGGATCTACCCG']\n\n\n['CTGTCACGCACGATCCGAAA',\n 'GGTGCCAAAGTAGAACGGAA',\n 'CCACGGTATTCTGCAACCCA',\n 'AAGCGAGGTTAACTACGAGC']\n\n\n\n\n4.0.6 DNA data preprocessing\n\nbase_dict = {'A':0, 'C':1, 'G':2, 'T':3}\n\n# response variable for pos\nonehot_encode_pos = np.zeros((num_sample, seq_length, 4))\nonehot_encode_pos_label = np.zeros((num_sample, 2), dtype=int)\nonehot_encode_pos_label[:,0] = 1\n# print(onehot_encode_pos_label)\n\n# response variable for pos\nonehot_encode_neg = np.zeros((num_sample, seq_length, 4))\nonehot_encode_neg_label = np.zeros((num_sample, 2), dtype=int)\nonehot_encode_neg_label[:,1] = 1\n# print(onehot_encode_neg_label)\n\n# convert sequence to onehot\nfor i in range(num_sample):\n    for j in range(seq_length):\n        onehot_encode_pos[i,j,base_dict[pos[i,j]]] = 1\n        onehot_encode_neg[i,j,base_dict[neg[i,j]]] = 1\n\n# concatenation\nX = np.vstack((onehot_encode_pos, onehot_encode_neg))\ny = np.vstack((onehot_encode_pos_label, onehot_encode_neg_label))\n\nprint(X.shape, y.shape)\n# (2000, 20, 4) (2000, 2)\n\n(2000, 20, 4) (2000, 2)\n\n\n\nPyTorch Conv1d requires [batch_size, channels, length] so transpose(1,2) excuted\n\n\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\n# 데이터를 훈련 세트와 테스트 세트로 나눔\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=125)\nprint(X_train.shape, y_train.shape)\n\n# NumPy 배열을 PyTorch 텐서로 변환\nX_train = torch.tensor(X_train, dtype=torch.float32).transpose(1,2)\nX_test = torch.tensor(X_test, dtype=torch.float32).transpose(1,2)\ny_train = torch.tensor(y_train, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.float32)\nprint(y_test.dtype)\n\n# DataLoader 설정\ntrain_dataset = TensorDataset(X_train, y_train)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nprint(train_loader.dataset.tensors[0].shape)\nprint(train_loader.dataset.tensors[1].shape)\ntest_dataset = TensorDataset(X_test, y_test)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n(1600, 20, 4) (1600, 2)\ntorch.float32\ntorch.Size([1600, 4, 20])\ntorch.Size([1600, 2])\n\n\n\nimport torch\n\nX_torch = torch.tensor(X_train, dtype=torch.float32)\nprint(X_torch.shape)\n\ntorch.Size([1600, 4, 20])\n\n\n/tmp/ipykernel_617536/3124571761.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_torch = torch.tensor(X_train, dtype=torch.float32)\n\n\n\n\n4.0.7 CNN model class\n\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass DNA_CNN(nn.Module):\n    def __init__(self):\n        super(DNA_CNN, self).__init__()\n        self.conv1 = nn.Conv1d(in_channels=4, out_channels=16, kernel_size=3, padding=1)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool1d(kernel_size=2)\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(160, 64)  # Adjust the input features according to your pooling and conv1d output\n        self.fc2 = nn.Linear(64, 2)  # Adjust according to your problem's needs (e.g., number of classes)\n        #self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.flatten(x)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        #x = self.softmax(x)\n        return x\n\nmodel = DNA_CNN()\nif torch.cuda.is_available():\n    model.cuda()\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nfrom torchsummary import summary\nsummary(model, input_size=(4, 20))  # (Channels, Length)\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv1d-1               [-1, 16, 20]             208\n              ReLU-2               [-1, 16, 20]               0\n         MaxPool1d-3               [-1, 16, 10]               0\n           Flatten-4                  [-1, 160]               0\n            Linear-5                   [-1, 64]          10,304\n            Linear-6                    [-1, 2]             130\n================================================================\nTotal params: 10,642\nTrainable params: 10,642\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.01\nParams size (MB): 0.04\nEstimated Total Size (MB): 0.05\n----------------------------------------------------------------\n\n\n\n\n4.0.8 training\n\n# 훈련 루프\nnum_epochs = 20\nfor epoch in range(num_epochs):\n    for inputs, labels in train_loader:\n        if torch.cuda.is_available():\n            inputs, labels = inputs.cuda(), labels.cuda()\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n\nEpoch [1/20], Loss: 0.5252\nEpoch [2/20], Loss: 0.2159\nEpoch [3/20], Loss: 0.0913\nEpoch [4/20], Loss: 0.0608\nEpoch [5/20], Loss: 0.0462\nEpoch [6/20], Loss: 0.0589\nEpoch [7/20], Loss: 0.1240\nEpoch [8/20], Loss: 0.0366\nEpoch [9/20], Loss: 0.0820\nEpoch [10/20], Loss: 0.0629\nEpoch [11/20], Loss: 0.0405\nEpoch [12/20], Loss: 0.0538\nEpoch [13/20], Loss: 0.0198\nEpoch [14/20], Loss: 0.0228\nEpoch [15/20], Loss: 0.0570\nEpoch [16/20], Loss: 0.0315\nEpoch [17/20], Loss: 0.0680\nEpoch [18/20], Loss: 0.1277\nEpoch [19/20], Loss: 0.0137\nEpoch [20/20], Loss: 0.0320\n\n\n\n\n4.0.9 Model evaluation\n\n# 모델 평가\nmodel.eval()\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for inputs, labels in test_loader:\n        if torch.cuda.is_available():\n            inputs, labels = inputs.cuda(), labels.cuda()\n        outputs = model(inputs)\n        #print(outputs.data)\n        _, predicted = torch.max(outputs.data, 1)\n        #print(predicted)\n        total += labels.size(0)\n        labels_max = torch.max(labels, 1)[1]\n        #print(labels_max)\n        correct += (predicted == labels_max).sum().item()\n\n    print(f'Accuracy of the model on the test images: {100 * correct / total} %')\n\nAccuracy of the model on the test images: 98.5 %\n\n\n\n\n4.0.10 Prediction\n\nimport matplotlib.pyplot as plt\n\n# 데이터 저장을 위한 리스트 초기화\ntrain_losses = []\nval_accuracies = []\n\nnum_epochs = 200\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        if torch.cuda.is_available():\n            inputs, labels = inputs.cuda(), labels.cuda()\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * inputs.size(0)\n\n    epoch_loss = running_loss / len(train_loader.dataset)\n    train_losses.append(epoch_loss)\n\n    # 모델 평가\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            if torch.cuda.is_available():\n                inputs, labels = inputs.cuda(), labels.cuda()\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            labels_max = torch.max(labels, 1)[1]\n            correct += (predicted == labels_max).sum().item()\n\n    epoch_accuracy = 100 * correct / total\n    val_accuracies.append(epoch_accuracy)\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n\n# 그래프 그리기\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label='Training Loss')\nplt.title('Training Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(val_accuracies, label='Validation Accuracy')\nplt.title('Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy (%)')\nplt.legend()\n\nplt.show()\n\nEpoch [1/200], Loss: 0.0370, Accuracy: 97.25%\nEpoch [2/200], Loss: 0.0413, Accuracy: 98.00%\nEpoch [3/200], Loss: 0.0376, Accuracy: 97.75%\nEpoch [4/200], Loss: 0.0344, Accuracy: 97.75%\nEpoch [5/200], Loss: 0.0334, Accuracy: 97.25%\nEpoch [6/200], Loss: 0.0333, Accuracy: 97.75%\nEpoch [7/200], Loss: 0.0338, Accuracy: 98.00%\nEpoch [8/200], Loss: 0.0324, Accuracy: 97.25%\nEpoch [9/200], Loss: 0.0289, Accuracy: 97.75%\nEpoch [10/200], Loss: 0.0282, Accuracy: 97.75%\nEpoch [11/200], Loss: 0.0297, Accuracy: 98.25%\nEpoch [12/200], Loss: 0.0283, Accuracy: 97.25%\nEpoch [13/200], Loss: 0.0247, Accuracy: 97.50%\nEpoch [14/200], Loss: 0.0233, Accuracy: 97.25%\nEpoch [15/200], Loss: 0.0246, Accuracy: 97.25%\nEpoch [16/200], Loss: 0.0223, Accuracy: 97.75%\nEpoch [17/200], Loss: 0.0190, Accuracy: 97.75%\nEpoch [18/200], Loss: 0.0182, Accuracy: 97.50%\nEpoch [19/200], Loss: 0.0168, Accuracy: 97.75%\nEpoch [20/200], Loss: 0.0162, Accuracy: 97.50%\nEpoch [21/200], Loss: 0.0152, Accuracy: 97.75%\nEpoch [22/200], Loss: 0.0141, Accuracy: 97.50%\nEpoch [23/200], Loss: 0.0164, Accuracy: 97.75%\nEpoch [24/200], Loss: 0.0138, Accuracy: 97.50%\nEpoch [25/200], Loss: 0.0124, Accuracy: 97.75%\nEpoch [26/200], Loss: 0.0119, Accuracy: 97.50%\nEpoch [27/200], Loss: 0.0105, Accuracy: 97.50%\nEpoch [28/200], Loss: 0.0136, Accuracy: 97.50%\nEpoch [29/200], Loss: 0.0087, Accuracy: 97.50%\nEpoch [30/200], Loss: 0.0089, Accuracy: 97.50%\nEpoch [31/200], Loss: 0.0084, Accuracy: 97.50%\nEpoch [32/200], Loss: 0.0071, Accuracy: 97.50%\nEpoch [33/200], Loss: 0.0066, Accuracy: 97.50%\nEpoch [34/200], Loss: 0.0068, Accuracy: 97.25%\nEpoch [35/200], Loss: 0.0065, Accuracy: 97.50%\nEpoch [36/200], Loss: 0.0059, Accuracy: 97.50%\nEpoch [37/200], Loss: 0.0054, Accuracy: 97.50%\nEpoch [38/200], Loss: 0.0055, Accuracy: 97.00%\nEpoch [39/200], Loss: 0.0054, Accuracy: 97.50%\nEpoch [40/200], Loss: 0.0056, Accuracy: 97.00%\nEpoch [41/200], Loss: 0.0124, Accuracy: 97.00%\nEpoch [42/200], Loss: 0.0063, Accuracy: 97.25%\nEpoch [43/200], Loss: 0.0044, Accuracy: 97.25%\nEpoch [44/200], Loss: 0.0040, Accuracy: 97.00%\nEpoch [45/200], Loss: 0.0032, Accuracy: 97.25%\nEpoch [46/200], Loss: 0.0034, Accuracy: 97.00%\nEpoch [47/200], Loss: 0.0037, Accuracy: 97.25%\nEpoch [48/200], Loss: 0.0032, Accuracy: 97.50%\nEpoch [49/200], Loss: 0.0031, Accuracy: 97.25%\nEpoch [50/200], Loss: 0.0025, Accuracy: 97.75%\nEpoch [51/200], Loss: 0.0026, Accuracy: 97.25%\nEpoch [52/200], Loss: 0.0024, Accuracy: 97.50%\nEpoch [53/200], Loss: 0.0024, Accuracy: 97.50%\nEpoch [54/200], Loss: 0.0021, Accuracy: 97.50%\nEpoch [55/200], Loss: 0.0024, Accuracy: 97.25%\nEpoch [56/200], Loss: 0.0021, Accuracy: 97.25%\nEpoch [57/200], Loss: 0.0021, Accuracy: 97.25%\nEpoch [58/200], Loss: 0.0020, Accuracy: 97.50%\nEpoch [59/200], Loss: 0.0017, Accuracy: 97.50%\nEpoch [60/200], Loss: 0.0018, Accuracy: 97.50%\nEpoch [61/200], Loss: 0.0019, Accuracy: 97.50%\nEpoch [62/200], Loss: 0.0016, Accuracy: 97.50%\nEpoch [63/200], Loss: 0.0017, Accuracy: 97.50%\nEpoch [64/200], Loss: 0.0015, Accuracy: 97.50%\nEpoch [65/200], Loss: 0.0012, Accuracy: 97.25%\nEpoch [66/200], Loss: 0.0014, Accuracy: 97.25%\nEpoch [67/200], Loss: 0.0012, Accuracy: 97.25%\nEpoch [68/200], Loss: 0.0013, Accuracy: 97.25%\nEpoch [69/200], Loss: 0.0011, Accuracy: 97.25%\nEpoch [70/200], Loss: 0.0010, Accuracy: 97.25%\nEpoch [71/200], Loss: 0.0011, Accuracy: 97.25%\nEpoch [72/200], Loss: 0.0010, Accuracy: 97.25%\nEpoch [73/200], Loss: 0.0010, Accuracy: 97.25%\nEpoch [74/200], Loss: 0.0009, Accuracy: 97.25%\nEpoch [75/200], Loss: 0.0009, Accuracy: 97.25%\nEpoch [76/200], Loss: 0.0009, Accuracy: 97.50%\nEpoch [77/200], Loss: 0.0009, Accuracy: 97.25%\nEpoch [78/200], Loss: 0.0008, Accuracy: 97.25%\nEpoch [79/200], Loss: 0.0008, Accuracy: 97.50%\nEpoch [80/200], Loss: 0.0009, Accuracy: 97.50%\nEpoch [81/200], Loss: 0.0008, Accuracy: 97.50%\nEpoch [82/200], Loss: 0.0007, Accuracy: 97.25%\nEpoch [83/200], Loss: 0.0007, Accuracy: 97.25%\nEpoch [84/200], Loss: 0.0007, Accuracy: 97.25%\nEpoch [85/200], Loss: 0.0007, Accuracy: 97.50%\nEpoch [86/200], Loss: 0.0006, Accuracy: 97.25%\nEpoch [87/200], Loss: 0.0006, Accuracy: 97.25%\nEpoch [88/200], Loss: 0.0006, Accuracy: 97.25%\nEpoch [89/200], Loss: 0.0006, Accuracy: 97.25%\nEpoch [90/200], Loss: 0.0006, Accuracy: 97.25%\nEpoch [91/200], Loss: 0.0006, Accuracy: 97.25%\nEpoch [92/200], Loss: 0.0005, Accuracy: 97.50%\nEpoch [93/200], Loss: 0.0006, Accuracy: 97.50%\nEpoch [94/200], Loss: 0.0005, Accuracy: 97.25%\nEpoch [95/200], Loss: 0.0005, Accuracy: 97.25%\nEpoch [96/200], Loss: 0.0005, Accuracy: 97.25%\nEpoch [97/200], Loss: 0.0005, Accuracy: 97.25%\nEpoch [98/200], Loss: 0.0004, Accuracy: 97.50%\nEpoch [99/200], Loss: 0.0004, Accuracy: 97.25%\nEpoch [100/200], Loss: 0.0004, Accuracy: 97.25%\nEpoch [101/200], Loss: 0.0005, Accuracy: 97.50%\nEpoch [102/200], Loss: 0.0004, Accuracy: 97.50%\nEpoch [103/200], Loss: 0.0004, Accuracy: 97.25%\nEpoch [104/200], Loss: 0.0004, Accuracy: 97.50%\nEpoch [105/200], Loss: 0.0004, Accuracy: 97.25%\nEpoch [106/200], Loss: 0.0004, Accuracy: 97.50%\nEpoch [107/200], Loss: 0.0004, Accuracy: 97.50%\nEpoch [108/200], Loss: 0.0003, Accuracy: 97.25%\nEpoch [109/200], Loss: 0.0003, Accuracy: 97.25%\nEpoch [110/200], Loss: 0.0003, Accuracy: 97.50%\nEpoch [111/200], Loss: 0.0003, Accuracy: 97.50%\nEpoch [112/200], Loss: 0.0004, Accuracy: 97.50%\nEpoch [113/200], Loss: 0.0003, Accuracy: 97.50%\nEpoch [114/200], Loss: 0.0003, Accuracy: 97.25%\nEpoch [115/200], Loss: 0.0003, Accuracy: 97.25%\nEpoch [116/200], Loss: 0.0003, Accuracy: 97.50%\nEpoch [117/200], Loss: 0.0003, Accuracy: 97.25%\nEpoch [118/200], Loss: 0.0003, Accuracy: 97.25%\nEpoch [119/200], Loss: 0.0003, Accuracy: 97.50%\nEpoch [120/200], Loss: 0.0003, Accuracy: 97.25%\nEpoch [121/200], Loss: 0.0003, Accuracy: 97.25%\nEpoch [122/200], Loss: 0.0002, Accuracy: 97.25%\nEpoch [123/200], Loss: 0.0002, Accuracy: 97.25%\nEpoch [124/200], Loss: 0.0003, Accuracy: 97.25%\nEpoch [125/200], Loss: 0.0002, Accuracy: 97.25%\nEpoch [126/200], Loss: 0.0002, Accuracy: 97.25%\nEpoch [127/200], Loss: 0.0002, Accuracy: 97.25%\nEpoch [128/200], Loss: 0.0002, Accuracy: 97.25%\nEpoch [129/200], Loss: 0.0002, Accuracy: 97.50%\nEpoch [130/200], Loss: 0.0002, Accuracy: 97.25%\nEpoch [131/200], Loss: 0.0002, Accuracy: 97.50%\nEpoch [132/200], Loss: 0.0002, Accuracy: 97.50%\nEpoch [133/200], Loss: 0.0002, Accuracy: 97.25%\nEpoch [134/200], Loss: 0.0002, Accuracy: 97.25%\nEpoch [135/200], Loss: 0.0002, Accuracy: 97.50%\nEpoch [136/200], Loss: 0.0002, Accuracy: 97.25%\nEpoch [137/200], Loss: 0.0002, Accuracy: 97.50%\nEpoch [138/200], Loss: 0.0002, Accuracy: 97.25%\nEpoch [139/200], Loss: 0.0002, Accuracy: 97.25%\nEpoch [140/200], Loss: 0.0002, Accuracy: 97.50%\nEpoch [141/200], Loss: 0.0002, Accuracy: 97.50%\nEpoch [142/200], Loss: 0.0002, Accuracy: 97.25%\nEpoch [143/200], Loss: 0.0002, Accuracy: 97.25%\nEpoch [144/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [145/200], Loss: 0.0001, Accuracy: 97.25%\nEpoch [146/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [147/200], Loss: 0.0001, Accuracy: 97.25%\nEpoch [148/200], Loss: 0.0001, Accuracy: 97.25%\nEpoch [149/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [150/200], Loss: 0.0001, Accuracy: 97.25%\nEpoch [151/200], Loss: 0.0001, Accuracy: 97.25%\nEpoch [152/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [153/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [154/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [155/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [156/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [157/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [158/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [159/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [160/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [161/200], Loss: 0.0001, Accuracy: 97.25%\nEpoch [162/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [163/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [164/200], Loss: 0.0001, Accuracy: 97.25%\nEpoch [165/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [166/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [167/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [168/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [169/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [170/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [171/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [172/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [173/200], Loss: 0.0001, Accuracy: 97.25%\nEpoch [174/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [175/200], Loss: 0.0001, Accuracy: 97.25%\nEpoch [176/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [177/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [178/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [179/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [180/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [181/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [182/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [183/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [184/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [185/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [186/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [187/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [188/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [189/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [190/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [191/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [192/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [193/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [194/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [195/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [196/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [197/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [198/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [199/200], Loss: 0.0001, Accuracy: 97.50%\nEpoch [200/200], Loss: 0.0001, Accuracy: 97.50%\n\n\n\n\n\n\n\n\n\n\n\n4.0.11 Prediction\n\ntorch.save(model.state_dict(), 'model.pth')\nprint('Model saved')\n\n# 모델 불러오기\nmodel = DNA_CNN()\nmodel.load_state_dict(torch.load('model.pth'))\nmodel.eval()\n\n# 새로운 데이터에 대한 예측\nnew_data = torch.tensor(X_test[:5], dtype=torch.float32)\nif torch.cuda.is_available():\n    new_data = new_data.cuda()\n\nwith torch.no_grad():\n    outputs = model(new_data)\n    _, predicted = torch.max(outputs.data, 1)\n    \nprint(predicted)\nprint(y_test[:5])\n\n\n    \n\nModel saved\n\n\n/tmp/ipykernel_617536/4019452672.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('model.pth'))\n/tmp/ipykernel_617536/4019452672.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  new_data = torch.tensor(X_test[:5], dtype=torch.float32)\n\n\nRuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Day3 Convolution Neural Networks with DNA sequence</span>"
    ]
  }
]