[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "KAIST Deep Learning",
    "section": "",
    "text": "1 KAIST EB502 programming",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "KAIST Deep Learning",
    "section": "1.1 Welcome",
    "text": "1.1 Welcome\n\n2024.11 카이스트, 공학생물학대학원 프로그래밍 강의 노트\nHaseong Kim (at KRIBB)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#환경",
    "href": "index.html#환경",
    "title": "KAIST Deep Learning",
    "section": "1.2 환경",
    "text": "1.2 환경\n\n실습 환경은 colab을 활용하며 파일 저장 등은 구글 드라이브를 활용함",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "day1_optimization.html",
    "href": "day1_optimization.html",
    "title": "2  Day1 Optimization",
    "section": "",
    "text": "2.1 Introduction of Google Colab",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Day1 Optimization</span>"
    ]
  },
  {
    "objectID": "day1_optimization.html#introduction-of-google-colab",
    "href": "day1_optimization.html#introduction-of-google-colab",
    "title": "2  Day1 Optimization",
    "section": "",
    "text": "2.1.0.1 Access Google Colab\n\nGo to Google Colab in your web browser.\nSign in with your Google account.\n\n\n\n2.1.0.2 Create a New Notebook\n\nClick on File -&gt; New Notebook to create a new notebook.\n\n\n\n2.1.0.3 Install Required Libraries\nGoogle Colab comes with many libraries pre-installed, but you might need to install some additional ones, such as biopython and scikit-bio. You can do this using the !pip install command directly in a cell.\n\n!pip install biopython scikit-bio matplotlib\n\n\n!pip install scikit-bio\n\n\n\n2.1.0.4 Import Libraries and Verify Installation\nIn a new code cell, import the libraries to ensure they are installed correctly.\n\n# Importing necessary libraries\nimport Bio\nimport skbio\n\nprint(\"Biopython version:\", Bio.__version__)\nprint(\"scikit-bio version:\", skbio.__version__)\n\nBiopython version: 1.84\nscikit-bio version: 0.6.2\n\n\n\n\n2.1.0.5 Upload Files to Colab\n\nCreate 2024-kaist-lecture folder\nipynb file open with colab\nDownload ganbank files from ncbi and upload the files\ncurrent directory\n\n\n!pwd\n\n/home/haseong/lecture/kaist-deeplearning-2024\n\n\n\n현재 작업 디렉토리를 위 생성한 디렉토리로 변경\n\n\nimport os\nos.chdir('drive/MyDrive/2024-kaist-lecture')\n\n\n!pwd\n\n/content/drive/MyDrive/2024-kaist-lecture\n\n\n\n분석을 위한 genbank 등의 파일을 ncbi에서 다운로드 후 위 폴더에 복사\n또는 아래 코드를 이용해서 현재 작업 디렉토리에 업로드\n\n\nfrom google.colab import files\n\nuploaded = files.upload()\n\n# Listing the uploaded files\nfor filename in uploaded.keys():\n    print(filename)\n\n\n     \n     \n      Upload widget is only available when the cell has been executed in the\n      current browser session. Please rerun this cell to enable.\n      \n       \n\n\nSaving nn.png to nn.png\nnn.png\n\n\n\n\n\nimage.png",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Day1 Optimization</span>"
    ]
  },
  {
    "objectID": "day1_optimization.html#numpy",
    "href": "day1_optimization.html#numpy",
    "title": "2  Day1 Optimization",
    "section": "2.2 NumPy",
    "text": "2.2 NumPy\nNumPy is a powerful library for numerical operations and handling arrays.\n\n2.2.1 Basics of NumPy\nInstallation:\n!pip install numpy\n\nimport numpy as np\n\n\n# Creating a 1D array\narr1 = np.array([1, 2, 3, 4, 5])\n\n# Creating a 2D array\narr2 = np.array([[1, 2, 3], [4, 5, 6]])\n\nprint(arr1)\nprint(arr2)\n\n\n# Element-wise operations\narr3 = arr1 * 2\nprint(arr3)\n\n# Mathematical functions\nprint(np.sqrt(arr1))\n\n[1 2 3 4 5]\n[[1 2 3]\n [4 5 6]]\n[ 2  4  6  8 10]\n[1.         1.41421356 1.73205081 2.         2.23606798]\n\n\n\n2.2.1.1 Numpy datatype ndarray\n\n행렬이나 다차원 배열 처리용 파이썬 라이브러리\n같은 타입의 데이터만 허용\n리스트에 비해 20배 이상 빠른 속도\n\n\nimport numpy as np\n\ndisplay(np.ones(4))\ndisplay(np.ones((3, 4)))\ndisplay(np.ones((2, 3, 4)))\n\narray([1., 1., 1., 1.])\n\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.]])\n\n\narray([[[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]],\n\n       [[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]]])\n\n\n\n\n\nalt text\n\n\n\nCreate numpy objects\n\n\nimport numpy as np\n\narr = [1, 2, 3]\nprint(arr)\nprint(type(arr))\n\na = np.array([1,2,3])\nprint(a)\nprint(a.dtype)\nprint(a.shape)\nprint(type(a))\n\n[1, 2, 3]\n&lt;class 'list'&gt;\n[1 2 3]\nint64\n(3,)\n&lt;class 'numpy.ndarray'&gt;\n\n\n\narr2 = np.array([[1,2,3], [4,5,6]])\nprint(arr2)\nprint(type(arr2))\nprint(arr2.shape)\nprint(arr2.dtype)\n\n[[1 2 3]\n [4 5 6]]\n&lt;class 'numpy.ndarray'&gt;\n(2, 3)\nint64\n\n\n\nnumpy 자료형\n\n부호가 있는 정수 int(8, 16, 32, 64)\n부호가 없는 정수 uint(8 ,16, 32, 54)\n실수 float(16, 32, 64, 128)\n복소수 complex(64, 128, 256)\n불리언 bool\n문자열 string_\n파이썬 오프젝트 object\n유니코드 unicode_\n\nnp.zeros(), np.ones(), np.arange()\n행렬 연산 지원\n\n\na = np.arange(1, 10).reshape(3,3) # [1, 10)\nprint(a)\na = np.ones((3,4), dtype=np.int16)\nb = np.ones((3,4), dtype=np.int16)\nprint(a)\nprint(b)\nprint(a+b)\nprint(a-b)\n\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\n[[1 1 1 1]\n [1 1 1 1]\n [1 1 1 1]]\n[[1 1 1 1]\n [1 1 1 1]\n [1 1 1 1]]\n[[2 2 2 2]\n [2 2 2 2]\n [2 2 2 2]]\n[[0 0 0 0]\n [0 0 0 0]\n [0 0 0 0]]\n\n\n\nnumpy 함수\n\nnp.sqrt()\nnp.log()\nnp.square()\nnp.log()\nnp.ceil()\nnp.floor()\nnp.isnan()\nnp.sum()\nnp.mean()\nnp.std()\nnp.min()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Day1 Optimization</span>"
    ]
  },
  {
    "objectID": "day1_optimization.html#simple-linear-regression",
    "href": "day1_optimization.html#simple-linear-regression",
    "title": "2  Day1 Optimization",
    "section": "2.3 Simple linear regression",
    "text": "2.3 Simple linear regression\n\nModel\n\\[\ny_i = b_0 + b_1 x_i  + \\epsilon_i \\text{ where } \\epsilon_i \\sim \\text{ iid } N(0, \\sigma^2)\n\\]\nparameters\n\n\\[\n\\theta = \\{ b_0, b_1 \\}\n\\]\n\nFind \\(\\theta\\) that minimize residuals\n\n\\[\n\\sum_{i=i}^n r_i^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2  \\\\\n\\sum_{i=1}^n (y_i - \\hat{b_1}x_i - \\hat{b_0})^2\n\\]\n\nresiduals: difference between sample observed and estimated values\n\n\nimport numpy as np\n\nnp.random.seed(123)\nX = 2 * np.random.rand(20, 1)\nY = 4 + X*0.8 + np.random.rand(20, 1)\n\nX_b = np.c_[np.ones(len(X)), X]\n# print(X)\ntheta_best = np.linalg.inv(X_b.T @ X_b) @ (X_b.T) @ Y\nY_pred_org = X_b @ theta_best\n# print(theta_best)\n\nX_new = 2 * np.random.rand(100, 1)\nX_new_b = np.c_[np.ones(len(X_new)), X_new]\nY_pred = X_new_b @ theta_best\n\nimport matplotlib.pyplot as plt\nplt.scatter(X, Y, color=\"#000000\")\nplt.plot(X_new, Y_pred, color='#cccccc', label='Predictions')\n\n# Plot residuals\nfor i in range(len(Y)):\n    plt.vlines(x=X[i], ymin=min(Y[i], Y_pred_org[i]), ymax=max(Y[i], Y_pred_org[i]), color='green', linestyle='dotted')\n\n\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.plot()\nplt.show()\n\n\n\n\n\n\n\n\n\n2.3.1 Ordinary least sequare (OLS)\n\nModel\n\n\\[\ny_i = b_0 + b_1 x_i  + \\epsilon_i \\text{ where } \\epsilon_i \\sim \\text{ iid } N(0, \\sigma^2), i = 1, 2, ..., n\n\\]\n\\[\nY = X \\beta + \\epsilon\n\\]\n\\[\\begin{equation}\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n... \\\\\ny_n\n\\end{bmatrix}\n\n=\n\n\\begin{pmatrix}\n1 \\ \\ x_1 \\\\\n1 \\ \\ x_2 \\\\\n... \\\\\n1 \\ \\ x_n\n\\end{pmatrix}\n\n\\begin{pmatrix}\nb_0 \\\\\nb_1\n\\end{pmatrix}\n\n+\n\n\n\\begin{pmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n... \\\\\n\\epsilon_n\n\\end{pmatrix}\n\\end{equation}\\]\n\\[\n\\mathbf{\\epsilon} = Y - X\\beta\n\\]\n\nResidual Sum of Squares (RSS)\n\n\\[\nRSS = (Y-X\\beta)^T(Y-X\\beta)\n\\]\n\nTake the gradient with respect to \\(\\beta\\) and set it to zero (Normal equation)\n\n\\[\n\\frac{\\partial RSS}{\\partial \\beta} = -2 X^T Y + 2 X^T X \\beta = 0\n\\]\n\nSolve for \\(\\beta\\) if \\((X^TX)^{-1}\\) exists\n\n\\[\n(X^T X) \\beta = X^T Y\n\\]\n\\[\n\\hat{\\beta} = (X^T X)^{-1} X^T Y\n\\]\n\n\n2.3.2 Maximum Likelihood Estimation (MLE)\n\nDetails: https://statproofbook.github.io/P/slr-mle.html\nConsider the regression as a joint probability model\nReasons to use\n\nThis framework is applicable to other complex models (non-linear, neural network)\n\nBayes rule where \\(D\\) is data, \\(\\theta\\) is parameter\n\n\\[\np(\\theta | D) = \\frac{p(D|\\theta) p(\\theta)}{p(D)}\n\\]\n\\[\n\\text{where $p(\\theta|D)$, $p(D|\\theta)$, $p(\\theta)$ are posterior, likelihood and prior, respectively}\n\\]\n\\[\np(\\theta | D) \\propto p(D|\\theta)\n\\]\n\nRegarding the likelihood, \\(p(Y|X, \\theta)\\) is interpreted as how the behaviour of the response \\(Y\\) is conditional on the values of the feature, \\(X\\), and parameters, \\(\\theta\\)\n\n\\[\n\\begin{align}\np(Y | X, \\theta)  = \\prod_{i=1}^n p( y_i | x_i, \\hat{\\theta})\n\\end{align}\n\\]\n\nThen, we can ask what is the probability of seeing the data, given a specific set of parameters? (== How the data likely to be observed given the parameters == which parameters maximize the likelihood)\n\n\\[\n\\hat{\\theta} = \\text{argmax}_\\theta \\text{ log } \\sum_{i=1}^n p( y_i | x_i, \\theta)\n\\]\n\nFor \\(p(y_i | x_i, \\theta)\\), we have assumption that all feature vectors are iid\n\n\\[\nY \\sim N(X\\beta, \\sigma)\n\\]\n\\[\n\\begin{align}\np( y_i | x_i, \\theta) &= N(y_i; X\\beta, \\sigma^2) \\\\\n&= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp{\\left( - \\frac{(y_i - b_0 - b_1 x_i)^2}{2 \\sigma^2} \\right)}\n\\end{align}\n\\]\n\nLog likelihood (LL) function\n\n\\[\n\\begin{align}\nLL(\\theta) &= \\text{ log } \\left( \\prod_{i=1}^n p( y_i | x_i, \\theta) \\right) \\\\\n&= \\text{ log } \\left( \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp{ \\left(- \\frac{(y_i - b_0 - b_1 x_i)^2}{2 \\sigma^2} \\right)} \\right) \\\\\n&= \\text{ log } \\left( \\frac{1}{\\sqrt{(2\\pi\\sigma^2)^n}} \\exp{ \\left( - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - b_0 - b_1 x_i)^2 \\right)} \\right) \\\\\n&= - \\frac{n}{2} \\log(2\\pi) - \\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - b_0 - b_1 x_i)^2\n\\end{align}\n\\]\n\nTake the gradient with respect to \\(\\beta\\) and set it to zero (OLS)\n\n\\[\n\\frac{\\partial LL(b_0, b_1, \\sigma^2)}{\\partial b_0} = \\frac{1}{\\sigma^2} \\sum_{i=1}^n (y_i - b_0 - b_1 x_i)\n\\]\n\\[\n\\begin{align}\n\\frac{\\partial LL(\\hat{b}_0, \\hat{b}_1, \\sigma^2)}{\\partial b_0}  = 0 \\\\\n\\sum_{i=1}^n (y_i - \\hat{b}_0 - \\hat{b}_1 x_i) = 0 \\\\\n\\hat{b}_0 = \\frac{1}{n}\\sum_{i=1}^n y_i - \\hat{b}_1 \\frac{1}{n} \\sum_{i=1}^n x_i \\\\\n\\end{align}\n\\]\n\\[\n\\frac{\\partial LL(\\hat{b}_0, b_1, \\sigma^2)}{\\partial b_1} = \\frac{1}{\\sigma^2} \\sum_{i=1}^n (x_i y_i - \\hat{b}_0 x_i - b_1 x_i^2)\n\\]\n\\[\n\\begin{align}\n\\frac{\\partial LL(\\hat{b}_0, \\hat{b}_1, \\sigma^2)}{\\partial b_1}  = 0 \\\\\n\\hat{b}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2 }\n\\end{align}\n\\]\n\nMaximize with respect to \\(\\sigma^2\\)\n\n\\[\n\\frac{\\partial LL(\\hat{b}_0, \\hat{b}_1, \\sigma^2)}{\\partial \\sigma^2} = - \\frac{n}{2 \\sigma^2} + \\frac{1}{2 (\\sigma^2)^2} \\sum_{i=1}^n (y_i - \\hat{b}_0 - \\hat{b}_1 x_i)^2\n\\]\n\\[\n\\frac{\\partial LL(\\hat{b}_0, \\hat{b}_1, \\hat{\\sigma}^2)}{\\partial \\sigma^2} = 0 \\\\\n\\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{b}_0 - \\hat{b}_1 x_i)^2\n\\]\n\nIn linear regression, MLE naturally leads to the OLS solution under the assumption of normally distributed residuals.\n\n\\[\n\\hat{\\beta} = (X^T X)^{-1} X^T Y\n\\]\n\\[\n\\hat{\\sigma}^2 = \\frac{1}{n} (Y-X\\beta)^T (Y-X\\beta)\n\\]\n\nHowever, MLE’s flexibility (e.g. customizable likelihood) extends beyond linear models, making it indispensable for logistic regression, mixture models, and modern deep learning frameworks.\n\n\n\n2.3.3 Gradient Decent\n\nAn iterative optimization algorithm for adjusting \\(\\beta\\) by minimizing a cost function. In linear regression, the cost function is the Mean Squared Error (MSE) under the assumption of normally distributed residuals.\nDefine a cost function \\(J(\\theta)\\)\n\n\\[\nLL(\\theta) = - \\frac{n}{2} \\log(2\\pi) - \\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - b_0 - b_1 x_i)^2\n\\]\n\\[\nJ(\\beta) =  \\sum_{i=1}^n (y_i - b_0 - b_1 x_i)^2\n\\]\n\nmatrix notation\n\n\\[\nJ(\\beta) = || Y - X\\beta ||^2 = (Y - X\\beta)^T(Y - X\\beta)\n\\]\n\nL1 norm, L2 norm (a metric for length or magnitude of a vector/matrix) \\[\n||X||_1 = \\sum_{i}^n |x_i|\n\\]\n\n\\[\n||X||_2 = \\sqrt{\\sum_i^n x_i^2}\n\\]\n\\[\nJ(\\beta) = Y^T Y - 2 Y^T X \\beta + \\beta^T X^T X \\beta\n\\]\n\nGradient of the cost function\n\n\\[\n\\nabla_\\beta J(\\beta) = \\frac{\\partial J(\\beta)}{\\partial \\beta}\n\\]\n\\[\n\\begin{align}\n\\nabla_\\beta J(\\beta) &= 0 - 2 X^T Y + 2 X^T X \\beta \\\\\n&= - 2 X^T(Y-X\\beta)\n\\end{align}\n\\]\n\nparameter update rule\n\n\\[\n\\beta^{(t+1)} = \\beta^{(t)} - \\alpha \\nabla_\\beta J(\\beta) \\text{ where } \\alpha \\text{ is learning rate}\n\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate synthetic data\nnp.random.seed(42)\nX = 2 * np.random.rand(100, 1)  # 100 samples, 1 feature from [0, 1) uniform distribution\ny = 4 + 3 * X + np.random.randn(100, 1)  # y = 4 + 3X + noise (Gaussian noise)\n\n# Add bias term (intercept)\nX_b = np.c_[np.ones((X.shape[0], 1)), X]\n\n# Initialize parameters\nbeta = np.random.randn(2, 1)  # Random initial coefficients\nlearning_rate = 0.01\nn_iterations = 100\nm = X_b.shape[0]  # Number of samples\n\nbeta_updates = [beta.copy()]\n\n# Gradient Descent\nfor iteration in range(n_iterations):\n    gradients = -2/m * X_b.T @ (y - X_b @ beta)  # Compute gradient\n    beta = beta - learning_rate * gradients  # Update parameters\n    beta_updates.append(beta.copy())\n\n# Final parameters\nprint(\"Estimated coefficients (beta):\", beta)\n\n# Predictions\ny_pred = X_b @ beta\n\n# Plot the data and regression line\nplt.figure(figsize=(8, 6))\nplt.scatter(X, y, color=\"blue\", label=\"Data points\")\nplt.plot(X, y_pred, color=\"red\", label=\"Regression line\")\nplt.xlabel(\"X\")\nplt.ylabel(\"y\")\nplt.title(\"Linear Regression using Gradient Descent\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\nEstimated coefficients (beta): [[2.96262018]\n [3.80192916]]\n\n\n\n\n\n\n\n\n\n\n# Visualize beta updates\nfor i, beta in enumerate(beta_updates):\n    print(f\"Iteration {i}: beta = {beta.flatten()}\")\n\n# Plot convergence of coefficients\nbeta_updates = np.array(beta_updates).squeeze()\n\nplt.figure(figsize=(8, 6))\nplt.plot(range(n_iterations + 1), beta_updates[:, 0], label='Intercept (beta[0])')\nplt.plot(range(n_iterations + 1), beta_updates[:, 1], label='Slope (beta[1])')\nplt.xlabel('Iteration')\nplt.ylabel('Value of beta')\nplt.title('Convergence of Coefficients')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nIteration 0: beta = [0.01300189 1.45353408]\nIteration 1: beta = [0.12180499 1.56507648]\nIteration 2: beta = [0.22633422 1.67181808]\nIteration 3: beta = [0.32676535 1.77395782]\nIteration 4: beta = [0.42326689 1.8716864 ]\nIteration 5: beta = [0.5160004  1.96518667]\nIteration 6: beta = [0.60512075 2.05463391]\nIteration 7: beta = [0.69077645 2.14019616]\nIteration 8: beta = [0.77310984 2.22203453]\nIteration 9: beta = [0.85225741 2.30030345]\nIteration 10: beta = [0.92835001 2.37515099]\nIteration 11: beta = [1.00151308 2.44671909]\nIteration 12: beta = [1.0718669  2.51514384]\nIteration 13: beta = [1.13952675 2.58055569]\nIteration 14: beta = [1.20460319 2.64307972]\nIteration 15: beta = [1.26720221 2.70283582]\nIteration 16: beta = [1.32742539 2.75993895]\nIteration 17: beta = [1.38537016 2.81449929]\nIteration 18: beta = [1.4411299 2.8666225]\nIteration 19: beta = [1.49479416 2.91640985]\nIteration 20: beta = [1.54644877 2.96395843]\nIteration 21: beta = [1.59617603 3.00936133]\nIteration 22: beta = [1.64405484 3.05270779]\nIteration 23: beta = [1.69016085 3.09408334]\nIteration 24: beta = [1.73456657 3.13357001]\nIteration 25: beta = [1.77734155 3.17124641]\nIteration 26: beta = [1.81855245 3.20718793]\nIteration 27: beta = [1.85826316 3.24146681]\nIteration 28: beta = [1.89653497 3.27415234]\nIteration 29: beta = [1.93342661 3.30531092]\nIteration 30: beta = [1.96899442 3.33500621]\nIteration 31: beta = [2.00329239 3.36329925]\nIteration 32: beta = [2.03637228 3.39024855]\nIteration 33: beta = [2.06828373 3.4159102 ]\nIteration 34: beta = [2.09907433 3.44033798]\nIteration 35: beta = [2.1287897  3.46358343]\nIteration 36: beta = [2.15747358 3.48569598]\nIteration 37: beta = [2.1851679 3.506723 ]\nIteration 38: beta = [2.21191288 3.52670991]\nIteration 39: beta = [2.23774706 3.54570024]\nIteration 40: beta = [2.26270741 3.56373574]\nIteration 41: beta = [2.28682934 3.58085643]\nIteration 42: beta = [2.31014685 3.59710066]\nIteration 43: beta = [2.3326925 3.6125052]\nIteration 44: beta = [2.35449751 3.62710531]\nIteration 45: beta = [2.37559184 3.64093478]\nIteration 46: beta = [2.39600419 3.65402601]\nIteration 47: beta = [2.41576208 3.66641005]\nIteration 48: beta = [2.43489191 3.67811668]\nIteration 49: beta = [2.45341896 3.68917444]\nIteration 50: beta = [2.47136752 3.69961069]\nIteration 51: beta = [2.48876082 3.70945165]\nIteration 52: beta = [2.50562118 3.71872248]\nIteration 53: beta = [2.52196997 3.72744726]\nIteration 54: beta = [2.53782769 3.73564912]\nIteration 55: beta = [2.55321401 3.74335019]\nIteration 56: beta = [2.56814776 3.75057171]\nIteration 57: beta = [2.58264703 3.75733403]\nIteration 58: beta = [2.59672912 3.76365667]\nIteration 59: beta = [2.61041067 3.76955833]\nIteration 60: beta = [2.62370759 3.77505693]\nIteration 61: beta = [2.63663515 3.78016967]\nIteration 62: beta = [2.64920801 3.78491302]\nIteration 63: beta = [2.66144021 3.78930278]\nIteration 64: beta = [2.6733452  3.79335407]\nIteration 65: beta = [2.68493589 3.79708142]\nIteration 66: beta = [2.69622467 3.80049874]\nIteration 67: beta = [2.70722341 3.80361935]\nIteration 68: beta = [2.71794348 3.80645605]\nIteration 69: beta = [2.7283958  3.80902108]\nIteration 70: beta = [2.73859083 3.81132618]\nIteration 71: beta = [2.74853861 3.81338263]\nIteration 72: beta = [2.75824876 3.8152012 ]\nIteration 73: beta = [2.7677305  3.81679224]\nIteration 74: beta = [2.77699268 3.81816566]\nIteration 75: beta = [2.78604379 3.81933097]\nIteration 76: beta = [2.79489196 3.82029728]\nIteration 77: beta = [2.803545   3.82107331]\nIteration 78: beta = [2.81201037 3.82166745]\nIteration 79: beta = [2.82029527 3.82208769]\nIteration 80: beta = [2.82840657 3.82234175]\nIteration 81: beta = [2.83635086 3.82243698]\nIteration 82: beta = [2.84413447 3.82238045]\nIteration 83: beta = [2.85176348 3.82217893]\nIteration 84: beta = [2.85924369 3.8218389 ]\nIteration 85: beta = [2.8665807  3.82136659]\nIteration 86: beta = [2.87377985 3.82076795]\nIteration 87: beta = [2.88084627 3.8200487 ]\nIteration 88: beta = [2.8877849  3.81921431]\nIteration 89: beta = [2.89460044 3.81827003]\nIteration 90: beta = [2.90129743 3.81722089]\nIteration 91: beta = [2.90788021 3.8160717 ]\nIteration 92: beta = [2.91435295 3.81482709]\nIteration 93: beta = [2.92071965 3.81349148]\nIteration 94: beta = [2.92698413 3.81206911]\nIteration 95: beta = [2.93315007 3.81056405]\nIteration 96: beta = [2.93922099 3.8089802 ]\nIteration 97: beta = [2.94520029 3.80732128]\nIteration 98: beta = [2.9510912  3.80559087]\nIteration 99: beta = [2.95689684 3.8037924 ]\nIteration 100: beta = [2.96262018 3.80192916]\n\n\n\n\n\n\n\n\n\n\n2.3.3.1 Reasons to use GD instead of MLE, OLS\n\nGradient Descent is favored over MLE or OLS in scenarios involving large-scale data, high-dimensional features, non-linear models, or custom loss functions due to its flexibility, efficiency, and scalability. However, for simple, small-scale problems, OLS or MLE may still be preferred for their directness and precision.\n\n\n\n\n2.3.4 Model fitting\n\nFinding parameters with LSE, MLE, and GD\nGD provides more flexible (non-linear) and scalable (high-dimentional data) way for modeling\nGD procedure\n\nSet random initial parameters\nCompute gradient that reduces cost\nParameter update until conversing",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Day1 Optimization</span>"
    ]
  },
  {
    "objectID": "day2_neural_networks.html",
    "href": "day2_neural_networks.html",
    "title": "3  Day2 Neural Networks",
    "section": "",
    "text": "3.1 Modeling procedure (Testing vs. Prediction)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Day2 Neural Networks</span>"
    ]
  },
  {
    "objectID": "day2_neural_networks.html#modeling-procedure-testing-vs.-prediction",
    "href": "day2_neural_networks.html#modeling-procedure-testing-vs.-prediction",
    "title": "3  Day2 Neural Networks",
    "section": "",
    "text": "Modeling for statistical testing (e.g. t-test)\n\nHypothesis\nModel architecture (e.g. linear regression, classification, …, \\(y=b_0 + b_1x\\))\nData collection\nData preprocessing (e.g. normalization etc.)\nModel fitting (finding \\(b_0\\) and \\(b_1\\))\nFind the best model by repeating 2 and 5 (not including 3 and 4)\nHypothesis evaluation\n\nModeling for prediction (deep learning model)\n\nObjectives\nModel architecture (regression, classification, cnn, lstm, transformer, embeding models)\nData collection\nData preprocessing (e.g. encoding and embeding etc.)\nModel fitting (forward, backward)\nFind the best model by repeating 2, 4, 5\nDeploy (Use for prediction)\nMonitoring",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Day2 Neural Networks</span>"
    ]
  },
  {
    "objectID": "day2_neural_networks.html#multiple-regression",
    "href": "day2_neural_networks.html#multiple-regression",
    "title": "3  Day2 Neural Networks",
    "section": "3.2 Multiple regression",
    "text": "3.2 Multiple regression\n\nMultiple Regression is a type of supervised learning where the model assumes a linear relationship between the input features \\(X\\) and the target \\(y\\): \\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p + \\epsilon\n\\]\nWhere:\n\n\\(X = [x_1, x_2, \\ldots, x_p]\\) are the input features (gene \\(x_i\\) expresion value, \\(y\\) is target gene expression value)\n\\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) are the model coefficients (parameters).\n\\(\\epsilon\\) is the error term.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Day2 Neural Networks</span>"
    ]
  },
  {
    "objectID": "day2_neural_networks.html#neural-networks",
    "href": "day2_neural_networks.html#neural-networks",
    "title": "3  Day2 Neural Networks",
    "section": "3.3 Neural networks",
    "text": "3.3 Neural networks\n\nA neural network with no hidden layers and a single output node is essentially a multiple regression model \\[\ny = \\sigma(W \\cdot X + b)\n\\]\n\nWhere:\n\n\\(W\\): Weight vector (equivalent to () in regression).\n\\(b\\): Bias term (equivalent to (_0) in regression).\n\\(\\sigma\\): activation function\nMultiple regression can be seen as a very simple neural network\n\n\n\n3.3.1 Key Differences between linear regression and neural networks\n\nLinear vs. non-linear: Neural Networks with hidden layers and non-linear activation functions (e.g., ReLU, sigmoid), they can model highly complex, non-linear relationships.\nInteractions: Multiple Regression requires manual engineering of interaction terms (e.g., \\(x_1 \\times x_2\\). But Neural Networks automatically learn interactions between features through layers of non-linear transformations.\nScale: linear regression is available with low-dimensional features while NN handle high-dimensional inputs, such as images or text.\n\n\n\n3.3.2 Neural Networks Extension\nBy introducing hidden layers and non-linear activation functions, neural networks extend regression to capture non-linear relationships: \\[\ny = \\sigma(W_2 \\cdot \\sigma(W_1 \\cdot X + b_1) + b_2)\n\\] where: - \\(W_1\\) and \\(W_2\\): Weight matrices for hidden and output layers. - \\(b_1\\) and \\(b_2\\): Bias terms. - \\(\\sigma\\): Non-linear activation function (e.g., ReLU, sigmoid).\n\nDeeper networks with multiple layers allow for hierarchical feature learning, capturing increasingly abstract patterns in data.\n\n\n\n\n3.3.3 Architecture of neural networks\n\nNeurons (Nodes): Basic units of a neural network that take inputs, perform a computation, and pass outputs\nLayers:\n\nInput Layer: The starting point where data is fed into the network\nHidden Layers: Intermediate layers between the input and output\nOutput Layer: The final layer that produces the result (classification, regression)\n\n\n\n\n3.3.4 Weights and Biases:\n\nWeights: Parameters that determine the strength. Adjusted during training\nBiases: Additional parameters added to the input. It allows more flexibility\n\n\n\n\n3.3.5 Activation Functions\n\nActivation functions introduce non-linearity to the model, enabling it to learn complex patterns.\nCommon activation functions include:\n\nReLU (Rectified Linear Unit): $ f(x) = (0, x) $\nSigmoid: $ f(x) = $\nTanh: $ f(x) = $\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate synthetic data\nnp.random.seed(42)\nX = np.linspace(-10, 10, 100).reshape(-1, 1)  # Input: 100 points from -10 to 10\nweights = np.random.randn(1, 1)  # Random weights\nbias = np.random.randn(1)  # Random bias\n\n# Linear transformation\nlinear_output = X @ weights + bias\n\n# Activation functions\ndef identity(x):\n    return x  # Linear (no activation)\n\ndef relu(x):\n    return np.maximum(0, x)  # ReLU\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))  # Sigmoid\n\n# Apply activation functions\noutput_identity = identity(linear_output)\noutput_relu = relu(linear_output)\noutput_sigmoid = sigmoid(linear_output)\n\n# Plot results\nplt.figure(figsize=(12, 6))\n\n# Plot the input-output relationship for each activation function\nplt.subplot(1, 3, 1)\nplt.plot(X, output_identity, label=\"Linear Activation (Identity)\", color=\"blue\")\nplt.title(\"Linear Activation\")\nplt.xlabel(\"Input\")\nplt.ylabel(\"Output\")\nplt.grid(True)\n\nplt.subplot(1, 3, 2)\nplt.plot(X, output_relu, label=\"ReLU Activation\", color=\"red\")\nplt.title(\"ReLU Activation\")\nplt.xlabel(\"Input\")\nplt.ylabel(\"Output\")\nplt.grid(True)\n\nplt.subplot(1, 3, 3)\nplt.plot(X, output_sigmoid, label=\"Sigmoid Activation\", color=\"green\")\nplt.title(\"Sigmoid Activation\")\nplt.xlabel(\"Input\")\nplt.ylabel(\"Output\")\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3.3.6 Forward Pass\n\nThe process of passing input data through the network, layer by layer, to produce an output.\nEach neuron computes a weighted sum of its inputs, adds a bias, and passes the result through an activation function.\nThis involves applying a linear transformation.\n\n\n3.3.6.1 (Example) of Forward Pass and Loss Function\nLet’s consider a simple neural network with: - 1 Input Layer: 2 input nodes. - 1 Hidden Layer: 3 nodes with ReLU (Rectified Linear Unit) activation. - 1 Output Layer: 1 node with sigmoid activation.\n\nThe forward pass computes: 1. Input to Hidden Layer: \\[\n   Z^{(1)} = X W^{(1)} + b^{(1)} \\\\\n\\]\n\\[\\begin{equation}\n\\begin{pmatrix}\nz^{(1)}_1 \\\\\nz^{(1)}_2 \\\\\nz^{(1)}_3\n\\end{pmatrix}\n\n=\n\n\\begin{pmatrix}\nx_1 \\ x_2\n\\end{pmatrix}\n\n\\begin{pmatrix}\nw^{(1)}_{11} \\ w^{(1)}_{12} \\ w^{(1)}_{1n} \\\\\nw^{(1)}_{21} \\ w^{(1)}_{21} \\  w^{(1)}_{2n}\n\\end{pmatrix}\n\n\n+\n\n\\begin{pmatrix}\nb^{(1)}_1 \\\\\nb^{(1)}_2 \\\\\nb^{(1)}_3\n\\end{pmatrix}\n\n\\end{equation}\\]\n\\[\n   A^{(1)} = \\text{ReLU}(Z^{(1)})\n\\]\n\\[\\begin{equation}\n\\begin{pmatrix}\na^{(1)}_1 \\\\\na^{(1)}_2 \\\\\na^{(1)}_3\n\\end{pmatrix}\n\n=\n\n\\begin{pmatrix}\n\\text{Relu}(z^{(1)}_1) \\\\\n\\text{Relu}(z^{(1)}_2) \\\\\n\\text{Relu}(z^{(1)}_3)\n\\end{pmatrix}\n\n\\end{equation}\\]\n\nHidden to Output Layer: \\[\nZ^{(2)} = A^{(1)} W^{(2)} + b^{(2)} \\\\\n\\]\n\n\\[\\begin{equation}\n\\begin{pmatrix}\nz^{(2)}_1\n\\end{pmatrix}\n\n=\n\n\\begin{pmatrix}\na^{(1)}_1 \\ a^{(1)}_2 \\ a^{(1)}_3\n\\end{pmatrix}\n\n\\begin{pmatrix}\nw^{(2)}_{11} \\\\\nw^{(2)}_{21} \\\\\nw^{(2)}_{31}\n\\end{pmatrix}\n\n+\n\n\\begin{pmatrix}\nb^{(2)}_1\n\\end{pmatrix}\n\\end{equation}\\]\n\\[\nA^{(2)} = \\text{Sigmoid}(Z^{(2)})\n\\]\n\\[\\begin{equation}\n\\begin{pmatrix}\na^{(2)}_1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\text{Sigmoid} (z^{(2)}_1 )\n\\end{pmatrix}\n\\end{equation}\\]\n\\[\n\\hat{y} = a^{(2)}_1\n\\]\n\nimport numpy as np\n\n# Activation functions\ndef relu(x):\n    return np.maximum(0, x)\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nnp.random.seed(42)\n# Input data (2 features for each samplen)\nX = np.array([[0.5, 0.2], [0.1, 0.4], [0.6, 0.9]])  # Shape (3 samples, 2 features)\ny = np.array([[1], [0], [1]]) \n\nprint(\"X:\\n\", X)\nprint(\"y:\\n\", y)\n\n# Weights and biases for the hidden layer\nW1 = np.random.randn(2, 3)  # Shape (2 input nodes, 3 hidden nodes)\nb1 = np.random.randn(1, 3)  # Shape (1 bias per hidden node)\n\n# Weights and biases for the output layer\nW2 = np.random.randn(3, 1)  # Shape (3 hidden nodes, 1 output node)\nb2 = np.random.randn(1, 1)  # Shape (1 bias for the output node)\n\nprint(\"W1:\\n\", W1)\nprint(\"b1:\\n\", b1)\n\nprint(\"W2:\\n\", W2)\nprint(\"b2:\\n\", b2)\n\n# Forward Pass\n# Step 1: Input to Hidden Layer\nZ1 = X @ W1 + b1  # Linear transformation\nA1 = relu(Z1)     # Activation\n\n# Step 2: Hidden to Output Layer\nZ2 = A1 @ W2 + b2  # Linear transformation\nA2 = sigmoid(Z2)   # Activation (Final output)\ny_hat = A2\n\n# Print results\nprint(\"Hidden Layer Output (after ReLU):\\n\", A1)\nprint(\"Final Output (after Sigmoid):\\n\", A2)\n\nX:\n [[0.5 0.2]\n [0.1 0.4]\n [0.6 0.9]]\ny:\n [[1]\n [0]\n [1]]\nW1:\n [[ 0.49671415 -0.1382643   0.64768854]\n [ 1.52302986 -0.23415337 -0.23413696]]\nb1:\n [[ 1.57921282  0.76743473 -0.46947439]]\nW2:\n [[ 0.54256004]\n [-0.46341769]\n [-0.46572975]]\nb2:\n [[0.24196227]]\nHidden Layer Output (after ReLU):\n [[2.13217586 0.6514719  0.        ]\n [2.23809617 0.65994695 0.        ]\n [3.24796818 0.47373811 0.        ]]\nFinal Output (after Sigmoid):\n [[0.74967732]\n [0.75958995]\n [0.85626904]]\n\n\n\n\n\n3.3.7 Loss Function\n\nA measure of how well the neural network’s predictions match the actual data. Common loss functions include Mean Squared Error (MSE) for regression and Cross-Entropy Loss for classification.\n\n\\[\nL = \\frac{1}{n} || y - \\hat{y} ||^2 =  \\frac{1}{n}\\sum(y_i-\\hat{y}_i)^2\n\\]\n\nnp.mean((y - y_hat)**2)\n\n0.22009897381259744\n\n\n\n\n3.3.8 Backpropagation and Training\n\nBackpropagation: The process of adjusting weights and biases based on the loss. It uses gradients computed by the chain rule to update these parameters through gradient descent or other optimization algorithms.\nOptimization Algorithm: Techniques like Stochastic Gradient Descent (SGD) or Adam adjust the model parameters to minimize the loss.\n\n\n3.3.8.1 Chain Rule\nDerivative of a composite function \\[\nz = f(g(x))\n\\]\n\\[\n\\frac{dz}{dx} = f'(g(x)) \\cdot g'(x)\n\\]\n\\[\ny = g(x) \\\\\nz = f(y)\n\\]\n\\[\n\\frac{dz}{dx} = \\frac{dy}{dx} \\frac{dz}{dy} = f'(y)g'(x)\n\\]\n\n\nSet \\(w_{11}^{(2)}\\) randomaly\nForward pass for loss calculation \\(J(\\theta)\\)\nCompute gradient of loss with respect to \\(w_{11}^{(2)}\\)\n\n\\[\n\\frac{\\partial L}{\\partial w_{11}^{(2)}} =  \\frac{\\partial z^{(2)}}{\\partial w_{11}^{(2)}} \\frac{\\partial y}{\\partial z^{(2)}} \\frac{\\partial L}{\\partial y}\n\\]\n\nUpdate \\(w_{11}^{(2)}\\) and iteratively compute forward pass and backprop until convergence\n\n\nimport numpy as np\n\n# Activation functions and their derivatives\ndef relu(x):\n    return np.maximum(0, x)\n\ndef relu_derivative(x):\n    return (x &gt; 0).astype(float)\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n    return sigmoid(x) * (1 - sigmoid(x))\n\n# Mean Squared Error Loss and its derivative\ndef mse_loss(y_true, y_pred):\n    return np.mean((y_true - y_pred) ** 2)\n\ndef mse_loss_derivative(y_true, y_pred):\n    return -(y_true - y_pred)\n\n# Input data and true labels\nX = np.array([[0.5, 0.2], [0.1, 0.4], [0.6, 0.9]])  # Shape (3 samples, 2 features)\ny = np.array([[1], [0], [1]])                      # True labels (Shape: 3x1)\n\n# Initialize weights and biases\nnp.random.seed(42)\nW1 = np.random.randn(2, 3)  # Weights for input to hidden layer\nb1 = np.random.randn(1, 3)  # Bias for hidden layer\nW2 = np.random.randn(3, 1)  # Weights for hidden to output layer\nb2 = np.random.randn(1, 1)  # Bias for output layer\n\n# Training parameters\nlearning_rate = 0.1\nepochs = 1000\n\n# Training loop\nfor epoch in range(epochs):\n    # Forward Pass\n    Z1 = X @ W1 + b1\n    A1 = relu(Z1)\n    Z2 = A1 @ W2 + b2\n    A2 = sigmoid(Z2)\n\n    # Compute Loss\n    loss = mse_loss(y, A2)\n\n    # Backpropagation\n    # Output layer\n    dA2 = mse_loss_derivative(y, A2)\n    dZ2 = dA2 * sigmoid_derivative(Z2)\n    dW2 = A1.T @ dZ2\n    db2 = np.sum(dZ2, axis=0, keepdims=True)\n\n    # Hidden layer\n    dA1 = dZ2 @ W2.T\n    dZ1 = dA1 * relu_derivative(Z1)\n    dW1 = X.T @ dZ1\n    db1 = np.sum(dZ1, axis=0, keepdims=True)\n\n    # Update weights and biases\n    W2 -= learning_rate * dW2\n    b2 -= learning_rate * db2\n    W1 -= learning_rate * dW1\n    b1 -= learning_rate * db1\n\n    # Print loss every 100 epochs\n    if epoch % 100 == 0:\n        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n\n# Final predictions\nprint(\"\\nFinal Predictions:\")\nZ1 = X @ W1 + b1\nA1 = relu(Z1)\nZ2 = A1 @ W2 + b2\nA2 = sigmoid(Z2)\nprint(A2)\n\nEpoch 0, Loss: 0.2201\nEpoch 100, Loss: 0.1853\nEpoch 200, Loss: 0.1417\nEpoch 300, Loss: 0.0806\nEpoch 400, Loss: 0.0383\nEpoch 500, Loss: 0.0198\nEpoch 600, Loss: 0.0118\nEpoch 700, Loss: 0.0079\nEpoch 800, Loss: 0.0058\nEpoch 900, Loss: 0.0044\n\nFinal Predictions:\n[[0.93258656]\n [0.0744951 ]\n [0.97627875]]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Day2 Neural Networks</span>"
    ]
  },
  {
    "objectID": "day2_neural_networks.html#pytorch",
    "href": "day2_neural_networks.html#pytorch",
    "title": "3  Day2 Neural Networks",
    "section": "3.4 PyTorch",
    "text": "3.4 PyTorch\nPyTorch is an open-source deep learning framework developed by Facebook’s AI Research lab (FAIR). It is widely used in research and industry due to its dynamic computation graph and ease of use.\nPyTorch Ecosystem Overview:\ntorch: The core library for tensor operations and automatic differentiation.\ntorch.nn: A sub-library used to build and train neural network models.\ntorch.optim: Tools for optimization algorithms (e.g., SGD, Adam).\ntorchvision: Provides datasets, pre-trained models, and image transformations.\nTensors\nTensors are the primary data structures in PyTorch, analogous to NumPy arrays but with added capabilities such as the ability to run on GPUs for faster computation.\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n\n\n\n# Input data and true labels\nX = torch.tensor([[0.5, 0.2], [0.1, 0.4], [0.6, 0.9]], dtype=torch.float32)  # Shape (3 samples, 2 features)\ny = torch.tensor([[1], [0], [1]], dtype=torch.float32)                      # True labels (Shape: 3x1)\n\n# Define the neural network\nclass SimpleNN(nn.Module):\n    def __init__(self):\n        super(SimpleNN, self).__init__()\n        # Define layers\n        self.hidden = nn.Linear(2, 3)  # Input to hidden layer (2 inputs, 3 hidden nodes)\n        self.output = nn.Linear(3, 1)  # Hidden to output layer (3 hidden nodes, 1 output)\n\n    def forward(self, x):\n        # Forward pass: Input -&gt; Hidden Layer -&gt; Output Layer\n        x = torch.relu(self.hidden(x))       # ReLU activation for hidden layer\n        x = torch.sigmoid(self.output(x))    # Sigmoid activation for output layer\n        return x\n\n# Initialize the network\nmodel = SimpleNN()\n\n# Loss function and optimizer\ncriterion = nn.MSELoss()  # Mean Squared Error Loss\noptimizer = optim.SGD(model.parameters(), lr=0.1)  # Stochastic Gradient Descent optimizer\n\n# Training parameters\nepochs = 1000\n\n# Containers to store loss and accuracy for each epoch\nloss_history = []\naccuracy_history = []\n\n\n# Training loop\nfor epoch in range(epochs):\n    # Forward pass\n    outputs = model(X)  # Predicted outputs\n    loss = criterion(outputs, y)  # Compute loss\n\n    # Compute accuracy\n    predicted = (outputs &gt;= 0.5).float()  # Threshold at 0.5 for binary classification\n    accuracy = (predicted == y).sum().item() / y.size(0)\n\n\n    # Backpropagation\n    optimizer.zero_grad()  # Clear previous gradients\n    loss.backward()        # Compute gradients\n    optimizer.step()       # Update weights\n\n    # Record loss and accuracy\n    loss_history.append(loss.item())\n    accuracy_history.append(accuracy)\n\n\n    # Print loss every 100 epochs\n    if epoch % 100 == 0:\n        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n\n# Final predictions\nwith torch.no_grad():  # No need to compute gradients during inference\n    final_outputs = model(X)\n    print(\"\\nFinal Predictions:\")\n    print(final_outputs)\n\n\n# Plot the loss and accuracy over epochs\nplt.figure(figsize=(12, 5))\n\n# Loss plot\nplt.subplot(1, 2, 1)\nplt.plot(range(epochs), loss_history, label=\"Loss\", color=\"red\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss over Epochs\")\nplt.grid(True)\nplt.legend()\n\n# Accuracy plot\nplt.subplot(1, 2, 2)\nplt.plot(range(epochs), accuracy_history, label=\"Accuracy\", color=\"blue\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy over Epochs\")\nplt.grid(True)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\nEpoch 0, Loss: 0.2275\nEpoch 100, Loss: 0.2192\nEpoch 200, Loss: 0.2141\nEpoch 300, Loss: 0.2061\nEpoch 400, Loss: 0.1925\nEpoch 500, Loss: 0.1692\nEpoch 600, Loss: 0.1317\nEpoch 700, Loss: 0.0874\nEpoch 800, Loss: 0.0543\nEpoch 900, Loss: 0.0376\n\nFinal Predictions:\ntensor([[0.8810],\n        [0.2476],\n        [0.9134]])",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Day2 Neural Networks</span>"
    ]
  },
  {
    "objectID": "day3_cnn_dna.html",
    "href": "day3_cnn_dna.html",
    "title": "4  Day3 CNN with DNA sequence",
    "section": "",
    "text": "4.1 Objectives",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Day3 CNN with DNA sequence</span>"
    ]
  },
  {
    "objectID": "day3_cnn_dna.html#objectives",
    "href": "day3_cnn_dna.html#objectives",
    "title": "4  Day3 CNN with DNA sequence",
    "section": "",
    "text": "Developing a CNN Model for classifying sequences\nExample: Developing a model to identify specific DNA motifs bound by an arbitrary transcription factor. The model predicts whether a given transcription factor binds to an input DNA sequence (output: 1) or not (output: 0).”",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Day3 CNN with DNA sequence</span>"
    ]
  },
  {
    "objectID": "day3_cnn_dna.html#data",
    "href": "day3_cnn_dna.html#data",
    "title": "4  Day3 CNN with DNA sequence",
    "section": "4.2 Data",
    "text": "4.2 Data\n\nTo train a deep learning model, labeled data is required (though not always necessary in modern self-supervised learning approaches).\nIn sequence analysis, DNA sequences paired with their corresponding phenotypes can serve as labeled data (genotype-phenotype paired data).\n\nFor example, to train a deep learning model to predict DNA sequences bound by a specific transcription factor, you would need a dataset containing transcription factor sequence data along with labels indicating whether or not the transcription factor binds to a given DNA sequence (True or False).\n\nGenerally, data for statistical analysis is represented as a 2D array, with rows corresponding to samples and columns to variables. In deep learning, data is represented in the same way. The number of samples required depends on the complexity of the model, but typically, at least thousands of samples are needed. Using tens of thousands or more samples is recommended for optimal results.\nA dataset collected for deep learning is divided into training and test datasets. Sometimes, the training dataset is further split into training and validation datasets for model development and performance evaluation.\n\n\n4.2.1 One-hot encoding\n\nFor deep learning, data must be represented numerically in a format that machines can process.\nOne-hot encoding is one of the most widely used methods in deep learning.\nFor DNA sequences with four types of nucleotides, encoding can be done as follows:\n\n“A” → [1, 0, 0, 0]\n\n“T” → [0, 0, 0, 1]\n\n“G” → [0, 0, 1, 0]\n\n“C” → [0, 1, 0, 0]\n\n\n\nimport numpy as np\n\nmy_string=\"ATACAA\"\nmy_array=np.array(list(my_string))\nprint(my_array)\n\n['A' 'T' 'A' 'C' 'A' 'A']\n\n\n\nlist(my_string)\n\n['A', 'T', 'A', 'C', 'A', 'A']\n\n\n\nnp.zeros((7,5))\n\narray([[0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.]])\n\n\n\nbox = np.zeros((3, 7, 5))\ntype(box)\nbox\n\narray([[[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]]])\n\n\n\nonehot_encode = np.zeros((len(my_array),4), dtype=int)\nbase_dict = {\"A\":0, \"C\":1, \"G\":2, \"T\":3}\nfor i in range(len(my_array)):\n    onehot_encode[i, base_dict[my_array[i]]] = 1\n\nprint(onehot_encode)\nprint(onehot_encode.shape)\n\n[[1 0 0 0]\n [0 0 0 1]\n [1 0 0 0]\n [0 1 0 0]\n [1 0 0 0]\n [1 0 0 0]]\n(6, 4)\n\n\n\n\n4.2.2 Set sequence motif for simulation\n\nUnderstanding the concepts of PFM (Position Frequency Matrix) and PWM (Position Weight Matrix) is essential.\nAssuming an alignment of several sequences, the PFM represents the frequency of each nucleotide (A, T, G, C) at specific positions in the sequences, while the PWM represents the proportion of each nucleotide at those positions.\n\n\nfrom Bio import motifs\nfrom Bio.Seq import Seq\n\ninstances = [Seq(\"TACAA\"), Seq(\"TACGA\"), Seq(\"TACAA\")]\nm = motifs.create(instances)\npfm = m.counts\nprint(pfm)\npwm = m.counts.normalize(pseudocounts=0.5)\nprint (pwm)\n\n        0      1      2      3      4\nA:   0.00   3.00   0.00   2.00   3.00\nC:   0.00   0.00   3.00   0.00   0.00\nG:   0.00   0.00   0.00   1.00   0.00\nT:   3.00   0.00   0.00   0.00   0.00\n\n        0      1      2      3      4\nA:   0.10   0.70   0.10   0.50   0.70\nC:   0.10   0.10   0.70   0.10   0.10\nG:   0.10   0.10   0.10   0.30   0.10\nT:   0.70   0.10   0.10   0.10   0.10\n\n\n\n\nPseudocounts are used in calculations to avoid division by NULL or zero.\nThe PWM (Position Weight Matrix) of a specific sequence motif can be used to search for the motif’s location in a new sequence provided in one-hot encoding format.\nUsing a sliding window approach, the motif can be scanned from the beginning to the end of the sequence to identify its presence.\nThe following demonstrates how to detect the presence of a given PWM motif (assuming a binary 0 and 1 representation) in a sequence.\n\n\n\n\nalt text\n\n\n\nIf there is a ‘5’ in the length-3 array as shown above, it indicates that the target sequence contains a sequence matching the motif.\nTo search for the presence of the PWM motif in the sequence “ATACAA,” a sliding window of length 5 can be used to divide the sequence into two sub-sequences: “ATACA” and “TACAA.”\n\nBy converting these two sub-sequences into one-hot encoding and multiplying their elements with the corresponding elements of the PWM, only the PWM values at the non-zero positions of the one-hot encoding remain.\nTo quantify how similar a given sequence is to the motif: 1. Multiply all non-zero values from the PWM. 2. Take the logarithm of the result.\nThe resulting scalar value indicates the similarity between the sequence and the motif. Theoretically, a value of 0 implies an identical sequence match to the motif.\n\n\n\npwm_arr = np.array(list(pwm.values())).transpose()\nprint(pwm_arr.shape)\n\nprint(onehot_encode.shape)\nprint(onehot_encode[0:5,].shape)\nprint(onehot_encode[1:6,].shape)\n\ns1 = np.multiply(onehot_encode[0:5,], pwm_arr)\ns2 = np.multiply(onehot_encode[1:6,], pwm_arr)\nprint(s1)\nprint(s2)\n\nprint(np.sum(s1, axis=1))\nprint(np.prod(np.sum(s1, axis=1)))\n\nprint(np.log(np.prod(np.sum(s1, axis=1)))) #s1 score\nprint(np.log(np.prod(np.sum(s2, axis=1)))) #s2 score\n\n(5, 4)\n(6, 4)\n(5, 4)\n(5, 4)\n[[0.1 0.  0.  0. ]\n [0.  0.  0.  0.1]\n [0.1 0.  0.  0. ]\n [0.  0.1 0.  0. ]\n [0.7 0.  0.  0. ]]\n[[0.  0.  0.  0.7]\n [0.7 0.  0.  0. ]\n [0.  0.7 0.  0. ]\n [0.5 0.  0.  0. ]\n [0.7 0.  0.  0. ]]\n[0.1 0.1 0.1 0.1 0.7]\n7.000000000000002e-05\n-9.567015315914915\n-2.119846956314875\n\n\n\nDeep learning styled array\n\n\n\n\nalt text\n\n\n\n\n4.2.3 Simulation data generation\nGenerate 1,000 simulated positive sequences by embedding a motif in the middle of the sequences, and 1,000 negative sequences with random DNA sequences.\n\nimport numpy as np\nseq_length = 20\nnum_sample = 1000\n#motif CCGGAA\nmotif_pwm = np.array([[10.41, 22.86, 1.92, 1.55, 98.60, 86.66],\n            [68.20, 65.25, 0.50, 0.35, 0.25, 2.57],\n            [17.27, 8.30, 94.77, 97.32, 0.87, 0.00],\n            [4.13, 3.59, 2.81, 0.78, 0.28, 10.77]])\npwm = np.hstack([np.ones((4, 7)), motif_pwm, np.ones((4, 7))])\npos = np.array([np.random.choice( ['A', 'C', 'G', 'T'], num_sample,\n                                  p=pwm[:,i]/sum(pwm[:,i])) for i in range(seq_length)]).transpose()\nneg = np.array([np.random.choice( ['A', 'C', 'G', 'T'], num_sample,\n                                  p=np.array([1,1,1,1])/4) for i in range(seq_length)]).transpose()\n\nprint(pos.shape)\ndisplay([''.join(x) for x in pos[1:5,]])\nprint()\ndisplay([''.join(x) for x in neg[1:5,]])\n\n(1000, 20)\n\n\n\n['GGATTAAACGGAAACTATTT',\n 'AAGACTGCCGGATGGGCTCG',\n 'CCCGAAGGCGGAAACAATCT',\n 'ATGGAAGCGGGAAATATTCT']\n\n\n['CTACCCTTACTCGCAGGGAA',\n 'ACTCACTAATTGGATTGAGA',\n 'AGGTACCTCGCGGCATCTGG',\n 'GGTATCTACGTGAAGAAGGG']\n\n\n\n\n4.2.4 DNA data preprocessing\n\nbase_dict = {'A':0, 'C':1, 'G':2, 'T':3}\n\n# response variable for pos\nonehot_encode_pos = np.zeros((num_sample, seq_length, 4))\nonehot_encode_pos_label = np.zeros((num_sample, 2), dtype=int)\nonehot_encode_pos_label[:,0] = 1\n# print(onehot_encode_pos_label)\n\n# response variable for pos\nonehot_encode_neg = np.zeros((num_sample, seq_length, 4))\nonehot_encode_neg_label = np.zeros((num_sample, 2), dtype=int)\nonehot_encode_neg_label[:,1] = 1\n# print(onehot_encode_neg_label)\n\n# convert sequence to onehot\nfor i in range(num_sample):\n    for j in range(seq_length):\n        onehot_encode_pos[i,j,base_dict[pos[i,j]]] = 1\n        onehot_encode_neg[i,j,base_dict[neg[i,j]]] = 1\n\n# concatenation\nX = np.vstack((onehot_encode_pos, onehot_encode_neg))\ny = np.vstack((onehot_encode_pos_label, onehot_encode_neg_label))\n\nprint(X.shape, y.shape)\n# (2000, 20, 4) (2000, 2)\n\n(2000, 20, 4) (2000, 2)\n\n\n\nPyTorch Conv1d requires [batch_size, channels, length] so transpose(1,2) excuted\n\n\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\n# 데이터를 훈련 세트와 테스트 세트로 나눔\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=125)\nprint(X_train.shape, y_train.shape)\n\n# NumPy 배열을 PyTorch 텐서로 변환\nX_train = torch.tensor(X_train, dtype=torch.float32).transpose(1,2)\nX_test = torch.tensor(X_test, dtype=torch.float32).transpose(1,2)\ny_train = torch.tensor(y_train, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.float32)\nprint(y_test.dtype)\n\n# DataLoader 설정\ntrain_dataset = TensorDataset(X_train, y_train)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nprint(train_loader.dataset.tensors[0].shape)\nprint(train_loader.dataset.tensors[1].shape)\ntest_dataset = TensorDataset(X_test, y_test)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n(1600, 20, 4) (1600, 2)\ntorch.float32\ntorch.Size([1600, 4, 20])\ntorch.Size([1600, 2])\n\n\n\nimport torch\n\nX_torch = torch.tensor(X_train, dtype=torch.float32)\nprint(X_torch.shape)\n\ntorch.Size([1600, 4, 20])\n\n\n/tmp/ipykernel_2688/3124571761.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_torch = torch.tensor(X_train, dtype=torch.float32)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Day3 CNN with DNA sequence</span>"
    ]
  },
  {
    "objectID": "day3_cnn_dna.html#cnn-model",
    "href": "day3_cnn_dna.html#cnn-model",
    "title": "4  Day3 CNN with DNA sequence",
    "section": "4.3 CNN model",
    "text": "4.3 CNN model\n\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass DNA_CNN(nn.Module):\n    def __init__(self):\n        super(DNA_CNN, self).__init__()\n        self.conv1 = nn.Conv1d(in_channels=4, out_channels=16, kernel_size=3, padding=1)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool1d(kernel_size=2)\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(160, 64)  # Adjust the input features according to your pooling and conv1d output\n        self.fc2 = nn.Linear(64, 2)  # Adjust according to your problem's needs (e.g., number of classes)\n        #self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.flatten(x)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        #x = self.softmax(x)\n        return x\n\nmodel = DNA_CNN()\nif torch.cuda.is_available():\n    model.cuda()\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nfrom torchsummary import summary\nsummary(model, input_size=(4, 20))  # (Channels, Length)\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv1d-1               [-1, 16, 20]             208\n              ReLU-2               [-1, 16, 20]               0\n         MaxPool1d-3               [-1, 16, 10]               0\n           Flatten-4                  [-1, 160]               0\n            Linear-5                   [-1, 64]          10,304\n            Linear-6                    [-1, 2]             130\n================================================================\nTotal params: 10,642\nTrainable params: 10,642\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.01\nParams size (MB): 0.04\nEstimated Total Size (MB): 0.05\n----------------------------------------------------------------\n\n\n\n4.3.1 Training\n\n# 훈련 루프\nnum_epochs = 20\nfor epoch in range(num_epochs):\n    for inputs, labels in train_loader:\n        if torch.cuda.is_available():\n            inputs, labels = inputs.cuda(), labels.cuda()\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n\nEpoch [1/20], Loss: 0.5050\nEpoch [2/20], Loss: 0.2472\nEpoch [3/20], Loss: 0.1437\nEpoch [4/20], Loss: 0.0821\nEpoch [5/20], Loss: 0.0966\nEpoch [6/20], Loss: 0.0719\nEpoch [7/20], Loss: 0.0729\nEpoch [8/20], Loss: 0.0387\nEpoch [9/20], Loss: 0.1018\nEpoch [10/20], Loss: 0.1520\nEpoch [11/20], Loss: 0.0712\nEpoch [12/20], Loss: 0.0277\nEpoch [13/20], Loss: 0.0272\nEpoch [14/20], Loss: 0.0318\nEpoch [15/20], Loss: 0.0215\nEpoch [16/20], Loss: 0.0464\nEpoch [17/20], Loss: 0.0975\nEpoch [18/20], Loss: 0.0566\nEpoch [19/20], Loss: 0.0191\nEpoch [20/20], Loss: 0.0516\n\n\n\n\n4.3.2 Model evaluation\n\n# 모델 평가\nmodel.eval()\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for inputs, labels in test_loader:\n        if torch.cuda.is_available():\n            inputs, labels = inputs.cuda(), labels.cuda()\n        outputs = model(inputs)\n        #print(outputs.data)\n        _, predicted = torch.max(outputs.data, 1)\n        #print(predicted)\n        total += labels.size(0)\n        labels_max = torch.max(labels, 1)[1]\n        #print(labels_max)\n        correct += (predicted == labels_max).sum().item()\n\n    print(f'Accuracy of the model on the test images: {100 * correct / total} %')\n\nAccuracy of the model on the test images: 98.5 %",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Day3 CNN with DNA sequence</span>"
    ]
  },
  {
    "objectID": "day3_cnn_dna.html#training-and-evaluation",
    "href": "day3_cnn_dna.html#training-and-evaluation",
    "title": "4  Day3 CNN with DNA sequence",
    "section": "4.4 Training and evaluation",
    "text": "4.4 Training and evaluation\n\nimport matplotlib.pyplot as plt\n\n# 데이터 저장을 위한 리스트 초기화\ntrain_losses = []\nval_accuracies = []\n\nnum_epochs = 200\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        if torch.cuda.is_available():\n            inputs, labels = inputs.cuda(), labels.cuda()\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * inputs.size(0)\n\n    epoch_loss = running_loss / len(train_loader.dataset)\n    train_losses.append(epoch_loss)\n\n    # 모델 평가\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            if torch.cuda.is_available():\n                inputs, labels = inputs.cuda(), labels.cuda()\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            labels_max = torch.max(labels, 1)[1]\n            correct += (predicted == labels_max).sum().item()\n\n    epoch_accuracy = 100 * correct / total\n    val_accuracies.append(epoch_accuracy)\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n\n# 그래프 그리기\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label='Training Loss')\nplt.title('Training Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(val_accuracies, label='Validation Accuracy')\nplt.title('Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy (%)')\nplt.legend()\n\nplt.show()\n\nNameError: name 'model' is not defined",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Day3 CNN with DNA sequence</span>"
    ]
  },
  {
    "objectID": "day4_cnn.html",
    "href": "day4_cnn.html",
    "title": "5  Day4 CNN with details",
    "section": "",
    "text": "5.1 Overview\nConvolutional Neural Networks (CNNs) are a type of deep learning model specifically designed for recognizing patterns and spatial hierarchies in data. While traditionally used in image processing, CNNs are also powerful for tasks involving sequential data, such as DNA sequences, because of their ability to detect local patterns.\nhttps://nafizshahriar.medium.com/what-is-convolutional-neural-network-cnn-deep-learning-b3921bdd82d5",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Day4 CNN with details</span>"
    ]
  },
  {
    "objectID": "day4_cnn.html#dimension-3d-4d",
    "href": "day4_cnn.html#dimension-3d-4d",
    "title": "5  Day4 CNN with details",
    "section": "5.2 Dimension (3D, 4D)",
    "text": "5.2 Dimension (3D, 4D)\n\nndarray (numpy)\ntensor (torch)\n\n\n\n\nalt text\n\n\n\n5.2.0.1 Image data\n\nndarray (numpy): Data is stored as (height, width, channels) (e.g., grayscale or RGB)\ntensor (PyTorch): Data is stored as (batch, channels, height, width)\n\n\nimport numpy as np\nimport torch\n\ndisplay(np.ones((2, 3, 4)))\ndisplay(torch.ones((2, 3, 4)))\n\narray([[[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]],\n\n       [[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]]])\n\n\ntensor([[[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]],\n\n        [[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]])\n\n\n\n\n\n5.2.0.2 Tensor\n\nimport torch\n\n# Scalar (0D tensor)\nscalar = torch.tensor(5)\nprint(\"Scalar:\", scalar)\n\n# Vector (1D tensor)\nvector = torch.tensor([1, 2, 3])\nprint(\"Vector:\", vector)\n\n# Matrix (2D tensor)\nmatrix = torch.tensor([[1, 2], [3, 4]])\nprint(\"Matrix:\\n\", matrix)\n\n# 3D Tensor (e.g., RGB image)\ntensor_3d = torch.rand(3, 4, 3)  # Random 3D tensor (height=3, width=4, channels=3)\nprint(\"3D Tensor (RGB image shape):\", tensor_3d.shape)\nprint(tensor_3d)\n\n# 4D Tensor (Batch of images)\ntensor_4d = torch.rand(2, 3, 4, 3)  # Batch size=2\nprint(\"4D Tensor (Batch of RGB images shape):\", tensor_4d.shape)\n\nScalar: tensor(5)\nVector: tensor([1, 2, 3])\nMatrix:\n tensor([[1, 2],\n        [3, 4]])\n3D Tensor (RGB image shape): torch.Size([3, 4, 3])\ntensor([[[0.9165, 0.6617, 0.1589],\n         [0.4045, 0.3932, 0.8812],\n         [0.8592, 0.8760, 0.5807],\n         [0.6333, 0.9934, 0.5808]],\n\n        [[0.7836, 0.8921, 0.0630],\n         [0.9065, 0.6955, 0.4772],\n         [0.8679, 0.4292, 0.9987],\n         [0.7440, 0.8195, 0.5946]],\n\n        [[0.7467, 0.4266, 0.1780],\n         [0.2938, 0.6778, 0.0691],\n         [0.3945, 0.7627, 0.3576],\n         [0.0728, 0.8352, 0.5470]]])\n4D Tensor (Batch of RGB images shape): torch.Size([2, 3, 4, 3])\n\n\n\n\n5.2.0.3 Example\n\nfrom PIL import Image  # For reading and processing images\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\n# Load the image\nimage_path = 'images/3cbe_model-1.jpeg'  # Replace with your image path\nimage = Image.open(image_path)\n\n# Show basic properties\nprint(\"Image Size:\", image.size)  # (width, height)\nprint(\"Image Mode:\", image.mode)  # e.g., \"RGB\"\n\n# Display the image\nplt.imshow(image)\nplt.title(\"3CBE\")\nplt.axis(\"off\")\nplt.show()\n\n\nImage Size: (500, 500)\nImage Mode: RGB\n\n\n\n\n\n\n\n\n\n\n\n5.2.0.4 Conversion\n\n# Convert image to NumPy array\nimage_np = np.array(image)\nprint(\"Image Shape (NumPy):\", image_np.shape)  # e.g., (height, width, channels)\n\nImage Shape (NumPy): (500, 500, 3)\n\n\n\n# Convert NumPy array to PyTorch tensor\nimage_tensor = torch.from_numpy(image_np).permute(2, 0, 1)  # Change to (channels, height, width)\nimage_tensor = image_tensor.float() / 255.0  # Normalize pixel values to [0, 1]\nprint(\"Image Shape (PyTorch Tensor):\", image_tensor.shape)  # e.g., (3, height, width)\n\n# Add batch dimension for model input\nimage_tensor = image_tensor.unsqueeze(0)  # Shape: (1, channels, height, width)\nprint(\"Image Shape with Batch Dimension:\", image_tensor.shape)\n\nImage Shape (PyTorch Tensor): torch.Size([3, 500, 500])\nImage Shape with Batch Dimension: torch.Size([1, 3, 500, 500])\n\n\n\n# Flip the image horizontally\nflipped_tensor = image_tensor.flip(2)  # Flip along the last dimension (width)\n\n# Convert back to NumPy for visualization\nflipped_image = flipped_tensor.squeeze(0).permute(1, 2, 0).numpy()\n\n# Display the flipped image\nplt.imshow(flipped_image)\nplt.title(\"Flipped Image\")\nplt.axis(\"off\")\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Day4 CNN with details</span>"
    ]
  },
  {
    "objectID": "day4_cnn.html#dot-products",
    "href": "day4_cnn.html#dot-products",
    "title": "5  Day4 CNN with details",
    "section": "5.3 Dot products",
    "text": "5.3 Dot products\n\nA fundamental operation in linear algebra that combines two vectors to produce a single scalar value.\nIt measures how aligned two vectors are and has applications in geometry, physics, and machine learning.\nFor two vectors $ = [a_1, a_2, , a_n] $ and $ = [b_1, b_2, , b_n] $ in \\(n\\)-dimensional space, the dot product is defined as:\n\n\\[\n\\mathbf{a} \\cdot \\mathbf{b} = a_1b_1 + a_2b_2 + \\dots + a_nb_n = \\sum_{i=1}^n a_i b_i\n\\]\n\nGeometric Interpretation:\n\nThe dot product measures the projection of one vector onto another.\nIt is related to the angle $ $ between the vectors: \\[\n\\mathbf{a} \\cdot \\mathbf{b} = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\cos\\theta\n\\]\n$ || $ and $ || $ are the magnitudes (lengths) of $ $ and $ $.\n\nOrthogonality:\n\nIf $ = 0 $, the vectors are perpendicular (orthogonal).\n\nSignificance of Value:\n\nPositive dot product: Vectors point in roughly the same direction.\nZero dot product: Vectors are orthogonal (90° apart).\nNegative dot product: Vectors point in opposite directions.\n\n\n\n\n5.3.0.1 Example\n\nSimple Numerical Example\n\nLet $ = [1, 2, 3] $ and $ = [4, 5, 6] \\(. The dot product is:\\)$ = (1)(4) + (2)(5) + (3)(6) = 4 + 10 + 18 = 32 $$\n\nGeometric Example If $ || = 5 $, $ || = 3 $, and the angle between them is $ = 60^\\(, the dot product is:\\)$ = || || = 5 (60^) = 15 = 7.5 $$\nApplications in Machine Learning\n\nSimilarity Measurement: Dot product measures similarity between vectors, such as in cosine similarity.\nConvolutions: Extract features by computing dot products between filters and input regions.\nAttention Mechanisms: Uses dot products to calculate importance weights between query and key vectors.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Day4 CNN with details</span>"
    ]
  },
  {
    "objectID": "day4_cnn.html#dataset-and-dataloader",
    "href": "day4_cnn.html#dataset-and-dataloader",
    "title": "5  Day4 CNN with details",
    "section": "5.4 Dataset and Dataloader",
    "text": "5.4 Dataset and Dataloader\n\n5.4.0.1 Dataset\n\nA class that represents your data, providing a way to access samples and their corresponding labels.\nWe can define a custom dataset by subclassing torch.utils.data.Dataset and overriding:\n\n__len__: Returns the total number of samples.\n__getitem__: Retrieves a single sample (data and label) by index.\n\n\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\nclass SimpleDataset(Dataset):\n    def __init__(self, size):\n        # Generate random x values\n        self.x = np.random.rand(size, 1) * 10  # Shape: (size, 1)\n        self.y = 2 * self.x + 3  # Generate labels (y = 2x + 3)\n\n    def __len__(self):\n        # Total number of samples\n        return len(self.x)\n\n    def __getitem__(self, idx):\n        # Retrieve the sample at index `idx`\n        sample = torch.tensor(self.x[idx], dtype=torch.float32)\n        label = torch.tensor(self.y[idx], dtype=torch.float32)\n        return sample, label\n\n# Create an instance of the dataset\ndataset = SimpleDataset(size=100)\n\n# Access the first sample\nsample, label = dataset[0]\nprint(\"Sample:\", sample, \"Label:\", label)\n\n# Check the length of the dataset\nprint(\"Number of samples in dataset:\", len(dataset))\n\n\nSample: tensor([0.6128]) Label: tensor([4.2255])\nNumber of samples in dataset: 100\n\n\n\n\n5.4.0.2 Dataloader\n\nIt provides “Efficient batching of data”, “Shuffling of data to avoid bias”, and “Parallel data loading using multiple workers.”\nBatch Processing:\n\nInstead of processing one sample at a time, DataLoader automatically groups samples into batches.\nThis improves computational efficiency, especially with GPUs.\n\nShuffling:\n\nShuffles the data during training to reduce bias.\n\nParallel Loading:\n\nLoads data in parallel using multiple workers (num_workers parameter).\n\n\n\n# Create a DataLoader to handle batching\ndataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n\n# Iterate through the DataLoader\nfor batch_idx, (batch_samples, batch_labels) in enumerate(dataloader):\n    print(f\"Batch {batch_idx + 1}\")\n    print(\"Samples:\\n\", batch_samples)\n    print(\"Labels:\\n\", batch_labels)\n    break  # Show only the first batch\n\nBatch 1\nSamples:\n tensor([[9.4416],\n        [7.3267],\n        [0.8215],\n        [3.6725],\n        [3.7980],\n        [3.2378],\n        [9.5629],\n        [9.5563],\n        [6.4680],\n        [4.2153]])\nLabels:\n tensor([[21.8831],\n        [17.6535],\n        [ 4.6430],\n        [10.3449],\n        [10.5959],\n        [ 9.4756],\n        [22.1257],\n        [22.1126],\n        [15.9360],\n        [11.4305]])\n\n\n\n\n5.4.0.3 Sequence example\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n# Data generation\nseq_length = 20\nnum_sample = 1000\n\n# Motif CCGGAA PWM\nmotif_pwm = np.array([[10.41, 22.86, 1.92, 1.55, 98.60, 86.66],\n                      [68.20, 65.25, 0.50, 0.35, 0.25, 2.57],\n                      [17.27, 8.30, 94.77, 97.32, 0.87, 0.00],\n                      [4.13, 3.59, 2.81, 0.78, 0.28, 10.77]])\npwm = np.hstack([np.ones((4, 7)), motif_pwm, np.ones((4, 7))])\n\n# Generate positive samples\npos = np.array([np.random.choice(['A', 'C', 'G', 'T'], num_sample,\n                                  p=pwm[:, i] / sum(pwm[:, i])) for i in range(seq_length)]).transpose()\n\n# Generate negative samples\nneg = np.array([np.random.choice(['A', 'C', 'G', 'T'], num_sample,\n                                  p=np.array([1, 1, 1, 1]) / 4) for i in range(seq_length)]).transpose()\n\n# Combine data and create labels\ndata = np.vstack([pos, neg])\nlabels = np.array([1] * num_sample + [0] * num_sample)  # Positive: 1, Negative: 0\n\nprint(data.shape, labels.shape)\n\n(2000, 20) (2000,)\n\n\n\nclass SequenceDataset(Dataset):\n    def __init__(self, sequences, labels):\n        \"\"\"\n        Initialize the dataset.\n        Args:\n        - sequences: A NumPy array of shape (num_samples, seq_length) containing DNA sequences.\n        - labels: A NumPy array of shape (num_samples,) containing labels (0 or 1).\n        \"\"\"\n        self.sequences = sequences\n        self.labels = labels\n        self.nucleotide_to_idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        # Convert sequence to one-hot encoding\n        sequence = self.sequences[idx]\n        one_hot = np.zeros((4, len(sequence)), dtype=np.float32)\n        for i, nucleotide in enumerate(sequence):\n            one_hot[self.nucleotide_to_idx[nucleotide], i] = 1.0\n\n        # Convert to PyTorch tensor\n        one_hot_tensor = torch.tensor(one_hot, dtype=torch.float32)\n        label_tensor = torch.tensor(self.labels[idx], dtype=torch.float32)\n\n        return one_hot_tensor, label_tensor\n\n# Create the dataset\ndataset = SequenceDataset(data, labels)\n\n# Create the DataLoader\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Check a batch of data\nfor batch_idx, (sequences, labels) in enumerate(dataloader):\n    print(f\"Batch {batch_idx + 1}\")\n    print(\"Sequences Shape:\", sequences.shape)  # Shape: (batch_size, 4, seq_length)\n    print(\"Labels Shape:\", labels.shape)        # Shape: (batch_size,)\n    print(\"First Sequence (One-Hot):\\n\", sequences[0])\n    print(\"First Label:\", labels[0])\n    break  # Show only the first batch\n\nBatch 1\nSequences Shape: torch.Size([32, 4, 20])\nLabels Shape: torch.Size([32])\nFirst Sequence (One-Hot):\n tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n         0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n         1., 0.],\n        [0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n         0., 0.],\n        [1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 1.]])\nFirst Label: tensor(1.)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Day4 CNN with details</span>"
    ]
  },
  {
    "objectID": "day4_cnn.html#convolutional-layers",
    "href": "day4_cnn.html#convolutional-layers",
    "title": "5  Day4 CNN with details",
    "section": "5.5 Convolutional Layers",
    "text": "5.5 Convolutional Layers\n\nExtract local patterns (features) from the input data (e.g., motifs in DNA sequences, image patterns).\nA small sliding window (filter or kernel) moves across the data.\nThe filter computes a dot product between its weights and the input it covers, producing a feature map.\n\n\n\n5.5.0.1 Kernel (Filter)\n\nA kernel (also called a filter) is a small, learnable matrix used in the convolution operation.\nKernels slide over the input data to detect patterns, such as edges in images or motifs in DNA sequences.\nThe values inside the kernel are the parameters that the model learns during training.\nFor a 2D convolution: \\[\n\\text{Kernel} =\n\\begin{bmatrix}\n1 & 0 & -1 \\\\\n1 & 0 & -1 \\\\\n1 & 0 & -1\n\\end{bmatrix}\n\\]\nFor a 1D convolution: \\[\n\\text{Kernel} =\n\\begin{bmatrix}\n1 & 0 & -1\n\\end{bmatrix}\n\\]\nIn the case of DNA, a kernel for 1D convolution that detects “ATG”: \\[\n\\text{Kernel} =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 1 & 0\n\\end{bmatrix}\n\\]\nConsider the sequence: “ATGCGTTG”.\nOne-hot encoding of the sequence 4 by 8 matrix: \\[\n\\text{Input} =\n\\begin{bmatrix}\n1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\  % A\n0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 \\\\  % C\n0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 \\\\  % G\n0 & 0 & 1 & 0 & 0 & 1 & 1 & 0    % T\n\\end{bmatrix}\n\\]\n\n\n\n5.5.0.2 Stride and padding\n\nStride = 1: The kernel moves one position at a time. This results in a highly overlapping convolution operation\nStride &gt; 1: The kernel skips positions while sliding, reducing the spatial dimensions of the feature map. This makes the computation faster but may lose some spatial detail.\n\nFor a 1D convolution, the output size is calculated as: \\[\n\\text{Output Length} = \\left\\lfloor \\frac{\\text{Input Length} - \\text{Kernel Size} + 2 \\times \\text{Padding}}{\\text{Stride}} \\right\\rfloor + 1\n\\]\nFor a 2D convolution, the output size for height and width is: \\[\n\\text{Output Height} = \\left\\lfloor \\frac{\\text{Input Height} - \\text{Kernel Height} + 2 \\times \\text{Padding}}{\\text{Stride}} \\right\\rfloor + 1\n\\]\n\\[\n\\text{Output Width} = \\left\\lfloor \\frac{\\text{Input Width} - \\text{Kernel Width} + 2 \\times \\text{Padding}}{\\text{Stride}} \\right\\rfloor + 1\n\\]\n\nimport torch\nimport torch.nn as nn\n\n# Define a 1D convolutional layer\nconv1d = nn.Conv1d(in_channels=4, out_channels=8, kernel_size=3, stride=1, padding=1)\n\n# Input tensor (batch_size=1, channels=1, seq_length=10)\ninput_tensor = torch.randn(1, 4, 10)\n\n# Apply convolution\noutput = conv1d(input_tensor)\nprint(\"Input Shape:\", input_tensor.shape)  # (1, 4, 10)\nprint(\"Output Shape:\", output.shape)       # (1, 8, 10) \n\nInput Shape: torch.Size([1, 4, 10])\nOutput Shape: torch.Size([1, 8, 10])\n\n\n\nimport torch\nimport torch.nn as nn\n\n# Define a 1D convolutional layer\nconv1d = nn.Conv1d(in_channels=4, out_channels=8, kernel_size=3, stride=3, padding=1)\n\n# Input tensor (batch_size=1, channels=1, seq_length=10)\ninput_tensor = torch.randn(1, 4, 10)\n\n# Apply convolution\noutput = conv1d(input_tensor)\nprint(\"Input Shape:\", input_tensor.shape)  # (1, 1, 10)\nprint(\"Output Shape:\", output.shape)       # (1, 1, 4) \n\nInput Shape: torch.Size([1, 4, 10])\nOutput Shape: torch.Size([1, 8, 4])\n\n\n\nWhat if padding = 0?\nWhat is the meaning of out_channels?\n\n\n\n5.5.0.3 numpy code for the motif example\n\nimport numpy as np\n\nnp.random.seed(0)\ndef conv1d_numpy(input_data, kernel):\n    \"\"\"\n    Perform 1D convolution for one-hot encoded DNA sequence.\n    Args:\n    - input_data: NumPy array of shape (4, seq_length) representing one-hot encoded DNA.\n    - kernel: NumPy array of shape (4, kernel_size) representing the convolution filter.\n    Returns:\n    - feature_map: NumPy array of the convolved output.\n    \"\"\"\n    num_channels, seq_length = input_data.shape\n    _, kernel_size = kernel.shape\n    output_length = seq_length - kernel_size + 1\n\n    # Initialize the feature map\n    feature_map = np.zeros(output_length)\n\n    # Perform convolution (dot product for each sliding window)\n    for i in range(output_length):\n        window = input_data[:, i:i+kernel_size]  # Extract sliding window\n        # out = np.multiply(window, kernel)\n        # feature_map[i] = np.sum(out)\n        feature_map[i] = np.sum(window * kernel) # dot product\n\n    return feature_map\n\n\nsequence, label = dataset[0]\nsequence_np = sequence.numpy()\n\nconv_kernel = np.random.rand(4, 5)\nconv_kernel\n\nfeature_map_np = conv1d_numpy(sequence_np, conv_kernel)\nprint(\"Feature Map (NumPy):\", feature_map_np)\nprint(\"Feature Map Shape (NumPy):\", feature_map_np.shape)\n\nFeature Map (NumPy): [2.20269505 1.83470684 3.41871594 3.17915171 3.3165857  3.15578407\n 2.97188702 3.00077732 2.71137158 3.33827867 2.95887059 3.40566087\n 2.48418074 2.8549015  3.95476016 3.13685259]\nFeature Map Shape (NumPy): (16,)\n\n\n\n\npytorch code\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DNA_CNN(nn.Module):\n    def __init__(self):\n        super(DNA_CNN, self).__init__()\n        # Convolution Layer: 4 input channels (A, C, G, T), \n        self.conv1 = nn.Conv1d(in_channels=4, out_channels=1, kernel_size=5, stride=1, padding=0)\n\n    def forward(self, x):\n        x = self.conv1(x)  # Apply convolution\n        return x\n\n# Instantiate the model\nmodel = DNA_CNN()\nprint(model)\n\nsequence, label = dataset[0]\nout = model(sequence)\nprint(\"Output Shape:\", out.shape)\nprint(out)\nnoact_out = out\n\nDNA_CNN(\n  (conv1): Conv1d(4, 1, kernel_size=(5,), stride=(1,))\n)\nOutput Shape: torch.Size([1, 16])\ntensor([[ 0.1340, -0.1981,  0.1160,  0.1986,  0.4438,  0.5583,  0.5739,  0.3242,\n         -0.0027, -0.2718,  0.2756,  0.4053,  0.2138,  0.2551,  0.4002,  0.4910]],\n       grad_fn=&lt;SqueezeBackward1&gt;)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Day4 CNN with details</span>"
    ]
  },
  {
    "objectID": "day4_cnn.html#activation-functions",
    "href": "day4_cnn.html#activation-functions",
    "title": "5  Day4 CNN with details",
    "section": "5.6 Activation Functions",
    "text": "5.6 Activation Functions\n\nIntroduce non-linearity into the model.\nCommon Function: ReLU (Rectified Linear Unit) is often used because it accelerates training and reduces the chance of vanishing gradients.\nTypes of activation functions\n\nReLU : Default choice for hidden layers in CNNs.\n\nSigmoid : Final output layer for binary classification.\n\nTanh : Hidden layers when symmetric output is beneficial (e.g., RNNs).\n\nLeaky ReLU : When addressing “dead neurons” in ReLU.\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DNA_CNN(nn.Module):\n    def __init__(self):\n        super(DNA_CNN, self).__init__()\n        # Convolution Layer: 4 input channels (A, C, G, T), \n        self.conv1 = nn.Conv1d(in_channels=4, out_channels=1, kernel_size=5, stride=1, padding=0)\n\n    def forward(self, x):\n        x = self.conv1(x)  # Apply convolution\n        x = F.relu(x)      # Apply ReLU activation\n        return x\n\nnp.random.seed(10)\n# Instantiate the model\nmodel = DNA_CNN()\nprint(model)\n\nsequence, label = dataset[0]\nout = model(sequence)\nprint(\"Output Shape:\", out.shape)\nprint(out)\n\n## plot and compare values in noact_out vs out with bar plot side by side\nimport matplotlib.pyplot as plt\n\n# Convert to NumPy arrays\nnoact_out_np = noact_out.squeeze(0).detach().numpy()\nout_np = out.squeeze(0).detach().numpy()\n\n# Plot the feature maps\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.bar(range(len(noact_out_np)), noact_out_np, color='b')\nplt.title(\"Before Activation (ReLU)\")\nplt.xlabel(\"Position\")\nplt.ylabel(\"Activation Value\")\nplt.subplot(1, 2, 2)\nplt.bar(range(len(out_np)), out_np, color='r')\nplt.title(\"After Activation (ReLU)\")\nplt.xlabel(\"Position\")\nplt.ylabel(\"Activation Value\")\nplt.show()\n\nDNA_CNN(\n  (conv1): Conv1d(4, 1, kernel_size=(5,), stride=(1,))\n)\nOutput Shape: torch.Size([1, 16])\ntensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.2317, 0.2173, 0.3338, 0.0836, 0.0000,\n         0.0000, 0.0000, 0.2567, 0.1971, 0.0000, 0.1429, 0.0000]],\n       grad_fn=&lt;ReluBackward0&gt;)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Day4 CNN with details</span>"
    ]
  },
  {
    "objectID": "day4_cnn.html#pooling-layers",
    "href": "day4_cnn.html#pooling-layers",
    "title": "5  Day4 CNN with details",
    "section": "5.7 Pooling layers",
    "text": "5.7 Pooling layers\n\nTo reduce the spatial dimensions of feature maps. It helps reducing the computational complexity of the network, aggregating features, making the model more robust to small translations or distortions in the input\nTypes:\n\nMax Pooling: Keeps the maximum value in a window.\nAverage Pooling: Averages the values in a window.\n\n\n\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DNA_CNN(nn.Module):\n    def __init__(self):\n        super(DNA_CNN, self).__init__()\n        # Convolution Layer: 4 input channels, 1 filter, kernel size=5\n        self.conv1 = nn.Conv1d(in_channels=4, out_channels=1, kernel_size=5, stride=1, padding=0)\n        # Pooling Layer: Max Pooling with kernel size=2, stride=2\n        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n\n    def forward(self, x):\n        x = self.conv1(x)  # Convolution\n        x = F.relu(x)      # ReLU activation\n        x = self.pool(x)   # Max Pooling\n        return x\n    \n\n# Example DNA sequence\nsequence, label = dataset[0]  # First sample\nsequence = sequence.unsqueeze(0)  # Add batch dimension (1, 4, 20)\n\n# Instantiate the model and pass data through it\nmodel = DNA_CNN()\noutput = model(sequence)\n\nprint(\"Input Shape:\", sequence.shape)  # (1, 4, 20)\nprint(\"Output Shape After Convolution:\", model.conv1(sequence).shape)  # (1, 1, 16)\nprint(\"Output Shape After Pooling:\", output.shape)  # (1, 1, 8)\n\n\nInput Shape: torch.Size([1, 4, 20])\nOutput Shape After Convolution: torch.Size([1, 1, 16])\nOutput Shape After Pooling: torch.Size([1, 1, 8])",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Day4 CNN with details</span>"
    ]
  },
  {
    "objectID": "day4_cnn.html#flattening",
    "href": "day4_cnn.html#flattening",
    "title": "5  Day4 CNN with details",
    "section": "5.8 Flattening",
    "text": "5.8 Flattening\n\nConverts the multidimensional output of a convolutional or pooling layer into a 1D vector.\nThis is necessary because the subsequent layers (like fully connected or dense layers) expect inputs to be in a flattened format.\n\n\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DNA_CNN(nn.Module):\n    def __init__(self):\n        super(DNA_CNN, self).__init__()\n        self.conv1 = nn.Conv1d(in_channels=4, out_channels=2, kernel_size=5, stride=1, padding=0)\n        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.fc1 = nn.Linear(1 * 16, 1)  # Fully connected layer (adjust input size)\n\n    def forward(self, x):\n        x = self.conv1(x)  # Convolution\n        x = F.relu(x)      # ReLU activation\n        x = self.pool(x)   # Max Pooling\n        x = torch.flatten(x, start_dim=1)  # Flatten for fully connected layer\n        x = self.fc1(x)    # Fully connected layer\n        return x\n\n# Example DNA sequence\nsequence, label = dataset[0]  # First sample\nsequence = sequence.unsqueeze(0)  # Add batch dimension (1, 4, 20)\n\n# Instantiate the model and pass data through it\nmodel = DNA_CNN()\noutput = model(sequence)\n\nprint(\"Input Shape:\", sequence.shape)  # (1, 4, 20)\nprint(\"Shape After Convolution:\", model.conv1(sequence).shape)  # (1, 1, 16)\nprint(\"Shape After Pooling:\", model.pool(model.conv1(sequence)).shape)  # (1, 1, 8)\nprint(\"Shape After Flattening:\", torch.flatten(model.pool(model.conv1(sequence)), start_dim=1).shape)  # (1, 8)\nprint(\"Output Shape (Final):\", output.shape)  # (1, 1)\n\nInput Shape: torch.Size([1, 4, 20])\nShape After Convolution: torch.Size([1, 2, 16])\nShape After Pooling: torch.Size([1, 2, 8])\nShape After Flattening: torch.Size([1, 16])\nOutput Shape (Final): torch.Size([1, 1])",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Day4 CNN with details</span>"
    ]
  },
  {
    "objectID": "day4_cnn.html#fully-connected-layers",
    "href": "day4_cnn.html#fully-connected-layers",
    "title": "5  Day4 CNN with details",
    "section": "5.9 Fully Connected Layers",
    "text": "5.9 Fully Connected Layers\n\nPerform classification or regression based on the extracted features.\nCombines all the features detected by earlier layers to predict an output.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Day4 CNN with details</span>"
    ]
  },
  {
    "objectID": "day4_cnn.html#output-layer",
    "href": "day4_cnn.html#output-layer",
    "title": "5  Day4 CNN with details",
    "section": "5.10 Output Layer",
    "text": "5.10 Output Layer\n\nGenerate the final prediction.\nActivation Functions:\n\nSigmoid: For binary classification.\nSoftmax: For multi-class classification.\n\n\n\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass DNA_CNN(nn.Module):\n    def __init__(self):\n        super(DNA_CNN, self).__init__()\n        self.conv1 = nn.Conv1d(in_channels=4, out_channels=16, kernel_size=3, stride=1, padding=1) # output length: 20 - 3 + 2*1 + 1 = 20\n        self.relu = nn.ReLU() # \n        self.maxpool = nn.MaxPool1d(kernel_size=2)\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(in_features=160, out_features=64)  \n        self.fc2 = nn.Linear(in_features=64, out_features=2)  \n        #self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.flatten(x)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        #x = self.softmax(x)\n        return x\n\nmodel = DNA_CNN()\nif torch.cuda.is_available():\n    model.cuda()\n\nfrom torchsummary import summary\nsummary(model, input_size=(4, 20))  # (Channels, Length)\n\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv1d-1               [-1, 16, 20]             208\n              ReLU-2               [-1, 16, 20]               0\n         MaxPool1d-3               [-1, 16, 10]               0\n           Flatten-4                  [-1, 160]               0\n            Linear-5                   [-1, 64]          10,304\n            Linear-6                    [-1, 2]             130\n================================================================\nTotal params: 10,642\nTrainable params: 10,642\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.01\nParams size (MB): 0.04\nEstimated Total Size (MB): 0.05\n----------------------------------------------------------------",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Day4 CNN with details</span>"
    ]
  },
  {
    "objectID": "day4_cnn.html#complete-code",
    "href": "day4_cnn.html#complete-code",
    "title": "5  Day4 CNN with details",
    "section": "5.11 Complete code",
    "text": "5.11 Complete code\n\n5.11.0.1 Data\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n# Data generation\nseq_length = 20\nnum_sample = 1000\n\n# Motif CCGGAA PWM\nmotif_pwm = np.array([[10.41, 22.86, 1.92, 1.55, 98.60, 86.66],\n                      [68.20, 65.25, 0.50, 0.35, 0.25, 2.57],\n                      [17.27, 8.30, 94.77, 97.32, 0.87, 0.00],\n                      [4.13, 3.59, 2.81, 0.78, 0.28, 10.77]])\npwm = np.hstack([np.ones((4, 7)), motif_pwm, np.ones((4, 7))])\n\n# Generate positive samples\npos = np.array([np.random.choice(['A', 'C', 'G', 'T'], num_sample,\n                                  p=pwm[:, i] / sum(pwm[:, i])) for i in range(seq_length)]).transpose()\n\n# Generate negative samples\nneg = np.array([np.random.choice(['A', 'C', 'G', 'T'], num_sample,\n                                  p=np.array([1, 1, 1, 1]) / 4) for i in range(seq_length)]).transpose()\n\n# Combine data and create labels\ndata = np.vstack([pos, neg])\nlabels = np.array([1] * num_sample + [0] * num_sample)  # Positive: 1, Negative: 0\n\n\nfrom sklearn.model_selection import train_test_split\n\n# Split data into training and test sets (80% training, 20% test)\ntrain_data, test_data, train_labels, test_labels = train_test_split(\n    data, labels, test_size=0.2, random_state=42, stratify=labels\n)\n\nprint(\"Training Data Shape:\", train_data.shape)\nprint(\"Test Data Shape:\", test_data.shape)\nprint(\"Training Labels Shape:\", train_labels.shape)\nprint(\"Test Labels Shape:\", test_labels.shape)\n\n\nclass SequenceDataset(Dataset):\n    def __init__(self, sequences, labels):\n        \"\"\"\n        Initialize the dataset.\n        Args:\n        - sequences: A NumPy array of shape (num_samples, seq_length) containing DNA sequences.\n        - labels: A NumPy array of shape (num_samples,) containing labels (0 or 1).\n        \"\"\"\n        self.sequences = sequences\n        self.labels = labels\n        self.nucleotide_to_idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        # Convert sequence to one-hot encoding\n        sequence = self.sequences[idx]\n        one_hot = np.zeros((4, len(sequence)), dtype=np.float32)\n        for i, nucleotide in enumerate(sequence):\n            one_hot[self.nucleotide_to_idx[nucleotide], i] = 1.0\n\n        # Convert to PyTorch tensor\n        one_hot_tensor = torch.tensor(one_hot, dtype=torch.float32)\n        label_tensor = torch.tensor(self.labels[idx], dtype=torch.float32)\n\n        return one_hot_tensor, label_tensor\n\n# Create the dataset\ntrain_dataset = SequenceDataset(train_data, train_labels)\n\n# Create the DataLoader\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\nTraining Data Shape: (1600, 20)\nTest Data Shape: (400, 20)\nTraining Labels Shape: (1600,)\nTest Labels Shape: (400,)\n\n\n\n\n5.11.0.2 Model and training\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass DNA_CNN(nn.Module):\n    def __init__(self):\n        super(DNA_CNN, self).__init__()\n        self.conv1 = nn.Conv1d(in_channels=4, out_channels=16, kernel_size=3, stride=1, padding=1) # output length: 20 - 3 + 2*1 + 1 = 20\n        self.relu = nn.ReLU() # \n        self.maxpool = nn.MaxPool1d(kernel_size=2)\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(in_features=160, out_features=64)  \n        self.fc2 = nn.Linear(in_features=64, out_features=2)  \n        #self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.flatten(x)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        #x = self.softmax(x)\n        return x\n\n\n\nmodel = DNA_CNN()\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nepochs = 10  # Number of epochs\n\nfor epoch in range(epochs):\n    model.train()  # Set model to training mode\n    total_loss = 0\n\n    for batch_idx, (sequences, labels) in enumerate(train_dataloader):\n        # Prepare data\n        sequences = sequences  # (batch_size, 4, seq_length)\n        labels = labels.long()  # Convert labels to long for CrossEntropyLoss\n\n        # Forward pass\n        outputs = model(sequences)\n\n        # Compute loss\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Accumulate loss\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_dataloader):.4f}\")\n\n\n\nEpoch 1/10, Loss: 0.4741\nEpoch 2/10, Loss: 0.1364\nEpoch 3/10, Loss: 0.0889\nEpoch 4/10, Loss: 0.0768\nEpoch 5/10, Loss: 0.0738\nEpoch 6/10, Loss: 0.0616\nEpoch 7/10, Loss: 0.0578\nEpoch 8/10, Loss: 0.0547\nEpoch 9/10, Loss: 0.0528\nEpoch 10/10, Loss: 0.0488\n\n\n\n\n5.11.0.3 Testing\n\n# Create test dataset and dataloader\ntest_dataset = SequenceDataset(test_data, test_labels)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# Evaluate the model\nmodel.eval()  # Set model to evaluation mode\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():  # Disable gradient calculation for testing\n    for sequences, labels in test_dataloader:\n        sequences = sequences  # (batch_size, 4, seq_length)\n        labels = labels.long()  # Convert labels to long for CrossEntropyLoss\n\n        # Forward pass\n        outputs = model(sequences)\n\n        # Get predictions\n        _, predictions = torch.max(outputs, 1)\n\n        # Count correct predictions\n        correct += (predictions == labels).sum().item()\n        total += labels.size(0)\n\nprint(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n\nTest Accuracy: 96.75%\n\n\n\n\n5.11.1 Device for computation\n\nimport torch\n\n# Check if CUDA is available\nif torch.cuda.is_available():\n    print(\"CUDA is available. Training will be performed on GPU.\")\nelse:\n    print(\"CUDA is not available. Training will be performed on CPU.\")\n\nprint(next(model.parameters()).device)\nprint(sequences.device)\nprint(labels.device)\n\nprint(f\"Allocated GPU memory: {torch.cuda.memory_allocated() / 1024 ** 2:.2f} MB\")\nprint(f\"Cached GPU memory: {torch.cuda.memory_reserved() / 1024 ** 2:.2f} MB\")\n\nCUDA is available. Training will be performed on GPU.\ncpu\ncpu\ncpu\nAllocated GPU memory: 8.44 MB\nCached GPU memory: 22.00 MB\n\n\n\n!nvidia-smi\n\nMon Dec  2 12:58:41 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.183.04             Driver Version: 538.78       CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA RTX A5500 Laptop GPU    On  | 00000000:01:00.0 Off |                  Off |\n| N/A   55C    P8              12W /  82W |   1419MiB / 16384MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|    0   N/A  N/A      2669      C   /python3.11                               N/A      |\n+---------------------------------------------------------------------------------------+\n\n\n\n\n5.11.2 Run on GPU\n\n\n\n# Check for GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Move the model to GPU\nmodel = DNA_CNN().to(device)\nimport torch.optim as optim\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()  # CrossEntropyLoss for classification\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nepochs = 5  # Number of epochs\nfor epoch in range(epochs):\n    model.train()  # Set the model to training mode\n    total_loss = 0\n\n    for batch_idx, (sequences, labels) in enumerate(train_dataloader):\n        # Move data to GPU\n        sequences = sequences.to(device)  # Move input to GPU\n        labels = labels.to(device).long()  # Move labels to GPU and ensure correct type\n\n        # Forward pass\n        outputs = model(sequences)\n\n        # Compute loss\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Accumulate loss\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_dataloader):.4f}\")\n\nUsing device: cuda\nEpoch 1/5, Loss: 0.4898\nEpoch 2/5, Loss: 0.1334\nEpoch 3/5, Loss: 0.0785\nEpoch 4/5, Loss: 0.0703\nEpoch 5/5, Loss: 0.0630\n\n\n\n# Test the model\nmodel.eval()  # Set the model to evaluation mode\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():  # Disable gradient calculation\n    for sequences, labels in test_dataloader:\n        # Move data to GPU\n        sequences = sequences.to(device)\n        labels = labels.to(device).long()\n\n        # Forward pass\n        outputs = model(sequences)\n\n        # Get predictions\n        _, predictions = torch.max(outputs, 1)\n\n        # Count correct predictions\n        correct += (predictions == labels).sum().item()\n        total += labels.size(0)\n\nprint(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n\nTest Accuracy: 96.50%\n\n\n\nimport torch\n\n# Check if CUDA is available\nif torch.cuda.is_available():\n    print(\"CUDA is available. Training will be performed on GPU.\")\nelse:\n    print(\"CUDA is not available. Training will be performed on CPU.\")\n\nprint(next(model.parameters()).device)\nprint(sequences.device)\nprint(labels.device)\n\nprint(f\"Allocated GPU memory: {torch.cuda.memory_allocated() / 1024 ** 2:.2f} MB\")\nprint(f\"Cached GPU memory: {torch.cuda.memory_reserved() / 1024 ** 2:.2f} MB\")\n\nCUDA is available. Training will be performed on GPU.\ncuda:0\ncuda:0\ncuda:0\nAllocated GPU memory: 16.74 MB\nCached GPU memory: 22.00 MB",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Day4 CNN with details</span>"
    ]
  },
  {
    "objectID": "day5_model_evaluation.html",
    "href": "day5_model_evaluation.html",
    "title": "6  Day5 Model Evaluation",
    "section": "",
    "text": "6.1 Basic Concepts",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Day5 Model Evaluation</span>"
    ]
  },
  {
    "objectID": "day5_model_evaluation.html#basic-concepts",
    "href": "day5_model_evaluation.html#basic-concepts",
    "title": "6  Day5 Model Evaluation",
    "section": "",
    "text": "When evaluating a classification model, predictions are compared with the actual (ground truth) labels.\n\n\n6.1.0.1 True Positive (TP)\n\nThe model correctly predicts the positive class.\n\nActual: 1 (positive).\nPredicted: 1 (positive).\n\n\n\n\n6.1.0.2 False Positive (FP) (Type I Error)\n\nThe model predicts the positive class, but the actual class is negative.\n\nActual: 0 (negative).\nPredicted: 1 (positive).\nIn disease detection, marking a normal person as patient.\n\n\n\n\n6.1.0.3 True Negative (TN)\n\nThe model correctly predicts the negative class.\n\nActual: 0 (negative).\nPredicted: 0 (negative).\n\nIndicates how well the model identifies the negative class.\n\n\n\n6.1.0.4 False Negative (FN) (Type II Error)\n\nThe model predicts the negative class, but the actual class is positive.\n\nActual: 1 (positive).\nPredicted: 0 (negative).\nIn disease detection, failing to identify a patient with the disease.\n\n\n\n\n6.1.0.5 Confusion matrix from wiki",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Day5 Model Evaluation</span>"
    ]
  },
  {
    "objectID": "day5_model_evaluation.html#accuracy",
    "href": "day5_model_evaluation.html#accuracy",
    "title": "6  Day5 Model Evaluation",
    "section": "6.2 Accuracy",
    "text": "6.2 Accuracy\n\nAccuracy is one of the most straightforward and commonly used metrics to evaluate the performance of a classification model.\nIt measures the proportion of correctly classified samples out of the total number of samples.\n\n\\[\n\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n\\]\n\nIn terms of the confusion matrix, accuracy can also be expressed as:\n\n\\[\n\\text{Accuracy} = \\frac{\\text{True Positives (TP)} + \\text{True Negatives (TN)}}{\\text{Total Number of Samples (TP + TN + FP + FN)}}\n\\]\n\nGood\n\nWhen the dataset is balanced, meaning the number of positive and negative samples is roughly equal.\nWhen all misclassification errors (false positives and false negatives) are equally costly.\nEasy to understand and implement.\nWorks well for balanced datasets.\n\nNo good\n\nwhen the dataset is imbalanced:\nFor example, if 95% of the samples belong to 1, a model that always predicts the majority class will achieve 95% accuracy, but this is misleading.\nAccuracy treats all errors equally, which is not suitable if false positives and false negatives have different costs.\n\n\n\nfrom sklearn.metrics import accuracy_score\n\n# Ground truth labels and model predictions\ny_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\ny_pred = [1, 0, 1, 0, 0, 1, 0, 0, 1, 1]\n\nTP = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 1)\nTN = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 0)\nFP = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 1)\nFN = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 0)\naccuracy = (TP + TN) / (TP + TN + FP + FN)\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")\n\naccuracy = accuracy_score(y_true, y_pred)\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")\n\nAccuracy: 80.00%\nAccuracy: 80.00%",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Day5 Model Evaluation</span>"
    ]
  },
  {
    "objectID": "day5_model_evaluation.html#precision",
    "href": "day5_model_evaluation.html#precision",
    "title": "6  Day5 Model Evaluation",
    "section": "6.3 Precision",
    "text": "6.3 Precision\n\nOf all the samples predicted as positive, how many are truly positive?\nIt measures the proportion of true positive predictions out of all the samples that were predicted as positive.\n\n\\[\n\\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Positives (FP)}}\n\\]\n\nPrecision is critical in scenarios where incorrectly predicting positives (FP) has significant consequences.\nHigh precision ensures the model is not making too many false positive predictions, thus reducing “false alarms.”\nGood\n\nEnsures positive predictions are reliable.\nHelps reduce “false alarms” in critical systems.\nPrecision is especially useful for imbalanced datasets where the positive class is rare.\n\nNo Good\n\nPrecision does not account for cases where the model misses actual positives (FN).\nIn situations where identifying all positives is crucial (e.g., disease detection), precision alone is insufficient.\n\n\n\n\nfrom sklearn.metrics import precision_score\n\n# True labels\ny_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n\n# Predicted labels\ny_pred = [1, 0, 1, 0, 0, 1, 0, 0, 1, 1]\n\nTP = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 1)\nFP = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 1)\nprecision = TP / (TP + FP)\nprint(f\"Precision: {precision * 100:.2f}%\")\n\n# Compute precision\nprecision = precision_score(y_true, y_pred)\nprint(f\"Precision: {precision * 100:.2f}%\")\n\nPrecision: 80.00%\nPrecision: 80.00%",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Day5 Model Evaluation</span>"
    ]
  },
  {
    "objectID": "day5_model_evaluation.html#recall",
    "href": "day5_model_evaluation.html#recall",
    "title": "6  Day5 Model Evaluation",
    "section": "6.4 Recall",
    "text": "6.4 Recall\n\nOf all the actual positive samples, how many did the model correctly identify?\nKnown as sensitivity or true positive rate (TPR)\n\n\\[\n\\text{Recall} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Negatives (FN)}}\n\\]\n\nIt focuses on the model’s ability to avoid missing true positives (false negatives).\nRecall is critical in scenarios where missing positive samples (FN) can have severe consequences.\n\nMedical Diagnosis: Failing to identify a disease could lead to fatal consequences.\n\nRecall and precision often have a trade-off:\n\nHigh recall increases false positives, lowering precision.\nHigh precision may miss true positives, lowering recall.\n\nGood\n\nRecall ensures that the model does not miss positive samples.\nIn applications like medical diagnosis, recall is a priority since false negatives are costly.\n\nNo good\n\nRecall does not account for how many false positives the model predicts.\nThis can lead to overly lenient models that classify many samples as positive to maximize recall.\nIn applications like spam detection, focusing solely on recall might result in excessive false positives (e.g., legitimate emails marked as spam).\n\n\n\n\nfrom sklearn.metrics import recall_score\n\n# True labels\ny_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n\n# Predicted labels\ny_pred = [1, 0, 1, 0, 0, 1, 0, 0, 1, 1]\n\n\n# Calculate recall\nTP = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 1)\nFN = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 0)\nrecall = TP / (TP + FN)\nprint(f\"Recall: {recall:.2f}\")\n\n# Compute recall\nrecall = recall_score(y_true, y_pred)\nprint(f\"Recall: {recall:.2f}\")\n\nRecall: 0.80\nRecall: 0.80",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Day5 Model Evaluation</span>"
    ]
  },
  {
    "objectID": "day6_seq2seq.html",
    "href": "day6_seq2seq.html",
    "title": "7  Day6 Seq2Seq - Encoder",
    "section": "",
    "text": "7.1 Encoder Components",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Day6 Seq2Seq - Encoder</span>"
    ]
  },
  {
    "objectID": "day6_seq2seq.html#encoder-components",
    "href": "day6_seq2seq.html#encoder-components",
    "title": "7  Day6 Seq2Seq - Encoder",
    "section": "",
    "text": "7.1.0.1 Embedding Layer\n\nConverts input token indices into dense, continuous vector representations.\nAllows the model to represent discrete tokens (e.g., words, characters) in a meaningful way for further processing.\n\n\n\n7.1.0.2 Recurrent Neural Network (RNN)\n\nProcesses the embeddings sequentially and captures temporal dependencies.\nVariants include:\n\nSimple RNN: Basic recurrent unit.\nLSTM (Long Short-Term Memory): Handles long-term dependencies.\nGRU (Gated Recurrent Unit): A simplified version of LSTM.\n\n\n\n\n7.1.0.3 Context Vector\n\nThe final hidden state (or all hidden states) of the RNN represents the input sequence context.\nPassed to the decoder for generating output.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Day6 Seq2Seq - Encoder</span>"
    ]
  },
  {
    "objectID": "day6_seq2seq.html#embedding-layer-1",
    "href": "day6_seq2seq.html#embedding-layer-1",
    "title": "7  Day6 Seq2Seq - Encoder",
    "section": "7.2 Embedding Layer",
    "text": "7.2 Embedding Layer\n\nAn embedding layer is used to represent discrete categorical data (like words or DNA sequences) as dense, continuous vectors.\nDimensionality Reduction\n\nInstead of representing a word or token as a large sparse vector (e.g., one-hot encoding), embeddings map them to a smaller, dense vector space.\nOne-hot vector for a vocabulary of 10,000 words: [0, 0, ..., 1, 0] (10,000 dimensions).\nEmbedding vector: [0.5, -0.2, 0.1] (e.g., 3 dimensions).\n\nCaptures Semantic Relationships\n\nSimilar words or tokens are mapped to similar embeddings in the vector space.\n“king” and “queen” might be close in embedding space, capturing semantic similarity.\n\nEfficient Computation\n\nDense vectors are smaller and faster to process compared to sparse representations like one-hot encoding.\n\n\n\n\n7.2.0.1 Types\n\nk-mer Encoding\n\nUsed for DNA/RNA sequences by splitting into overlapping k-length substrings.\nSequence: \"ACGTAC\"\n3-mer: [\"ACG\", \"CGT\", \"GTA\", \"TAC\"]\n\nLabel Encoding\n\nMaps each k-mer to a unique integer.\nEncoding: {\"ACG\": 0, \"CGT\": 1, \"GTA\": 2}\n\nOne-Hot Encoding\n\nRepresents nucleotides (e.g., A, C, G, T) as binary vectors.\nA → [1, 0, 0, 0]\n\nEmbedding\n\nConverts nucleotides, k-mers, or amino acids into dense vectors.\nA → [0.3, 0.7, -0.1]\n\n\n\n\n7.2.1 Word2Vec\n\nIt generates word embeddings dense vector representations of words that capture their meanings and relationships.\nUnlike traditional representations (e.g., one-hot encoding), Word2Vec places words with similar meanings closer together in a high-dimensional vector space, enabling models to understand semantic and syntactic relationships between words.\n\n\n\n7.2.1.1 Data\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\nnum_samples = 500\nnum_aa_len = 10\n\n# Define amino acids\namino_acids = [\"A\", \"R\", \"N\", \"D\", \"C\", \"Q\", \"E\", \"G\", \"H\", \"I\", \"L\", \"K\", \"M\", \"F\", \"P\", \"S\", \"T\", \"W\", \"Y\", \"V\"]\n\n# Generate synthetic amino acid sequences (random sequences)\nrandom_corpus_amino = [[random.choice(amino_acids) for _ in range(num_aa_len)] for _ in range(num_samples)]\n\n# Define specific amino acids to make closer\ntarget_amino_acids = [\"A\", \"C\", \"K\", \"I\"]  # Sequence to be inserted\n\n# Insert target_amino_acids into the middle of each sequence in random_corpus_amino\nfull_corpus_target_amino = []\nfor seq in random_corpus_amino:\n    mid_index = len(seq) // 2\n    new_seq = seq[:mid_index] + target_amino_acids + seq[mid_index:]\n    full_corpus_target_amino.append(new_seq)\n\n# Show shape and content of `full_corpus_target_amino`\nprint(\"Number of sequences in full_corpus_target_amino:\", len(full_corpus_target_amino))\nprint(\"Example sequence (with inserted target_amino_acids):\", full_corpus_target_amino[0])\n\nNumber of sequences in full_corpus_target_amino: 500\nExample sequence (with inserted target_amino_acids): ['G', 'N', 'R', 'P', 'P', 'A', 'C', 'K', 'I', 'C', 'Q', 'V', 'N', 'M']\n\n\n\ndisplay(full_corpus_target_amino[0])\nfor idx, target in enumerate(full_corpus_target_amino[0]):\n    print(f\"Index {idx}: {target}\")\n\n['G', 'N', 'R', 'P', 'P', 'A', 'C', 'K', 'I', 'C', 'Q', 'V', 'N', 'M']\n\n\nIndex 0: G\nIndex 1: N\nIndex 2: R\nIndex 3: P\nIndex 4: P\nIndex 5: A\nIndex 6: C\nIndex 7: K\nIndex 8: I\nIndex 9: C\nIndex 10: Q\nIndex 11: V\nIndex 12: N\nIndex 13: M\n\n\n\n\n# Build vocabulary\nvocab_target_amino = {aa: idx for idx, aa in enumerate(set(aa for seq in full_corpus_target_amino for aa in seq))}\nvocab_size_target_amino = len(vocab_target_amino)\n\nprint(\"Vocabulary size:\", vocab_size_target_amino)\n\n# Prepare training data (target, context) pairs\nwindow_size = 2  # Context window size\ntraining_data_target_amino = []\nfor seq in full_corpus_target_amino:\n    # print(\"Sequence:\", seq)\n    for idx, target in enumerate(seq):\n        context_indices = list(range(max(0, idx - window_size), min(len(seq), idx + window_size + 1)))\n        # print(\"Context indices:\", context_indices)\n        if idx in context_indices:  # Prevent self-reference\n            context_indices.remove(idx)\n        for context_idx in context_indices:\n            training_data_target_amino.append((vocab_target_amino[target], vocab_target_amino[seq[context_idx]]))\n            # print(\"Target:\", target, \"Context:\", seq[context_idx])\n    # print(\"\\n\")\n            \nprint(\"Number of training pairs:\", len(training_data_target_amino))\nprint(\"Example training pair:\", training_data_target_amino[:5])\n\nVocabulary size: 20\nNumber of training pairs: 25000\nExample training pair: [(10, 19), (10, 8), (19, 10), (19, 8), (19, 0)]\n\n\nVocabulary size: 20\nSequence: ['K', 'I', 'G', 'P', 'K', 'A', 'C', 'K', 'I', 'V', 'F', 'D', 'Q', 'G']\nTarget: K Context: I\nTarget: K Context: G\nTarget: I Context: K\nTarget: I Context: G\nTarget: I Context: P\nTarget: G Context: K\nTarget: G Context: I\nTarget: G Context: P\nTarget: G Context: K\nTarget: P Context: I\nTarget: P Context: G\nTarget: P Context: K\nTarget: P Context: A\nTarget: K Context: G\nTarget: K Context: P\nTarget: K Context: A\nTarget: K Context: C\nTarget: A Context: P\nTarget: A Context: K\nTarget: A Context: C\nTarget: A Context: K\n\n\n7.2.1.2 Training\n\n\n# Initialize model parameters\nembedding_dim = 2  # For visualization\nlearning_rate = 0.01\nepochs = 50\nW_input_target_amino = np.random.randn(vocab_size_target_amino, embedding_dim)  # Input weights\nW_output_target_amino = np.random.randn(embedding_dim, vocab_size_target_amino)  # Output weights\nlosses_target_amino = []\n\n# Helper functions\ndef one_hot_encoding(word_idx, vocab_size):\n    vec = np.zeros(vocab_size)\n    vec[word_idx] = 1\n    return vec\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum()\n\n# Save initial embeddings for visualization\ninitial_embeddings_target_amino = W_input_target_amino.copy()\n\n# Training loop\nfor epoch in range(epochs):\n    total_loss = 0\n    for target_idx, context_idx in training_data_target_amino:\n        # Forward pass\n        target_vector = one_hot_encoding(target_idx, vocab_size_target_amino)\n        hidden_layer = np.dot(target_vector, W_input_target_amino)\n        output_layer = np.dot(hidden_layer, W_output_target_amino)\n        y_pred = softmax(output_layer)\n        \n        # Loss: Negative log likelihood\n        loss = -np.log(y_pred[context_idx])\n        total_loss += loss\n\n        # Backpropagation\n        y_true = one_hot_encoding(context_idx, vocab_size_target_amino)\n        error = y_pred - y_true\n        dW_output = np.outer(hidden_layer, error)\n        dW_input = np.outer(target_vector, np.dot(W_output_target_amino, error.T))\n\n        # Update weights\n        W_output_target_amino -= learning_rate * dW_output\n        W_input_target_amino -= learning_rate * dW_input\n\n    losses_target_amino.append(total_loss)\n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n\n# Save final embeddings after training\nfinal_embeddings_target_amino = W_input_target_amino.copy()\n\nEpoch 0, Loss: 72028.0166\nEpoch 10, Loss: 68994.6796\nEpoch 20, Loss: 68995.5950\nEpoch 30, Loss: 68995.6983\nEpoch 40, Loss: 68995.7019\nEpoch 50, Loss: 68995.7002\nEpoch 60, Loss: 68995.6995\nEpoch 70, Loss: 68995.6993\nEpoch 80, Loss: 68995.6993\nEpoch 90, Loss: 68995.6992\n\n\n\n\n# Plot functions\ndef plot_embeddings(embeddings, title):\n    plt.figure(figsize=(8, 6))\n    for aa, idx in vocab_target_amino.items():\n        plt.scatter(embeddings[idx, 0], embeddings[idx, 1], label=aa)\n        plt.text(embeddings[idx, 0] + 0.02, embeddings[idx, 1], aa, fontsize=12)\n    plt.title(title)\n    plt.xlabel(\"Dimension 1\")\n    plt.ylabel(\"Dimension 2\")\n    plt.grid()\n    plt.legend()\n    plt.show()\n\n# Plot embeddings before and after training\nplot_embeddings(initial_embeddings_target_amino, \"Initial Embeddings (Before Training)\")\nplot_embeddings(final_embeddings_target_amino, \"Final Embeddings (After Training)\")\n\n# Plot training loss\nplt.figure(figsize=(8, 6))\nplt.plot(losses_target_amino)\nplt.title(\"Training Loss Over Epochs\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.2.2 Recurrent Neural Network\n\nAn RNN is a type of neural network designed for sequence data, where the order of inputs matters.\nUnlike feedforward networks, RNNs have a hidden state that carries information across time steps, enabling them to model dependencies in sequences.\n\n\n\n\nimport numpy as np\n\n\n# Example RNN with single hidden unit\ndef simple_rnn_step(x, h_prev, W_x, W_h, b):\n    return np.tanh(np.dot(x, W_x) + np.dot(h_prev, W_h) + b)\n\n# Input: Sequence of 10 time step (seq len), each with 2 features (embedding dim)\nemb_dim = 2\nseq_len = 10\nemb_sequence = np.random.randn(seq_len, emb_dim)\nhidden_dim = 5\nW_x = np.random.randn(emb_dim, hidden_dim)\nW_h = np.random.randn(hidden_dim, hidden_dim)\nb = np.random.randn(hidden_dim)\n\nprint(\"W_x:\", W_x.shape)\n# Initial hidden state\nh_prev = np.zeros(hidden_dim)\n\n# Process sequence\nhidden_states = []\nfor x in emb_sequence:\n    h_prev = simple_rnn_step(x, h_prev, W_x, W_h, b)\n    hidden_states.append(h_prev)\n\nhidden_states = np.stack(hidden_states)\nprint(\"Hidden states (NumPy):\")\nprint(hidden_states)\n\nW_x: (2, 5)\nHidden states (NumPy):\n[[-0.98000683  0.48687102 -0.02096787  0.45009672  0.27724678]\n [-0.84668577  0.64134528 -0.82791907  0.65346347 -0.26902507]\n [-0.99263245 -0.97898747  0.41942684 -0.06326473  0.89334266]\n [-0.99449218  0.98994133 -0.75759692  0.93759738  0.99037674]\n [ 0.43438319  0.99999769 -0.99964928  0.99738487 -0.99600442]\n [-0.89626476 -0.52253602 -0.76895233  0.43461405 -0.99375944]\n [-0.06383844 -0.40320671 -0.92602607  0.99031633 -0.99885451]\n [-0.97249039 -0.99738262 -0.3035591   0.67769993 -0.41296522]\n [-0.99998021 -0.99995791  0.94914681 -0.64043451  0.99998966]\n [-0.99742254  0.56102461  0.62906189  0.39046593  0.99953853]]\n\n\n\n\nimport torch\nfrom torch import nn\n\n# RNN with 4 input features and 2 hidden units\nrnn = nn.RNN(input_size=2, hidden_size=5, batch_first=True)\n\n# convert numpy array to tensor \nemb_sequence_tensor = torch.tensor(emb_sequence, dtype=torch.float32).unsqueeze(0)\nprint(emb_sequence_tensor.shape)\n\n# Process sequence\noutput, hidden = rnn(emb_sequence_tensor)\nprint(\"Output states:\")\nprint(output)\n\ntorch.Size([1, 10, 2])\nOutput states:\ntensor([[[ 0.5049,  0.1469,  0.0726,  0.4769, -0.0408],\n         [-0.0964,  0.5451, -0.2459,  0.2843, -0.2156],\n         [ 0.6120,  0.6717, -0.2687,  0.5920, -0.3996],\n         [ 0.6601,  0.2275,  0.2436,  0.8083, -0.2617],\n         [-0.2582,  0.0174, -0.0960,  0.2661,  0.1580],\n         [ 0.4932,  0.2232, -0.1469,  0.3265,  0.0755],\n         [-0.1702,  0.0664, -0.4214,  0.1334,  0.3371],\n         [ 0.4903,  0.3769, -0.3761,  0.3053,  0.1079],\n         [ 0.8995,  0.6985,  0.5925,  0.9079, -0.7493],\n         [ 0.2446,  0.4215,  0.4886,  0.7592, -0.6051]]],\n       grad_fn=&lt;TransposeBackward1&gt;)\n\n\n\nimport numpy as np\n\n# Define the encoder class\nclass Encoder:\n    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n\n        # Initialize weights\n        self.embeddings = np.random.randn(vocab_size, embedding_dim)\n        self.W_xh = np.random.randn(embedding_dim, hidden_dim)\n        self.W_hh = np.random.randn(hidden_dim, hidden_dim)\n        self.b_h = np.random.randn(hidden_dim)\n\n    def forward(self, sequence):\n        hidden_states = []\n        h_prev = np.zeros(self.hidden_dim)  # Initial hidden state\n        for idx in sequence:\n            x_emb = self.embeddings[idx]  # Get the embedding for the current input\n            h_t = np.tanh(np.dot(x_emb, self.W_xh) + np.dot(h_prev, self.W_hh) + self.b_h)  # Compute new hidden state\n            hidden_states.append(h_t)\n            h_prev = h_t  # Update the previous hidden state\n        return np.array(hidden_states)\n\n\n# Initialize encoder\nvocab_size = len(vocab_target_amino)  # Use the vocabulary size from your data\nembedding_dim = emb_sequence.shape[1]  # Use the embedding dimension from your data\n\nencoder = Encoder(vocab_size, embedding_dim, hidden_dim)\n\n# Encode full corpus\nencoded_corpus = []\nfor seq in full_corpus_target_amino:\n    # Convert sequence of amino acids to indices\n    sequence_indices = [vocab_target_amino[aa] for aa in seq]\n    # Encode the sequence\n    hidden_states = encoder.forward(sequence_indices)\n    encoded_corpus.append(hidden_states)\n\n# Display the results for the first sequence\nprint(\"Encoded hidden states shape for the first sequence:\", encoded_corpus[0].shape)\nprint(\"Encoded hidden states for the first sequence:\")\nprint(encoded_corpus[0])\n\n\nEncoded hidden states shape for the first sequence: (14, 5)\nEncoded hidden states for the first sequence:\n[[ 0.03868501  0.66606342  0.11239537  0.94252519  0.32926839]\n [-0.99970181  0.89515207  0.05415328  0.99985625 -0.97463257]\n [ 0.75058652  0.9999487  -0.97298639  0.99986108  0.5502992 ]\n [-0.79309047  0.93758224  0.77651802  0.97977334  0.68871062]\n [-0.6675474   0.66976273 -0.99566127  0.99975604 -0.99902124]\n [ 0.21431908  0.99999278 -0.22389798  0.99643739  0.75816754]\n [ 0.99707863  0.53458673 -0.9346094   0.96186194  0.8659825 ]\n [ 0.94180551 -0.1262172   0.7471511   0.76933377  0.99698075]\n [ 0.97346783 -0.94293276 -0.96551345  0.93134456  0.89477287]\n [ 0.93972524  0.03184492  0.54691498 -0.82663898  0.95346251]\n [ 0.81065257 -0.99878861  0.49700672  0.20539777  0.75313826]\n [-0.9592758  -0.93075257  0.1209835   0.81122606 -0.64387445]\n [-0.9999626   0.99963746 -0.42684594  0.99916255 -0.9996164 ]\n [ 0.7287792   0.99997848 -0.86902579  0.99966907  0.73170174]]\n\n\n\n\n7.2.3 Context Vector\n\nThe final hidden state (or all hidden states) of the RNN represents the input sequence context.\nPassed to the decoder for generating output.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Day6 Seq2Seq - Encoder</span>"
    ]
  },
  {
    "objectID": "day7_seq2seq_aa.html",
    "href": "day7_seq2seq_aa.html",
    "title": "8  Day7 Seq2Seq with Amino Acid",
    "section": "",
    "text": "8.1 Encoder with AA",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Day7 Seq2Seq with Amino Acid</span>"
    ]
  },
  {
    "objectID": "day7_seq2seq_aa.html#encoder-with-aa",
    "href": "day7_seq2seq_aa.html#encoder-with-aa",
    "title": "8  Day7 Seq2Seq with Amino Acid",
    "section": "",
    "text": "8.1.1 Applications\n\nInput: AA, Output: GO\nInput: AA, Output: Secondary structure (Alpha helix H, Beta sheet E, Coil C)\nInput: AA, Output: AA (mutant)\nInput: AA, Output: AA (input AA)\netc\n\n\n8.1.1.1 Training\n\n\n\n8.1.1.2 Testing\n\n\n\n\n8.1.2 Data generation (simulation)\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass Seq2SeqDataset(Dataset):\n    def __init__(self, source_sequences, target_sequences):\n        self.source_sequences = source_sequences\n        self.target_sequences = target_sequences\n\n    def __len__(self):\n        return len(self.source_sequences)\n\n    def __getitem__(self, idx):\n        return self.source_sequences[idx], self.target_sequences[idx]\n\n# Example amino acids\namino_acids = [\"A\", \"R\", \"N\", \"D\", \"C\", \"Q\", \"E\", \"G\", \"H\", \"I\", \"L\", \"K\", \"M\", \"F\", \"P\", \"S\", \"T\", \"W\", \"Y\", \"V\"]\n\n# Generate source sequences\nnum_sequences = 100 # number of samples\nsequence_length = 10 \nsource_sequences = [\n    \"\".join(np.random.choice(amino_acids, size=sequence_length)) for _ in range(num_sequences)\n]\n\n# Transformation pattern for generating target sequences\ndef transform_to_target(sequence):\n    # Example pattern: Reverse the sequence and replace specific amino acids\n    mapping = {\"A\": \"T\", \"R\": \"N\", \"N\": \"R\", \"D\": \"C\", \"C\": \"D\"}  # Custom substitution rules\n    transformed = [mapping.get(aa, aa) for aa in sequence]  # substitution\n    return \"\".join(transformed)\n\n# Create target sequences based on the pattern\ntarget_sequences = [transform_to_target(seq) for seq in source_sequences]\n\n# Build vocabulary\nvocab = {aa: idx for idx, aa in enumerate(amino_acids)}\nvocab_size = len(vocab)\n\n# build idx2vocab\nidx2vocab = {idx: aa for aa, idx in vocab.items()}\n\n\n# Encode sequences into integer indices\ndef encode_sequence(sequence, vocab):\n    return [vocab[aa] for aa in sequence]\n\nencoded_source_sequences = [encode_sequence(seq, vocab) for seq in source_sequences]\nencoded_target_sequences = [encode_sequence(seq, vocab) for seq in target_sequences]\n\n# Collate function for padding\n# The collate_batch function handles these differences by padding shorter sequences so that all sequences in a batch have the same length.\ndef collate_batch(batch, vocab_size):\n    source_batch, target_batch = zip(*batch)\n    source_lengths = [len(seq) for seq in source_batch]\n    target_lengths = [len(seq) for seq in target_batch]\n\n    # Pad sequences\n    max_src_len = max(source_lengths)\n    max_tgt_len = max(target_lengths)\n    padded_src = torch.zeros(len(source_batch), max_src_len, dtype=torch.long)\n    padded_tgt = torch.zeros(len(target_batch), max_tgt_len, dtype=torch.long)\n\n    for i, seq in enumerate(source_batch):\n        padded_src[i, :len(seq)] = torch.tensor(seq, dtype=torch.long)\n    for i, seq in enumerate(target_batch):\n        padded_tgt[i, :len(seq)] = torch.tensor(seq, dtype=torch.long)\n\n    return padded_src, source_lengths, padded_tgt, target_lengths\n\n# Create dataset and dataloader\ndataset = Seq2SeqDataset(encoded_source_sequences, encoded_target_sequences)\ndataloader = DataLoader(dataset, batch_size=10, shuffle=True, collate_fn=lambda x: collate_batch(x, vocab_size))\n\n# Example batch\nfor batch in dataloader:\n    src, src_len, tgt, tgt_len = batch\n    print(\"Source batch shape:\", src.shape)\n    print(\"Source lengths:\", src_len)\n    print(\"Target batch shape:\", tgt.shape)\n    print(\"Target lengths:\", tgt_len)\n    break\n\nSource batch shape: torch.Size([10, 10])\nSource lengths: [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\nTarget batch shape: torch.Size([10, 10])\nTarget lengths: [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n\n\n\nprint(len(dataset))\nprint(dataset[0][0])\nprint(\"source:\", [idx2vocab[idx] for idx in dataset[0][0]])\nprint(\"source:\", [idx2vocab[idx] for idx in dataset[0][1]])\n\n100\n[2, 0, 12, 4, 6, 3, 17, 14, 6, 7]\nsource: ['N', 'A', 'M', 'C', 'E', 'D', 'W', 'P', 'E', 'G']\nsource: ['R', 'T', 'M', 'D', 'E', 'C', 'W', 'P', 'E', 'G']\n\n\n\nimport torch.nn as nn\n\n# Define the Seq2Seq Encoder\nclass Seq2SeqEncoder(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n        super(Seq2SeqEncoder, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n\n    def forward(self, x, lengths):\n        # Embed the input\n        embedded = self.embedding(x)\n        # Pack padded sequences for efficiency\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n        return hidden, cell\n\n# Initialize the encoder\nembedding_dim = 2\nhidden_dim = 5\nencoder = Seq2SeqEncoder(vocab_size, embedding_dim, hidden_dim)\n\n# Encode full corpus\nencoded_corpus = []\n\nfor batch in dataloader:\n    src, src_len, tgt, tgt_len = batch\n    hidden, cell = encoder(src, src_len)\n    encoded_corpus.append(hidden.squeeze(0).detach().numpy())\n    \nencoded_corpus = np.array(encoded_corpus)\nprint(encoded_corpus[9])\n\n\n[[-0.21292557  0.10630303  0.16578075 -0.04456259 -0.06418022]\n [-0.07944258  0.13207509  0.11697454  0.17667645 -0.21632138]\n [-0.1618241   0.08796652  0.1672011   0.11215756 -0.09479723]\n [-0.2151655  -0.02942984  0.09883276 -0.0554556  -0.03854832]\n [-0.14274308  0.0080431   0.10658699  0.08972608 -0.04642739]\n [-0.0626098   0.18232365  0.17132407  0.2373879  -0.20108236]\n [-0.24831475 -0.07180832  0.11317882 -0.10191159 -0.02745344]\n [-0.12048124  0.10678984  0.1391682   0.19496335 -0.11692207]\n [-0.10247634  0.15764618  0.13710086  0.21111788 -0.1389591 ]\n [-0.17694989  0.0927358   0.13019726  0.10573725 -0.07771247]]\n\n\n\n\n8.1.3 Encoder with the AA simulation data",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Day7 Seq2Seq with Amino Acid</span>"
    ]
  },
  {
    "objectID": "day7_seq2seq_aa.html#decoder",
    "href": "day7_seq2seq_aa.html#decoder",
    "title": "8  Day7 Seq2Seq with Amino Acid",
    "section": "8.2 Decoder",
    "text": "8.2 Decoder\n\nThe decoder generates the output sequence step-by-step, conditioned on the latent representation provided by the encoder\nThe decoder typically consists of an RNN-based architecture, such as an LSTM or GRU, and operates sequentially.\n\n\n8.2.1 Embedding Layer\n\nConverts input indices (target tokens) into dense vectors.\n\n\n\n8.2.2 RNN Layer\n\nGenerates hidden states based on embeddings and previous hidden states.\n\n\n\n8.2.3 Linear Layer\n\nMaps the hidden states to the output vocabulary space.\n\n\n\n8.2.4 Softmax Layer\n\nConverts raw scores (logits) into probabilities over the vocabulary.\n\n\n\n\n8.2.5 Teacher forcing\n\nAt each time step, the decoder receives\n\nThe actual token from the target sequence as input.\nThe hidden states from the previous step (or from the encoder for the first step).\n\nTeacher forcing speeds up training but may cause discrepancies during inference.\n\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n        super(Seq2Seq, self).__init__()\n        self.encoder = Seq2SeqEncoder(vocab_size, embedding_dim, hidden_dim)\n        self.decoder = Seq2SeqDecoder(vocab_size, embedding_dim, hidden_dim)\n\n    def forward(self, src, src_lengths, tgt, teacher_forcing_ratio=0.5):\n        # Encode the input sequence\n        hidden, cell = self.encoder(src, src_lengths)\n        \n        # Prepare decoder input (start token)\n        batch_size = tgt.size(0)\n        tgt_length = tgt.size(1)\n        outputs = torch.zeros(batch_size, tgt_length, vocab_size)\n        \n        input_token = tgt[:, 0]  # First input token (e.g., start token)\n\n        for t in range(1, tgt_length):\n            # Decode one step\n            output, hidden, cell = self.decoder(input_token.unsqueeze(1), hidden, cell)\n            outputs[:, t, :] = output  # Store output\n\n            # Decide whether to use teacher forcing\n            teacher_force = torch.rand(1).item() &lt; teacher_forcing_ratio\n            input_token = tgt[:, t] if teacher_force else output.argmax(1)\n\n        return outputs",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Day7 Seq2Seq with Amino Acid</span>"
    ]
  }
]