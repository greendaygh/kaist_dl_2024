[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "KAIST Deep Learning",
    "section": "",
    "text": "1 KAIST EB502 programming",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "KAIST Deep Learning",
    "section": "1.1 Welcome",
    "text": "1.1 Welcome\n\n2024.11 카이스트, 공학생물학대학원 프로그래밍 강의 노트\nHaseong Kim (at KRIBB)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#환경",
    "href": "index.html#환경",
    "title": "KAIST Deep Learning",
    "section": "1.2 환경",
    "text": "1.2 환경\n\n실습 환경은 colab을 활용하며 파일 저장 등은 구글 드라이브를 활용함",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "day1_optimization.html",
    "href": "day1_optimization.html",
    "title": "2  Day1 Optimization",
    "section": "",
    "text": "Learn python for biological data analysis with chatGPT\ncolab의 default working directory에 개인의 google drive 연결\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\n2.0.1 Introduction of Google Colab\n\n2.0.1.1 Access Google Colab\n\nGo to Google Colab in your web browser.\nSign in with your Google account.\n\n\n\n2.0.1.2 Create a New Notebook\n\nClick on File -&gt; New Notebook to create a new notebook.\n\n\n\n2.0.1.3 Install Required Libraries\nGoogle Colab comes with many libraries pre-installed, but you might need to install some additional ones, such as biopython and scikit-bio. You can do this using the !pip install command directly in a cell.\n\n!pip install biopython scikit-bio matplotlib\n\n\n!pip install scikit-bio\n\n\n\n2.0.1.4 Import Libraries and Verify Installation\nIn a new code cell, import the libraries to ensure they are installed correctly.\n\n# Importing necessary libraries\nimport Bio\nimport skbio\n\nprint(\"Biopython version:\", Bio.__version__)\nprint(\"scikit-bio version:\", skbio.__version__)\n\nBiopython version: 1.84\nscikit-bio version: 0.6.2\n\n\n\n\n2.0.1.5 Upload Files to Colab\n\nCreate 2024-kaist-lecture folder\nipynb file open with colab\nDownload ganbank files from ncbi and upload the files\ncurrent directory\n\n\n!pwd\n\n/home/haseong/lecture/kaist-deeplearning-2024\n\n\n\n현재 작업 디렉토리를 위 생성한 디렉토리로 변경\n\n\nimport os\nos.chdir('drive/MyDrive/2024-kaist-lecture')\n\n\n!pwd\n\n/content/drive/MyDrive/2024-kaist-lecture\n\n\n\n분석을 위한 genbank 등의 파일을 ncbi에서 다운로드 후 위 폴더에 복사\n또는 아래 코드를 이용해서 현재 작업 디렉토리에 업로드\n\n\nfrom google.colab import files\n\nuploaded = files.upload()\n\n# Listing the uploaded files\nfor filename in uploaded.keys():\n    print(filename)\n\n\n     \n     \n      Upload widget is only available when the cell has been executed in the\n      current browser session. Please rerun this cell to enable.\n      \n       \n\n\nSaving nn.png to nn.png\nnn.png\n\n\n\n\n\nimage.png\n\n\n\n\n\n2.0.2 NumPy\nNumPy is a powerful library for numerical operations and handling arrays.\n\n2.0.2.1 Basics of NumPy\nInstallation:\n!pip install numpy\n\nimport numpy as np\n\n\n# Creating a 1D array\narr1 = np.array([1, 2, 3, 4, 5])\n\n# Creating a 2D array\narr2 = np.array([[1, 2, 3], [4, 5, 6]])\n\nprint(arr1)\nprint(arr2)\n\n\n# Element-wise operations\narr3 = arr1 * 2\nprint(arr3)\n\n# Mathematical functions\nprint(np.sqrt(arr1))\n\n[1 2 3 4 5]\n[[1 2 3]\n [4 5 6]]\n[ 2  4  6  8 10]\n[1.         1.41421356 1.73205081 2.         2.23606798]\n\n\n\n\n2.0.2.2 Numpy datatype ndarray\n\n행렬이나 다차원 배열 처리용 파이썬 라이브러리\n같은 타입의 데이터만 허용\n리스트에 비해 20배 이상 빠른 속도\n\n\nimport numpy as np\n\ndisplay(np.ones(4))\ndisplay(np.ones((3, 4)))\ndisplay(np.ones((2, 3, 4)))\n\narray([1., 1., 1., 1.])\n\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.]])\n\n\narray([[[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]],\n\n       [[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]]])\n\n\n\n\n\nalt text\n\n\n\nCreate numpy objects\n\n\nimport numpy as np\n\narr = [1, 2, 3]\nprint(arr)\nprint(type(arr))\n\na = np.array([1,2,3])\nprint(a)\nprint(a.dtype)\nprint(a.shape)\nprint(type(a))\n\n[1, 2, 3]\n&lt;class 'list'&gt;\n[1 2 3]\nint64\n(3,)\n&lt;class 'numpy.ndarray'&gt;\n\n\n\narr2 = np.array([[1,2,3], [4,5,6]])\nprint(arr2)\nprint(type(arr2))\nprint(arr2.shape)\nprint(arr2.dtype)\n\n[[1 2 3]\n [4 5 6]]\n&lt;class 'numpy.ndarray'&gt;\n(2, 3)\nint64\n\n\n\nnumpy 자료형\n\n부호가 있는 정수 int(8, 16, 32, 64)\n부호가 없는 정수 uint(8 ,16, 32, 54)\n실수 float(16, 32, 64, 128)\n복소수 complex(64, 128, 256)\n불리언 bool\n문자열 string_\n파이썬 오프젝트 object\n유니코드 unicode_\n\nnp.zeros(), np.ones(), np.arange()\n행렬 연산 지원\n\n\na = np.arange(1, 10).reshape(3,3) # [1, 10)\nprint(a)\na = np.ones((3,4), dtype=np.int16)\nb = np.ones((3,4), dtype=np.int16)\nprint(a)\nprint(b)\nprint(a+b)\nprint(a-b)\n\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\n[[1 1 1 1]\n [1 1 1 1]\n [1 1 1 1]]\n[[1 1 1 1]\n [1 1 1 1]\n [1 1 1 1]]\n[[2 2 2 2]\n [2 2 2 2]\n [2 2 2 2]]\n[[0 0 0 0]\n [0 0 0 0]\n [0 0 0 0]]\n\n\n\nnumpy 함수\n\nnp.sqrt()\nnp.log()\nnp.square()\nnp.log()\nnp.ceil()\nnp.floor()\nnp.isnan()\nnp.sum()\nnp.mean()\nnp.std()\nnp.min()\n\n\n\n\n\n2.0.3 Simple linear regression\n\nModel\n\\[\ny_i = b_0 + b_1 x_i  + \\epsilon_i \\text{ where } \\epsilon_i \\sim \\text{ iid } N(0, \\sigma^2)\n\\]\nparameters\n\n\\[\n\\theta = \\{ b_0, b_1 \\}\n\\]\n\nFind \\(\\theta\\) that minimize residuals\n\n\\[\n\\sum_{i=i}^n r_i^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2  \\\\\n\\sum_{i=1}^n (y_i - \\hat{b_1}x_i - \\hat{b_0})^2\n\\]\n\nresiduals: difference between sample observed and estimated values\n\n\nimport numpy as np\n\nnp.random.seed(123)\nX = 2 * np.random.rand(20, 1)\nY = 4 + X*0.8 + np.random.rand(20, 1)\n\nX_b = np.c_[np.ones(len(X)), X]\n# print(X)\ntheta_best = np.linalg.inv(X_b.T @ X_b) @ (X_b.T) @ Y\nY_pred_org = X_b @ theta_best\n# print(theta_best)\n\nX_new = 2 * np.random.rand(100, 1)\nX_new_b = np.c_[np.ones(len(X_new)), X_new]\nY_pred = X_new_b @ theta_best\n\nimport matplotlib.pyplot as plt\nplt.scatter(X, Y, color=\"#000000\")\nplt.plot(X_new, Y_pred, color='#cccccc', label='Predictions')\n\n# Plot residuals\nfor i in range(len(Y)):\n    plt.vlines(x=X[i], ymin=min(Y[i], Y_pred_org[i]), ymax=max(Y[i], Y_pred_org[i]), color='green', linestyle='dotted')\n\n\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.plot()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2.0.4 Ordinary least sequare (OLS)\n\nModel\n\n\\[\ny_i = b_0 + b_1 x_i  + \\epsilon_i \\text{ where } \\epsilon_i \\sim \\text{ iid } N(0, \\sigma^2), i = 1, 2, ..., n\n\\]\n\\[\nY = X \\beta + \\epsilon\n\\]\n$$\n\\[\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n... \\\\\ny_n\n\\end{bmatrix}\\]\n=\n\\[\\begin{pmatrix}\n1 \\ \\ x_1 \\\\\n1 \\ \\ x_2 \\\\\n... \\\\\n1 \\ \\ x_n\n\\end{pmatrix}\\]\n\\[\\begin{pmatrix}\nb_0 \\\\\nb_1\n\\end{pmatrix}\\]\n\n\n\n\\[\\begin{pmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n... \\\\\n\\epsilon_n\n\\end{pmatrix}\\]\n$$\n\\[\n\\mathbf{\\epsilon} = Y - X\\beta\n\\]\n\nResidual Sum of Squares (RSS)\n\n\\[\nRSS = (Y-X\\beta)^T(Y-X\\beta)\n\\]\n\nTake the gradient with respect to \\(\\beta\\) and set it to zero (Normal equation)\n\n\\[\n\\frac{\\partial RSS}{\\partial \\beta} = -2 X^T Y + 2 X^T X \\beta = 0\n\\]\n\nSolve for \\(\\beta\\) if \\((X^TX)^{-1}\\) exists\n\n\\[\n(X^T X) \\beta = X^T Y\n\\]\n\\[\n\\hat{\\beta} = (X^T X)^{-1} X^T Y\n\\]\n\n\n2.0.5 Maximum Likelihood Estimation (MLE)\n\nDetails: https://statproofbook.github.io/P/slr-mle.html\nConsider the regression as a joint probability model\nReasons to use\n\nThis framework is applicable to other complex models (non-linear, neural network)\n\nBayes rule where \\(D\\) is data, \\(\\theta\\) is parameter\n\n\\[\np(\\theta | D) = \\frac{p(D|\\theta) p(\\theta)}{p(D)}\n\\]\n\\[\n\\text{where $p(\\theta|D)$, $p(D|\\theta)$, $p(\\theta)$ are posterior, likelihood and prior, respectively}\n\\]\n\\[\np(\\theta | D) \\propto p(D|\\theta)\n\\]\n\nRegarding the likelihood, \\(p(Y|X, \\theta)\\) is interpreted as how the behaviour of the response \\(Y\\) is conditional on the values of the feature, \\(X\\), and parameters, \\(\\theta\\)\n\n\\[\n\\begin{align}\np(Y | X, \\theta)  = \\prod_{i=1}^n p( y_i | x_i, \\hat{\\theta})\n\\end{align}\n\\]\n\nThen, we can ask what is the probability of seeing the data, given a specific set of parameters? (== How the data likely to be observed given the parameters == which parameters maximize the likelihood)\n\n\\[\n\\hat{\\theta} = \\text{argmax}_\\theta \\text{ log } \\sum_{i=1}^n p( y_i | x_i, \\theta)\n\\]\n\nFor \\(p(y_i | x_i, \\theta)\\), we have assumption that all feature vectors are iid\n\n\\[\nY \\sim N(X\\beta, \\sigma)\n\\]\n\\[\n\\begin{align}\np( y_i | x_i, \\theta) &= N(y_i; X\\beta, \\sigma^2) \\\\\n&= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp{\\left( - \\frac{(y_i - b_0 - b_1 x_i)^2}{2 \\sigma^2} \\right)}\n\\end{align}\n\\]\n\nLog likelihood (LL) function\n\n\\[\n\\begin{align}\nLL(\\theta) &= \\text{ log } \\left( \\prod_{i=1}^n p( y_i | x_i, \\theta) \\right) \\\\\n&= \\text{ log } \\left( \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp{ \\left(- \\frac{(y_i - b_0 - b_1 x_i)^2}{2 \\sigma^2} \\right)} \\right) \\\\\n&= \\text{ log } \\left( \\frac{1}{\\sqrt{(2\\pi\\sigma^2)^n}} \\exp{ \\left( - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - b_0 - b_1 x_i)^2 \\right)} \\right) \\\\\n&= - \\frac{n}{2} \\log(2\\pi) - \\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - b_0 - b_1 x_i)^2\n\\end{align}\n\\]\n\nTake the gradient with respect to \\(\\beta\\) and set it to zero (OLS)\n\n\\[\n\\frac{\\partial LL(b_0, b_1, \\sigma^2)}{\\partial b_0} = \\frac{1}{\\sigma^2} \\sum_{i=1}^n (y_i - b_0 - b_1 x_i)\n\\]\n\\[\n\\begin{align}\n\\frac{\\partial LL(\\hat{b}_0, \\hat{b}_1, \\sigma^2)}{\\partial b_0}  = 0 \\\\\n\\sum_{i=1}^n (y_i - \\hat{b}_0 - \\hat{b}_1 x_i) = 0 \\\\\n\\hat{b}_0 = \\frac{1}{n}\\sum_{i=1}^n y_i - \\hat{b}_1 \\frac{1}{n} \\sum_{i=1}^n x_i \\\\\n\\end{align}\n\\]\n\\[\n\\frac{\\partial LL(\\hat{b}_0, b_1, \\sigma^2)}{\\partial b_1} = \\frac{1}{\\sigma^2} \\sum_{i=1}^n (x_i y_i - \\hat{b}_0 x_i - b_1 x_i^2)\n\\]\n\\[\n\\begin{align}\n\\frac{\\partial LL(\\hat{b}_0, \\hat{b}_1, \\sigma^2)}{\\partial b_1}  = 0 \\\\\n\\hat{b}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2 }\n\\end{align}\n\\]\n\nMaximize with respect to \\(\\sigma^2\\)\n\n\\[\n\\frac{\\partial LL(\\hat{b}_0, \\hat{b}_1, \\sigma^2)}{\\partial \\sigma^2} = - \\frac{n}{2 \\sigma^2} + \\frac{1}{2 (\\sigma^2)^2} \\sum_{i=1}^n (y_i - \\hat{b}_0 - \\hat{b}_1 x_i)^2\n\\]\n\\[\n\\frac{\\partial LL(\\hat{b}_0, \\hat{b}_1, \\hat{\\sigma}^2)}{\\partial \\sigma^2} = 0 \\\\\n\\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{b}_0 - \\hat{b}_1 x_i)^2\n\\]\n\nIn linear regression, MLE naturally leads to the OLS solution under the assumption of normally distributed residuals.\n\n\\[\n\\hat{\\beta} = (X^T X)^{-1} X^T Y\n\\]\n\\[\n\\hat{\\sigma}^2 = \\frac{1}{n} (Y-X\\beta)^T (Y-X\\beta)\n\\]\n\nHowever, MLE’s flexibility (e.g. customizable likelihood) extends beyond linear models, making it indispensable for logistic regression, mixture models, and modern deep learning frameworks.\n\n\n\n2.0.6 Gradient Decent\n\nAn iterative optimization algorithm for adjusting \\(\\beta\\) by minimizing a cost function. In linear regression, the cost function is the Mean Squared Error (MSE) under the assumption of normally distributed residuals.\nDefine a cost function \\(J(\\theta)\\)\n\n\\[\nLL(\\theta) = - \\frac{n}{2} \\log(2\\pi) - \\frac{n}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - b_0 - b_1 x_i)^2\n\\]\n\\[\nJ(\\beta) =  \\sum_{i=1}^n (y_i - b_0 - b_1 x_i)^2\n\\]\n\nmatrix notation\n\n\\[\nJ(\\beta) = || Y - X\\beta ||^2 = (Y - X\\beta)^T(Y - X\\beta)\n\\]\n\nL1 norm, L2 norm (a metric for length or magnitude of a vector/matrix) \\[\n||X||_1 = \\sum_{i}^n |x_i|\n\\]\n\n\\[\n||X||_2 = \\sqrt{\\sum_i^n x_i^2}\n\\]\n\\[\nJ(\\beta) = Y^T Y - 2 Y^T X \\beta + \\beta^T X^T X \\beta\n\\]\n\nGradient of the cost function\n\n\\[\n\\nabla_\\beta J(\\beta) = \\frac{\\partial J(\\beta)}{\\partial \\beta}\n\\]\n\\[\n\\begin{align}\n\\nabla_\\beta J(\\beta) &= 0 - 2 X^T Y + 2 X^T X \\beta \\\\\n&= - 2 X^T(Y-X\\beta)\n\\end{align}\n\\]\n\nparameter update rule\n\n\\[\n\\beta^{(t+1)} = \\beta^{(t)} - \\alpha \\nabla_\\beta J(\\beta) \\text{ where } \\alpha \\text{ is learning rate}\n\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate synthetic data\nnp.random.seed(42)\nX = 2 * np.random.rand(100, 1)  # 100 samples, 1 feature from [0, 1) uniform distribution\ny = 4 + 3 * X + np.random.randn(100, 1)  # y = 4 + 3X + noise (Gaussian noise)\n\n# Add bias term (intercept)\nX_b = np.c_[np.ones((X.shape[0], 1)), X]\n\n# Initialize parameters\nbeta = np.random.randn(2, 1)  # Random initial coefficients\nlearning_rate = 0.01\nn_iterations = 100\nm = X_b.shape[0]  # Number of samples\n\nbeta_updates = [beta.copy()]\n\n# Gradient Descent\nfor iteration in range(n_iterations):\n    gradients = -2/m * X_b.T @ (y - X_b @ beta)  # Compute gradient\n    beta = beta - learning_rate * gradients  # Update parameters\n    beta_updates.append(beta.copy())\n\n# Final parameters\nprint(\"Estimated coefficients (beta):\", beta)\n\n# Predictions\ny_pred = X_b @ beta\n\n# Plot the data and regression line\nplt.figure(figsize=(8, 6))\nplt.scatter(X, y, color=\"blue\", label=\"Data points\")\nplt.plot(X, y_pred, color=\"red\", label=\"Regression line\")\nplt.xlabel(\"X\")\nplt.ylabel(\"y\")\nplt.title(\"Linear Regression using Gradient Descent\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\nEstimated coefficients (beta): [[2.96262018]\n [3.80192916]]\n\n\n\n\n\n\n\n\n\n\n# Visualize beta updates\nfor i, beta in enumerate(beta_updates):\n    print(f\"Iteration {i}: beta = {beta.flatten()}\")\n\n# Plot convergence of coefficients\nbeta_updates = np.array(beta_updates).squeeze()\n\nplt.figure(figsize=(8, 6))\nplt.plot(range(n_iterations + 1), beta_updates[:, 0], label='Intercept (beta[0])')\nplt.plot(range(n_iterations + 1), beta_updates[:, 1], label='Slope (beta[1])')\nplt.xlabel('Iteration')\nplt.ylabel('Value of beta')\nplt.title('Convergence of Coefficients')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nIteration 0: beta = [0.01300189 1.45353408]\nIteration 1: beta = [0.12180499 1.56507648]\nIteration 2: beta = [0.22633422 1.67181808]\nIteration 3: beta = [0.32676535 1.77395782]\nIteration 4: beta = [0.42326689 1.8716864 ]\nIteration 5: beta = [0.5160004  1.96518667]\nIteration 6: beta = [0.60512075 2.05463391]\nIteration 7: beta = [0.69077645 2.14019616]\nIteration 8: beta = [0.77310984 2.22203453]\nIteration 9: beta = [0.85225741 2.30030345]\nIteration 10: beta = [0.92835001 2.37515099]\nIteration 11: beta = [1.00151308 2.44671909]\nIteration 12: beta = [1.0718669  2.51514384]\nIteration 13: beta = [1.13952675 2.58055569]\nIteration 14: beta = [1.20460319 2.64307972]\nIteration 15: beta = [1.26720221 2.70283582]\nIteration 16: beta = [1.32742539 2.75993895]\nIteration 17: beta = [1.38537016 2.81449929]\nIteration 18: beta = [1.4411299 2.8666225]\nIteration 19: beta = [1.49479416 2.91640985]\nIteration 20: beta = [1.54644877 2.96395843]\nIteration 21: beta = [1.59617603 3.00936133]\nIteration 22: beta = [1.64405484 3.05270779]\nIteration 23: beta = [1.69016085 3.09408334]\nIteration 24: beta = [1.73456657 3.13357001]\nIteration 25: beta = [1.77734155 3.17124641]\nIteration 26: beta = [1.81855245 3.20718793]\nIteration 27: beta = [1.85826316 3.24146681]\nIteration 28: beta = [1.89653497 3.27415234]\nIteration 29: beta = [1.93342661 3.30531092]\nIteration 30: beta = [1.96899442 3.33500621]\nIteration 31: beta = [2.00329239 3.36329925]\nIteration 32: beta = [2.03637228 3.39024855]\nIteration 33: beta = [2.06828373 3.4159102 ]\nIteration 34: beta = [2.09907433 3.44033798]\nIteration 35: beta = [2.1287897  3.46358343]\nIteration 36: beta = [2.15747358 3.48569598]\nIteration 37: beta = [2.1851679 3.506723 ]\nIteration 38: beta = [2.21191288 3.52670991]\nIteration 39: beta = [2.23774706 3.54570024]\nIteration 40: beta = [2.26270741 3.56373574]\nIteration 41: beta = [2.28682934 3.58085643]\nIteration 42: beta = [2.31014685 3.59710066]\nIteration 43: beta = [2.3326925 3.6125052]\nIteration 44: beta = [2.35449751 3.62710531]\nIteration 45: beta = [2.37559184 3.64093478]\nIteration 46: beta = [2.39600419 3.65402601]\nIteration 47: beta = [2.41576208 3.66641005]\nIteration 48: beta = [2.43489191 3.67811668]\nIteration 49: beta = [2.45341896 3.68917444]\nIteration 50: beta = [2.47136752 3.69961069]\nIteration 51: beta = [2.48876082 3.70945165]\nIteration 52: beta = [2.50562118 3.71872248]\nIteration 53: beta = [2.52196997 3.72744726]\nIteration 54: beta = [2.53782769 3.73564912]\nIteration 55: beta = [2.55321401 3.74335019]\nIteration 56: beta = [2.56814776 3.75057171]\nIteration 57: beta = [2.58264703 3.75733403]\nIteration 58: beta = [2.59672912 3.76365667]\nIteration 59: beta = [2.61041067 3.76955833]\nIteration 60: beta = [2.62370759 3.77505693]\nIteration 61: beta = [2.63663515 3.78016967]\nIteration 62: beta = [2.64920801 3.78491302]\nIteration 63: beta = [2.66144021 3.78930278]\nIteration 64: beta = [2.6733452  3.79335407]\nIteration 65: beta = [2.68493589 3.79708142]\nIteration 66: beta = [2.69622467 3.80049874]\nIteration 67: beta = [2.70722341 3.80361935]\nIteration 68: beta = [2.71794348 3.80645605]\nIteration 69: beta = [2.7283958  3.80902108]\nIteration 70: beta = [2.73859083 3.81132618]\nIteration 71: beta = [2.74853861 3.81338263]\nIteration 72: beta = [2.75824876 3.8152012 ]\nIteration 73: beta = [2.7677305  3.81679224]\nIteration 74: beta = [2.77699268 3.81816566]\nIteration 75: beta = [2.78604379 3.81933097]\nIteration 76: beta = [2.79489196 3.82029728]\nIteration 77: beta = [2.803545   3.82107331]\nIteration 78: beta = [2.81201037 3.82166745]\nIteration 79: beta = [2.82029527 3.82208769]\nIteration 80: beta = [2.82840657 3.82234175]\nIteration 81: beta = [2.83635086 3.82243698]\nIteration 82: beta = [2.84413447 3.82238045]\nIteration 83: beta = [2.85176348 3.82217893]\nIteration 84: beta = [2.85924369 3.8218389 ]\nIteration 85: beta = [2.8665807  3.82136659]\nIteration 86: beta = [2.87377985 3.82076795]\nIteration 87: beta = [2.88084627 3.8200487 ]\nIteration 88: beta = [2.8877849  3.81921431]\nIteration 89: beta = [2.89460044 3.81827003]\nIteration 90: beta = [2.90129743 3.81722089]\nIteration 91: beta = [2.90788021 3.8160717 ]\nIteration 92: beta = [2.91435295 3.81482709]\nIteration 93: beta = [2.92071965 3.81349148]\nIteration 94: beta = [2.92698413 3.81206911]\nIteration 95: beta = [2.93315007 3.81056405]\nIteration 96: beta = [2.93922099 3.8089802 ]\nIteration 97: beta = [2.94520029 3.80732128]\nIteration 98: beta = [2.9510912  3.80559087]\nIteration 99: beta = [2.95689684 3.8037924 ]\nIteration 100: beta = [2.96262018 3.80192916]\n\n\n\n\n\n\n\n\n\n\n2.0.6.1 Reasons to use GD instead of MLE, OLS\n\nGradient Descent is favored over MLE or OLS in scenarios involving large-scale data, high-dimensional features, non-linear models, or custom loss functions due to its flexibility, efficiency, and scalability. However, for simple, small-scale problems, OLS or MLE may still be preferred for their directness and precision.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Day1 Optimization</span>"
    ]
  },
  {
    "objectID": "day2_neural_networks.html",
    "href": "day2_neural_networks.html",
    "title": "3  Day2 Neural Networks",
    "section": "",
    "text": "3.0.1 Multiple regression",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Day2 Neural Networks</span>"
    ]
  },
  {
    "objectID": "day2_neural_networks.html#pytorch",
    "href": "day2_neural_networks.html#pytorch",
    "title": "3  Day2 Neural Networks",
    "section": "3.1 PyTorch",
    "text": "3.1 PyTorch\nPyTorch is an open-source deep learning framework developed by Facebook’s AI Research lab (FAIR). It is widely used in research and industry due to its dynamic computation graph and ease of use.\nPyTorch Ecosystem Overview:\ntorch: The core library for tensor operations and automatic differentiation.\ntorch.nn: A sub-library used to build and train neural network models.\ntorch.optim: Tools for optimization algorithms (e.g., SGD, Adam).\ntorchvision: Provides datasets, pre-trained models, and image transformations.\nTensors\nTensors are the primary data structures in PyTorch, analogous to NumPy arrays but with added capabilities such as the ability to run on GPUs for faster computation.\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimg = mpimg.imread('images/nn.png')\nplt.imshow(img)\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n3.1.1 How a Neural Network Works:\nImagine a simple neural network with one input layer, one hidden layer, and one output layer. Here’s how it functions: 1. Input data is passed to the input layer. 2. The input is transformed and propagated through the hidden layer(s), where each neuron applies weights, biases, and an activation function. 3. The result reaches the output layer, which produces the final prediction. 4. The loss function calculates the error between the predicted output and the true value. 5. Backpropagation is used to adjust the weights and biases to reduce the loss in future iterations.\n\n\n3.1.2 Simple Example:\nA neural network to classify handwritten digits (like in the MNIST dataset) might: - Input Layer: Accept an image of a digit (28x28 pixels). - Hidden Layers: Extract features and learn patterns in the image. - Output Layer: Produce probabilities for each digit (0-9), indicating the most likely classification.\n\n\n3.1.3 Applications of Neural Networks:\n\nImage recognition (e.g., identifying objects in pictures)\nSpeech recognition (e.g., voice-to-text)\nNatural language processing (e.g., language translation)\nMedical diagnosis (e.g., detecting diseases from scans)\n\nNeural networks have evolved into more complex structures such as Convolutional Neural Networks (CNNs) for image tasks and Recurrent Neural Networks (RNNs) for sequential data, enhancing their ability to solve specialized problems.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Day2 Neural Networks</span>"
    ]
  }
]