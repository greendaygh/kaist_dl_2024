{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day5 Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Concepts\n",
    "\n",
    "- When evaluating a classification model, predictions are compared with the actual (ground truth) labels.\n",
    "\n",
    "\n",
    "#### True Positive (TP)\n",
    "- The model correctly predicts the positive class.\n",
    "  - Actual: 1 (positive).\n",
    "  - Predicted: 1 (positive).\n",
    "\n",
    "#### False Positive (FP) (Type I Error)\n",
    "- The model predicts the positive class, but the actual class is negative.\n",
    "  - Actual: 0 (negative).\n",
    "  - Predicted: 1 (positive).\n",
    "  - In disease detection, marking a normal person as patient.\n",
    "\n",
    "#### True Negative (TN)\n",
    "- The model correctly predicts the negative class.\n",
    "  - Actual: 0 (negative).\n",
    "  - Predicted: 0 (negative).\n",
    "- Indicates how well the model identifies the negative class.\n",
    "\n",
    "#### False Negative (FN) (Type II Error)\n",
    "- The model predicts the negative class, but the actual class is positive.\n",
    "  - Actual: 1 (positive).\n",
    "  - Predicted: 0 (negative).\n",
    "  - In disease detection, failing to identify a patient with the disease.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix from wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/confusion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "- Accuracy is one of the most straightforward and commonly used metrics to evaluate the performance of a classification model. \n",
    "- It measures the proportion of correctly classified samples out of the total number of samples.\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n",
    "$$\n",
    "\n",
    "- In terms of the confusion matrix, accuracy can also be expressed as:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{True Positives (TP)} + \\text{True Negatives (TN)}}{\\text{Total Number of Samples (TP + TN + FP + FN)}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "- Good\n",
    "  - When the dataset is balanced, meaning the number of positive and negative samples is roughly equal.\n",
    "  - When all misclassification errors (false positives and false negatives) are equally costly.\n",
    "  - Easy to understand and implement.\n",
    "  - Works well for balanced datasets.\n",
    "\n",
    "- No good \n",
    "  - when the dataset is imbalanced:\n",
    "  - For example, if 95% of the samples belong to 1, a model that always predicts the majority class will achieve 95% accuracy, but this is misleading.\n",
    "  - Accuracy treats all errors equally, which is not suitable if false positives and false negatives have different costs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.00%\n",
      "Accuracy: 80.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Ground truth labels and model predictions\n",
    "y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
    "y_pred = [1, 0, 1, 0, 0, 1, 0, 0, 1, 1]\n",
    "\n",
    "TP = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 1)\n",
    "TN = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 0)\n",
    "FP = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 1)\n",
    "FN = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 0)\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision\n",
    "\n",
    "- **Of all the samples predicted as positive, how many are truly positive?**\n",
    "- It measures the proportion of true positive predictions out of all the samples that were predicted as positive.\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Positives (FP)}}\n",
    "$$\n",
    "\n",
    "\n",
    "- Precision is critical in scenarios where incorrectly predicting positives (FP) has significant consequences.\n",
    "- High precision ensures the model is not making too many false positive predictions, thus reducing \"false alarms.\"\n",
    "\n",
    "- Good\n",
    "   - Ensures positive predictions are reliable.\n",
    "   - Helps reduce \"false alarms\" in critical systems.\n",
    "   - Precision is especially useful for imbalanced datasets where the positive class is rare.\n",
    "\n",
    "- No Good\n",
    "   - Precision does not account for cases where the model misses actual positives (FN).\n",
    "   - In situations where identifying all positives is crucial (e.g., disease detection), precision alone is insufficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 80.00%\n",
      "Precision: 80.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# True labels\n",
    "y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
    "\n",
    "# Predicted labels\n",
    "y_pred = [1, 0, 1, 0, 0, 1, 0, 0, 1, 1]\n",
    "\n",
    "TP = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 1)\n",
    "FP = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 1)\n",
    "precision = TP / (TP + FP)\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "\n",
    "# Compute precision\n",
    "precision = precision_score(y_true, y_pred)\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall\n",
    "\n",
    "- **Of all the actual positive samples, how many did the model correctly identify?**\n",
    "- Known as sensitivity or true positive rate (TPR)\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Negatives (FN)}}\n",
    "$$\n",
    "\n",
    "- It focuses on the modelâ€™s ability to avoid missing true positives (false negatives).\n",
    "- Recall is critical in scenarios where missing positive samples (FN) can have severe consequences.\n",
    "  - Medical Diagnosis: Failing to identify a disease could lead to fatal consequences.\n",
    "\n",
    "- Recall and precision often have a trade-off:\n",
    "  - High recall increases false positives, lowering precision.\n",
    "  - High precision may miss true positives, lowering recall.\n",
    "\n",
    "\n",
    "- Good \n",
    "  - Recall ensures that the model does not miss positive samples.\n",
    "  - In applications like medical diagnosis, recall is a priority since false negatives are costly.\n",
    "\n",
    "- No good\n",
    "  - Recall does not account for how many false positives the model predicts.\n",
    "  - This can lead to overly lenient models that classify many samples as positive to maximize recall.\n",
    "  - In applications like spam detection, focusing solely on recall might result in excessive false positives (e.g., legitimate emails marked as spam).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.80\n",
      "Recall: 0.80\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# True labels\n",
    "y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
    "\n",
    "# Predicted labels\n",
    "y_pred = [1, 0, 1, 0, 0, 1, 0, 0, 1, 1]\n",
    "\n",
    "\n",
    "# Calculate recall\n",
    "TP = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 1)\n",
    "FN = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 0)\n",
    "recall = TP / (TP + FN)\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "\n",
    "# Compute recall\n",
    "recall = recall_score(y_true, y_pred)\n",
    "print(f\"Recall: {recall:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaist-prog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
